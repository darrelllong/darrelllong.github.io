[
  {
    "id": 1,
    "title": "Estimating the Reliability of Regeneration-Based Replica Control Protocols",
    "short_description": "",
    "full_content": "The accessibility of vital information can be enhanced by replicating the data on several sites, and employing a consistency control protocol to manage the replicas. The reliability of a replicated data object depends on maintaining a viable set of current replicas. When storage is limited, it may not be feasible to simply replicate a data object at enough sites to achieve the desired level of reliability. Regeneration approximates the reliability provided by additional replicas for a modest increase in storage costs, and is applicable whenever a new replica of a data object can be created faster than a system failure can be repaired. Regeneration enhances reliability by creating new replicas on other sites in response to site failures. Several strategies for replica maintenance are considered, and the benefits of each are analyzed using simulation and both algebraic and numeric solutions to systems of differential equations. Formulas describing the reliability of a replicated data object are presented, and closed-form solutions are given for the tractable cases. Numerical solutions, validated by simulation results, are used to analyze the trade-offs between reliability and storage costs. The space savings attributable to regeneration are also quantified.",
    "author": [
      "Darrell D. E. Long",
      "John L. Carroll",
      "Kris Stewart"
    ],
    "date": "Dec 1, 1989",
    "url": "https://ssrc.us/media/pubs/aa5ffe2d7d9ecc32c3c9a1fa347724599c773956.pdf",
    "bibTeX": {
      "@article": "long-tc89",
      "author": "Darrell D. E. Long and John L. Carroll and Kris Stewart",
      "title": "Estimating the Reliability of Regeneration-Based Replica Control Protocols",
      "journal": "IEEE Transactions on Computers",
      "volume": 38,
      "number": 12,
      "year": 1989,
      "month": "dec",
      "pages": "1691--1702"
    }
  },
  {
    "id": 2,
    "title": "Resilient Memory-Resident Data Objects",
    "short_description": "",
    "full_content": "Data replication has been widely used to build resilient data objects. These objects normally consist of several replicas stored in stable storage and a replication control protocol managing these replicas. Replicated data objects incur a significant penalty resulting from the increased number of disk accesses. We investigate the feasibility of replicated data objects consisting of several memory-resident replicas and one append-only log maintained on disk. We analyze, under standard Markovian hypotheses, the availability of these data objects when combined with three of the most popular replication control protocols: available copy (AC), majority consensus voting (MCV) and dynamic-linear voting (DLV). We show that replicated objects consisting of n memory-resident replicas and a disk-resident log have almost the same availability as replicated objects having n disk-resident replicas.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 1991",
    "url": "https://ssrc.us/media/pubs/a6a7a22b9280c2c99ea13639b7d543113a3493ff.pdf",
    "bibTeX": {
      "@inproceedings": "paris-ipccc91",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Resilient Memory-Resident Data Objects",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 1991,
      "month": "mar",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "145--151"
    }
  },
  {
    "id": 3,
    "title": "Voting with Regenerable Volatile Witnesses",
    "short_description": "",
    "full_content": "Voting protocols ensure the consistency of replicated objects by requiring all read and write requests to collect an appropriate quorum of replicas. We propose to replace some of these replicas by volatile witnesses that have no data and require no stable storage, and to regenerate them instead of waiting for recovery. The small size of volatile witnesses allows them to be regenerated much easier than full replicas. Regeneration attempts are also much more likely to succeed since volatile witnesses can be stored on diskless sites. We show that under standard Markovian assumptions two full replicas and one regenerable volatile witness managed by a two-tier dynamic voting protocol provide a higher data availability than three full replicas managed by majority consensus voting or optimistic dynamic voting provided site failures can be detected significantly faster than they can be repaired.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 1991",
    "url": "https://ssrc.us/media/pubs/65be35ba630fa9ce7ae749368810901343b2f2cf.pdf",
    "bibTeX": {
      "@inproceedings": "paris-icde91",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Voting with Regenerable Volatile Witnesses",
      "booktitle": "Proceedings of the Seventh International Conference on Data Engineering (ICDE)",
      "year": 1991,
      "month": "apr",
      "address": "Kobe, Japan",
      "organization": "IEEE",
      "pages": "112--119"
    }
  },
  {
    "id": 4,
    "title": "Swift: A Distributed Storage Architecture for Large Objects",
    "short_description": "",
    "full_content": "Managing large objects with high data-rate requirements is difficult for current computing systems. We describe an Input/Output architecture, called Swift, that addresses the problem of storing and retrieving very large data objects from slow secondary storage at very high data-rates. Applications that require this capability are poorly supported in current systems, even though they are made possible by high-speed networks. These range from storage and visualization of scientific computations to recodring and play-back of color video in real-time. Swift addresses the problem of providing the data rates required by digital video by exploiting the available interconnection capacity and by using several slower storage devices in parallel. We have done two studies to validate the Swift architecture: a simulation study and an Ethernet-based proof-of-concept implementation. Both studies indicate that the aggregation principle proposed in Swift can yield very high data-rates. We present a brief summary of these studies.",
    "author": [
      "Luis-Felipe Cabrera",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 1991",
    "url": "https://ssrc.us/media/pubs/b05f14d6012bbf31e75e617ef652b57e579edbb0.pdf",
    "bibTeX": {
      "@inproceedings": "cabrera-mss91",
      "author": "Luis-Felipe Cabrera and Darrell D. E. Long",
      "title": "Swift: A Distributed Storage Architecture for Large Objects",
      "booktitle": "Proceedings of the Eleventh Symposium on Mass Storage Systems (MSS)",
      "year": 1991,
      "month": "oct",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "123--128"
    }
  },
  {
    "id": 5,
    "title": "A Comparison of Two Implementations of the Token Ring Priority Function",
    "short_description": "",
    "full_content": "System performance is strongly influenced by the quality of its implementations. We describe four ways of evaluating the performance of two implementations of the priority mechanism used in the Token Ring local area network. We use various methods for comparing the small priority machine's differences and show how a simplified comparison can lead to bad results. With connection to the evaluation complexity, we also consider the simulation cost of the implementation process.",
    "author": [
      "Ivan Fellner",
      "Ivan Racko",
      "Milos Racek",
      "Karol Fabian",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1994",
    "url": "https://ssrc.us/media/pubs/a08002805a8cadaf36fd9471a6e5fd62390979e1.pdf",
    "bibTeX": {
      "@article": "fellner-ijcs94",
      "author": "Ivan Fellner and Ivan Racko and Milos Racek and Karol Fabian and Darrell D. E. Long",
      "title": "A Comparison of Two Implementations of the Token Ring Priority Function",
      "journal": "International Journal in Computer Simulation",
      "volume": 5,
      "number": 1,
      "year": 1994,
      "pages": "13--28"
    }
  },
  {
    "id": 6,
    "title": "Using an Object-oriented Framework to Construct Wide-area Group Communication Mechanisms",
    "short_description": "",
    "full_content": "Many wide-area distributed applications, including distributed databases, can be implemented using a group communication mechanism. We have developed a family of weak-consistency group communication mechanisms, based on the timestamped anti-entropy communication protocol, that provides the scalability and fault-tolerance needed by wide-area systems. We discuss an object-oriented framework for constructing this kind of group communication mechanism, and how its components can be selected to take advantage of specific application semantics. We examine several design choices that we made in building two very different wide-area distributed database applications, and how this framework led to simple, efficient implementations in both systems.",
    "author": [
      "Richard A. Golding",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 1993",
    "url": "https://ssrc.us/media/pubs/35affe3607a3ec1e5cfc9fb38c04d1a6836616ca.pdf",
    "bibTeX": {
      "@inproceedings": "golding-sac93",
      "author": "Richard A. Golding and Darrell D. E. Long",
      "title": "Using an Object-oriented Framework to Construct Wide-area Group Communication Mechanisms",
      "booktitle": "Proceedings of the International Symposium on Applied Computing",
      "year": 1993,
      "month": "oct",
      "address": "Monterrey, Mexico",
      "pages": "65--74",
      "publisher": "ACM"
    }
  },
  {
    "id": 7,
    "title": "Predicting file-system actions from prior events",
    "short_description": "",
    "full_content": "We have adapted a multi-order context modeling technique used in the data compression method Prediction by Partial Match (PPM) to track sequences of file access events. From this model, we are able to determine file system accesses that have a high probability of occurring as the next event. By prefetching the data for these events, we have transformed an LRU cache into a predictive cache that in our simulations averages 15 LRU. In fact, on average our four-megabyte predictive cache has a higher cache hit rate than a 90 megabyte LRU cache.",
    "author": [
      "Thomas M. Kroeger",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1996",
    "url": "https://ssrc.us/media/pubs/0608a3d38ff69b522073de54ba5f864ded8a240c.pdf",
    "bibTeX": {
      "@inproceedings": "kroeger-usenix96",
      "author": "Thomas M. Kroeger and Darrell D. E. Long",
      "title": "Predicting File-System Actions from Prior Events",
      "booktitle": "Proceedings of 1996 Usenix Winter Technical Conference",
      "year": 1996,
      "month": "jan",
      "address": "San Diego",
      "organization": "Usenix Association",
      "pages": "319--328"
    }
  },
  {
    "id": 8,
    "title": "Exploring the bounds of web latency reduction from caching and prefetching",
    "short_description": "",
    "full_content": "Prefetching and caching are techniques commonly used in I/O systems to reduce latency. Many researchers have advocated the use of caching and prefetching to reduce latency in the Web. We derive several bounds on the performance improvements seen from these techniques, and then use traces of Web proxy activity taken at Digital Equipment Corporation to quantify these bounds. We found that for these traces, local proxy caching could reduce latency by at best 26",
    "author": [
      "Thomas M. Kroeger",
      "Darrell D. E. Long",
      "Jeffrey C. Mogul"
    ],
    "date": "Dec 1, 1997",
    "url": "https://ssrc.us/media/pubs/507f23acd756883f557ca853b43dd6ffcd2941bd.pdf",
    "bibTeX": {
      "@inproceedings": "kroeger-sits97",
      "author": "Thomas M. Kroeger and Darrell D. E. Long and Jeffrey C. Mogul",
      "title": "Exploring the Bounds of Web Latency Reduction from Caching and Prefetching",
      "booktitle": "Proceedings of the Symposium on Internet Technologies and Systems",
      "year": 1997,
      "month": "dec",
      "address": "Monterey",
      "organization": "Usenix Association",
      "pages": "13--22"
    }
  },
  {
    "id": 9,
    "title": "In-Place Reconstruction of Delta Compressed Files",
    "short_description": "",
    "full_content": "We present an algorithm for modifying delta compressed files so that the compressed versions may be reconstructed without scratch space. This allows network clients with limited resources to efficiently update software by retrieving delta compressed versions over a network. Delta compression for binary files, compactly encoding a version of data with only the changed bytes from a previous version, may be used to efficiently distribute software over low bandwidth channels, such as the Internet. Traditional methods for rebuilding these delta files require memory or storage space on the target machine for both the old and new version of the file to be reconstructed. With the advent of network computing and Internet-enabled devices, many of these network attached target machines have limited additional scratch space. We present an algorithm for modifying a delta compressed version file so that it may rebuild the new file version in the space that the current version occupies.",
    "author": [
      "Randal C. Burns",
      "Darrell D. E. Long"
    ],
    "date": "Jun 1, 1998",
    "url": "https://ssrc.us/media/pubs/1e38dfef5b6289881f47ad0cd35bb4c509f6af3e.pdf",
    "bibTeX": {
      "@inproceedings": "burns-podc98",
      "author": "Randal C. Burns and Darrell D. E. Long",
      "title": "In-Place Reconstruction of Delta Compressed Files",
      "booktitle": "Proceedings of Principles of Distributed Computing (PODC)",
      "year": 1998,
      "month": "jun",
      "address": "Puerto Vallarta",
      "organization": "ACM",
      "pages": "267--275"
    }
  },
  {
    "id": 10,
    "title": "The case for efficient file access pattern modeling",
    "short_description": "",
    "full_content": "Most modern I/O systems treat each file access independently. However, events in a computer system are driven by programs. Thus, accesses to files occur in consistent patterns and are by no means independent. The result is that modern I/O systems ignore useful information. Using traces of file system activity we show that file accesses are strongly correlated with preceding accesses. In fact, a simple last-successor model (one that predicts each file access will be followed by the same file that followed the last time it was accessed) successfully predicted the next file 72",
    "author": [
      "Thomas M. Kroeger",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 1999",
    "url": "https://ssrc.us/media/pubs/7960c010601a7a90f42bb21ba1ea6e52b0626b22.pdf",
    "bibTeX": {
      "@inproceedings": "kroeger-hotos99",
      "author": "Thomas M. Kroeger and Darrell D. E. Long",
      "title": "The Case for Efficient File Access Pattern Modeling",
      "booktitle": "Proceedings of the Seventh Workshop on Hot Topics in Operating Systems (HotOS-VII)",
      "year": 1999,
      "month": "mar",
      "address": "Rio Rico, Arizona",
      "organization": "IEEE",
      "pages": "14--19"
    }
  },
  {
    "id": 11,
    "title": "A Zero-delay Broadcasting Protocol for Video on Demand",
    "short_description": "",
    "full_content": "… Pâris, SW Carter and DDE Long. A hybrid broadcasting protocol for video on … A zero-delay \nbroadcasting protocol for video on demand. Proc. 1999 ACM Multimedia Conference, Nov. …",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Patrick E. Mantey"
    ],
    "date": "Oct 1, 1999",
    "url": "https://ssrc.us/media/pubs/01b9fd1a5abb9b55809d73c35d7267f08858303f.pdf",
    "bibTeX": {
      "@inproceedings": "paris-acmmm99",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Patrick E. Mantey",
      "title": "A Zero-delay Broadcasting Protocol for Video on Demand",
      "booktitle": "Proceedings of the Seventh ACM International Multimedia Conference",
      "year": 1999,
      "month": "oct",
      "address": "Orlando",
      "organization": "ACM",
      "pages": "189--197"
    }
  },
  {
    "id": 12,
    "title": "Adaptive Disk Spin-Down for Mobile Computers",
    "short_description": "",
    "full_content": "We address the problem of deciding when to spin down the disk of a mobile computer in order to extend battery life. One of the most critical resources in mobile computing environments is battery life, and good energy conservation methods increase the utility of mobile systems. We use a simple and efficient algorithm based on machine learning techniques that has excellent performance. Using trace data, the algorithm outperforms several methods that are theoretically optimal under various worst-case assumptions, as well as the best fixed time-out strategy. In particular, the algorithm reduces the power consumption of the disk to about half of the energy consumed by a one minute fixed time-out policy. Furthermore, the algorithm uses as little as 88 computed in retrospect.",
    "author": [
      "David P. Helmbold",
      "Darrell D. E. Long",
      "Tracey L. Sconyers",
      "Bruce Sherrod"
    ],
    "date": "Jan 1, 2000",
    "url": "https://ssrc.us/media/pubs/23187ba74b5b5c465fc7c9595be10d6d5ea8a4dc.pdf",
    "bibTeX": {
      "@article": "helmbold-monet00",
      "author": "David P. Helmbold and Darrell D. E. Long and Tracey L. Sconyers and Bruce Sherrod",
      "title": "Adaptive Disk Spin-Down for Mobile Computers",
      "journal": "ACM/Baltzer Mobile Networks and Applications Journal",
      "volume": 5,
      "number": 4,
      "year": 2000,
      "pages": "285--297"
    }
  },
  {
    "id": 13,
    "title": "Semi-Preemptible Locks for a Distributed File System",
    "short_description": "",
    "full_content": "Many applications require the ability to obtain exclusive access to data, where an application is granted privileges to an object that cannot be preempted and limits the actions of other processes. Local file systems support exclusive access to files by maintaining information about the access rights of current open file instances, and checking subsequent opens for compatibility. Implementing exclusive access in this manner for distributed file systems degrades performance by requiring every open file to be registered with a server that maintains global open state. We introduce a distributed lock for managing file access, called a semi-preemptible lock, that avoids this performance limitation. Clients locally grant open requests for files that are consistent with a distributed semi-preemptible lock that they hold. File system clients retain or cache distributed locks, even in the absence of open file instances. When a file access lock is already cached, a client services open requests without a server message, improving performance by exploiting locality, the affinity of files to clients.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 2000",
    "url": "https://ssrc.us/media/pubs/509ccaeb00d3f565746b8af08c6d1e0bb35f353f.pdf",
    "bibTeX": {
      "@inproceedings": "burns-ipccc00",
      "author": "Randal C. Burns and Robert M. Rees and Darrell D. E. Long",
      "title": "Semi-Preemptible Locks for a Distributed File System",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2000,
      "month": "feb",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "397--404"
    }
  },
  {
    "id": 14,
    "title": "Increasing predictive accuracy through limited prefetching",
    "short_description": "",
    "full_content": "Prefetching multiple files per prediction can improve the predictive accuracy. However, it comes with the cost of using extra cache space and disk bandwidth. This paper discusses the most Recent distinct n Successor (RnS) model and uses it to demonstrate the effectiveness of our earlier work, Program-based Last n Successor (PLnS) model, a program-based prediction algorithm . We analyze the simulation results from different trace data and show that PLnS can perform better than RnS while it only predicts at most 59 RnS when the n in PLnS equals to two. PLnS is a good candidate when considering prefetching multiple files per prediction to improve predictive accuracy.",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "Jan 1, 2001",
    "url": "https://ssrc.us/media/pubs/f0c51402c600fd60463c6fa175cca43898e98623.pdf",
    "bibTeX": {
      "@inproceedings": "yeh-cnds02",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Increasing Predictive Accuracy through Limited Prefetching",
      "booktitle": "Proceedings of Communications Networks and Distributed Systems Modeling and Simulation (CNDS)",
      "year": 2002,
      "month": "jan",
      "address": "San Antonio",
      "organization": "Society for Computer Simulation",
      "pages": "131--138"
    }
  },
  {
    "id": 15,
    "title": "Design and implementation of a predictive file prefetching algorithm",
    "short_description": "",
    "full_content": "We have previously shown that the patterns in which files are accessed offer information that can accurately predict upcoming file accesses. Most modern caches ignore these patterns, thereby failing to use information that enables significant reductions in I/O latency. While prefetching heuristics that expect sequential accesses are often effective methods to reduce I/O latency, they cannot be applied across files, because the abstraction of a file has no intrinsic concept of a successor. This limits the ability of modern file systems to prefetch …",
    "author": [
      "Thomas M. Kroeger",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 2001",
    "url": "https://ssrc.us/media/pubs/fa6c3750ddb12fe0165bfd014593880513756115.pdf",
    "bibTeX": {
      "@inproceedings": "kroeger-usenix01",
      "author": "Thomas M. Kroeger and Darrell D. E. Long",
      "title": "Design and Implementation of a Predictive File Prefetching Algorithm",
      "booktitle": "Proceedings of Usenix Technical Conference",
      "year": 2001,
      "month": "jun",
      "address": "Boston",
      "organization": "Usenix Association",
      "pages": "105--118"
    }
  },
  {
    "id": 16,
    "title": "Strong security for distributed file systems",
    "short_description": "",
    "full_content": "We have developed a scheme to secure network-attached storage systems against many types of attacks. Our system uses strong cryptography to hide data from unauthorized users; someone gaining complete access to a disk cannot obtain any useful data from the system, and backups can be done without allowing the super-user access to unencrypted data. While denial-of-service attacks cannot be prevented, our system detects forged data. The system was developed using a raw disk, and can be integrated into common file systems. We discuss the design and security tradeoffs such a distributed file system makes. Our design guards against both remote intruders and those who gain physical access to the disk, using just enough security to thwart both types of attacks. This security can be achieved with little penalty to performance. We discuss the security operations that are necessary for each type of operation, and show that there is no longer any reason not to include strong encryption and authentication in network file systems.",
    "author": [
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "William E. Freeman",
      "Benjamin C. Reed"
    ],
    "date": "Apr 1, 2001",
    "url": "https://ssrc.us/media/pubs/13e25f6d063b2e30fae817e42387d13ef34de727.pdf",
    "bibTeX": {
      "@inproceedings": "miller-ipccc01",
      "author": "Ethan Miller and Darrell Long and William Freeman and Benjamin Reed",
      "title": "Strong Security for Distributed File Systems",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2001,
      "month": "apr",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "34--40"
    }
  },
  {
    "id": 17,
    "title": "Noah: Low-cost file access prediction through pairs",
    "short_description": "",
    "full_content": "Prediction is a powerful tool for performance and usability. It can reduce access latency for I/O systems, and can improve usability for mobile computing systems by automating the file hoarding process. We present recent research that has resulted in a file successor predictor that matches the performance of state-of-the-art context-modeling predictors, while requiring a small fraction of their space requirements. Noah is an on-line algorithm for predicting successor file access events, effectively identifying strong pairings (successor relationships) among files. Noah can accurately predict approximately 80 while tracking only two candidate successors of which only one requires regular dynamic updates.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2001",
    "url": "https://ssrc.us/media/pubs/ce55f8ccfc0ffef56bc5631f7db34eead40a4c59.pdf",
    "bibTeX": {
      "@inproceedings": "amer-ipccc01",
      "author": "Ahmed Amer and Darrell D. E. Long",
      "title": "Noah: Low-cost File Access Prediction Through Pairs",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2001,
      "month": "apr",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "27--33"
    }
  },
  {
    "id": 18,
    "title": "HeRMES: High-Performance Reliable MRAM-Enabled Storage",
    "short_description": "",
    "full_content": "Magnetic RAM (MRAM) is a new memory technology with access and cost characteristics comparable to those of conventional dynamic RAM (DRAM) and the non-volatility of magnetic media such as disk. Simply replacing DRAM with MRAM will make main memory non-volatile, but it will not improve file system performance. However, effective use of MRAM in a file system has the potential to significantly improve performance over existing file systems. The HeRMES file system will use MRAM to dramatically improve file system performance by using it as a permanent store for both file system data and metadata. In particular, metadata operations, which make up over 50",
    "author": [
      "Ethan L. Miller",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "May 1, 2001",
    "url": "https://ssrc.us/media/pubs/1065fdf3a405f4387b0f39afde5446cd98bf9a70.pdf",
    "bibTeX": {
      "@inproceedings": "miller-hotos01",
      "author": "Ethan L. Miller and Scott A. Brandt and Darrell D. E. Long",
      "title": "HeRMES: High-Performance Reliable MRAM-Enabled Storage",
      "booktitle": "Proceedings of the Eighth Workshop on Hot Topics in Operating Systems (HotOS-VIII)",
      "year": 2001,
      "month": "may",
      "address": "Elmau, Germany",
      "organization": "IEEE",
      "pages": "95--99"
    }
  },
  {
    "id": 19,
    "title": "Conserving Battery Energy through Making Fewer Incorrect File Predictions",
    "short_description": "",
    "full_content": "Recent increases in CPU performance have outpaced increases in hard drive performance. As a result, disk operations have become more expensive in terms of CPU cycles spent waiting for disk operations to complete. File prediction can mitigate this problem by prefetching files into cache before they are accessed. However, incorrect prediction is to a certain degree both unavoidable and costly. Battery is a valuable resource in a mobile computing environment. The utility of mobile computers is greatly affected the battery life. Incorrect prediction not only wastes cache space, also consumes battery energy. Consequently, incorrect prediction is more expensive to mobile computers than its counterpart to desktop computers. Last Successor (LS) is a commonly-used predicting algorithm in practice. We present the Program-based Last Successor (PLS) file prediction model that identifies relationships between files through the names of the programs accessing them. Our simulation results show that PLS makes 21",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "May 1, 2001",
    "url": "https://ssrc.us/media/pubs/e74e2913de61c1f751c4ac15c4d3e69e276ef853.pdf",
    "bibTeX": {
      "@inproceedings": "amer-icpads04",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Conserving Battery Energy through Making Fewer Incorrect File Predictions",
      "booktitle": "Proceedings of the International Conference on Parallel and Distributed Systems (ICPADS)",
      "year": 2004,
      "month": "jul",
      "address": "Newport Beach",
      "organization": "IEEE"
    }
  },
  {
    "id": 21,
    "title": "Efficient Data Distribution in a Web Server Farm",
    "short_description": "",
    "full_content": "For low-latency access and scalability, Web site replication with a distributed file system has advantages over proxy caching. In particular, file systems encapsulate data consistency and recovery from failure behind a simple interface so that Web servers and Internet applications can operate as if they were on a single system. However, distributing content changes to many Web servers using a file system can heavily load the network between the servers, resulting in added latency. Poor performance arises in this environment because the Web-serving workload differs radically from the assumed file system workload. To address this shortcoming of file systems, we introduce the publish cache consistency model well suited to the Web-serving workload, which relaxes cache-consistency constraints slightly in order to enhance performance. We implement publish consistency in the producer-consumer locking protocol, in which the file system pushes content updates to Web servers. A comparison of this protocol against other file system protocols by simulation shows that producer-consumer locking removes almost all latency due to protocol overhead and significantly reduces network load.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2001",
    "url": "https://ssrc.us/media/pubs/facc207da8dea2e441f6c114d02992e99927a0b9.pdf",
    "bibTeX": {
      "@article": "burns-ic01",
      "author": "Randal C. Burns and Robert M. Rees and Darrell D. E. Long",
      "title": "Efficient Data Distribution in a Web Server Farm",
      "journal": "IEEE Internet Computing",
      "volume": 5,
      "number": 4,
      "year": 2001,
      "month": "jul",
      "pages": "56--65"
    }
  },
  {
    "id": 22,
    "title": "Aggregating caches: A mechanism for implicit file prefetching",
    "short_description": "",
    "full_content": "We introduce the aggregating cache, and demonstrate how it can be used to reduce the number of file retrieval requests made by a caching client, improving storage system performance by reducing the impact of latency. The aggregating cache utilizes predetermined groupings of files to perform group retrievals. These groups are maintained by the server, and built dynamically using observed inter-file relationships. Through a simple analytical model we demonstrate how this mechanism has the potential to reduce average latencies by 75 Through trace-based simulation we demonstrate that a simple aggregating cache can reduce the number of demand fetches by almost 50 improving cache hit ratios by up to 5",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Aug 1, 2001",
    "url": "https://ssrc.us/media/pubs/47390cbc9be6f85f6b1a746ded73b1c2f0e2a3e9.pdf",
    "bibTeX": {
      "@inproceedings": "amer-mascots01",
      "author": "Ahmed Amer and Darrell D. E. Long",
      "title": "Aggregating Caches: A Mechanism for Implicit File Prefetching",
      "booktitle": "Proceedings of the Ninth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2001,
      "month": "aug",
      "address": "Cincinnati",
      "organization": "IEEE",
      "pages": "293--301"
    }
  },
  {
    "id": 23,
    "title": "Performing file prediction with a program-based successor model",
    "short_description": "",
    "full_content": "Recent increases in CPU performance have surpassed those in hard drives. As a result, disk operations have become more expensive in terms of the number of CPU cycles spent waiting for them to complete. File prediction can mitigate this problem by prefetching files into cache before they are accessed. Identifying relationships between individual files plays a key role in successfully performing file prefetching. It is well-known that previous patterns of file references can be used to predict future references. Nevertheless, knowledge about the programs producing the relationships between individual files has rarely been investigated. We present a Program-Based Successor (PBS) model that identifies relationships between files through the names of the programs accessing them. We develop a Program-based Last Successor (PLS) model derived from PBS to do file prediction. Our simulation results show that PLS makes 21 correct predictions as the Last-Successor (LS) model. We also examine the cache hit ratio achieved by applying PLS to the Least Recently Used (LRU) caching algorithm and show that a cache using PLS and LRU together can perform better than a cache up to 40 times larger using LRU alone. Finally, we argue that because program-based successors are more likely to be used soon, incorrectly prefetched program-based successors are more likely to be used and thus less incorrect than incorrectly prefetched files from non-program-based models. Keywords -- file prediction, prefetching",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "Aug 1, 2001",
    "url": "https://ssrc.us/media/pubs/e1b72bb0c98debd2f681bb45d00c511cf57c7e0b.pdf",
    "bibTeX": {
      "@inproceedings": "yeh-mascots01",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Performing File Prediction with a Program-Based Successor Model",
      "booktitle": "Proceedings of the Ninth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2001,
      "month": "aug",
      "address": "Cincinnati",
      "organization": "IEEE",
      "pages": "193--202"
    }
  },
  {
    "id": 24,
    "title": "Scalable Session Locking for a Distributed File System",
    "short_description": "",
    "full_content": "File systems provide an interface for applications to obtain exclusive access to files, in which a process holds privileges to a file that cannot be preempted and restrict the capabilities of other processes. Local file systems do this by maintaining information about the privileges of current file sessions, and checking subsequent sessions for compatibility. Implementing exclusive access in this manner for distributed file systems degrades performance by requiring every new session file to be registered with a lock server that maintains global session state. We present two techniques for improving the performance of session management in the distributed environment. We introduce a distributed lock for managing file access, called a semi-preemptible lock, that allows clients to cache privileges. Under a semi-preemptible lock, a file system creates new sessions without messages to the lock manager. This improves performance by exploiting locality -- the affinity of files to clients. We also present data structures and algorithms for the dynamic evaluation of locks that allow a distributed system file to efficiently manage arbitrarily complex locking systems. In this case, complex means that an object can be locked in a large number of unique modes. The combination of these techniques results in a distributed locking scheme that supports fine-grained concurrency control with low memory and message overhead and with the assurance that their locking system is correct and avoids unnecessary deadlocks.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Larry Stockmeyer",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2001",
    "url": "https://ssrc.us/media/pubs/c92d2f101cffec31e0f2c60230caa157b5409960.pdf",
    "bibTeX": {
      "@article": "burns-ccj01",
      "author": "Randal C. Burns and Robert M. Rees and Larry Stockmeyer and Darrell D. E. Long",
      "title": "Scalable Session Locking for a Distributed File System",
      "journal": "Cluster Computing Journal",
      "volume": 4,
      "number": 4,
      "year": 2001,
      "month": "oct",
      "pages": "295--306"
    }
  },
  {
    "id": 25,
    "title": "Using program and user information to improve file prediction performance",
    "short_description": "",
    "full_content": "Correct prediction of file accesses can improve system performance by mitigating the relative speed difference between CPU and disks. This paper discusses Program-based Last Successor (PLS) and presents Program- and User-based Last Successor (PULS), file prediction algorithms that utilize information about the program and user that access the files. Our simulation results show that PLS makes 21 incorrect predictions than last-successor with roughly the same number of correct predictions that last-successor makes. The cache space wasted on incorrect predictions can be reduced accordingly. We also show that a cache using the Least Recently Used (LRU) caching algorithm can perform better when the PULS is applied. In some cases, a cache using LRU and either PLS or PULS performs better than a cache up to 40 times larger using LRU alone.",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "Nov 1, 2001",
    "url": "https://ssrc.us/media/pubs/7f3a743da8b930fb0ddc8fae0df5dab33cab1b0d.pdf",
    "bibTeX": {
      "@inproceedings": "yeh-ispass01",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Using Program and User Information to Improve File Prediction Performance",
      "booktitle": "Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS)",
      "year": 2001,
      "month": "nov",
      "address": "Tucson",
      "organization": "IEEE",
      "pages": "111--119"
    }
  },
  {
    "id": 26,
    "title": "Compactly Encoding Unstructured Inputs with Differential Compression",
    "short_description": "",
    "full_content": "The subject of this paper is differential compression, the algorithmic task of finding common strings between versions of data and using them to encode one version compactly by describing it as a set of changes from its companion. A main goal of this work is to present new differencing algorithms that (i) operate at a fine granularity (the atomic unit of change), (ii) make no assumptions about the format or alignment of input data, and (iii) in practice use linear time, use constant space, and give good compression. We present new algorithms, which do not always compress optimally but use considerably less time or space than existing algorithms. One new algorithm runs in O(n) time and O(1) space in the worst case (where each unit of space contains n bits), as compared to algorithms that run in O(n) time and O(n) space or in O(n^2) time and O(1) space. We introduce two new techniques for differential compression and apply these to give additional algorithms that improve compression and time performance. We experimentally explore the properties of our algorithms by running them on actual versioned data. Finally, we present theoretical results that limit the compression power of differencing algorithms that are restricted to making only a single pass over the data. Categories and Subject Descriptors: D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement-- version control; E.4 [Data]: Coding and Information Theory -- data compaction and compression; E.5 [Data]: Files -- backup/recovery; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; H.2.7 [Database Management]: Database Administration -- data warehouse and repository; H.3.5 [Information Storage and Retrieval]: Online Information Services -- web-based services General Terms: Algorithms, Experimentation, Performance, Theory Additional Key Words and Phrases: Delta compression, differencing, differential compressions",
    "author": [
      "Miklós Ajtai",
      "Randal Burns",
      "Ronald Fagin",
      "Darrell D. E. Long",
      "Larry Stockmeyer"
    ],
    "date": "Jan 1, 2002",
    "url": "https://ssrc.us/media/pubs/5a186979c59835aa8409c8ba9261ae3fb6bdcff1.pdf",
    "bibTeX": {
      "@article": "ajtai-jacm02",
      "author": "Mikl\\'os Ajtai and Randal Burns and Ronald Fagin and Darrell Long and Larry Stockmeyer",
      "title": "Compactly Encoding Unstructured Inputs with Differential Compression",
      "journal": "Journal of the ACM",
      "volume": 49,
      "number": 3,
      "year": 2002,
      "month": "may",
      "pages": "318--367"
    }
  },
  {
    "id": 27,
    "title": "Strong Security for Network-Attached Storage",
    "short_description": "",
    "full_content": "We have developed a scheme to secure network-attached storage systems against many types of attacks. Our system uses strong cryptography to hide data from unauthorized users; someone gaining complete access to a disk cannot obtain any useful data from the system, and backups can be done without allowing the super-user access to clear text. While insider denial-of-service attacks cannot be prevented (an insider can physically destroy the storage devices), our system detects forged data. The system was developed using a raw disk, and can be integrated into common file systems. All of this security can be achieved with little penalty to performance. Our experiments show that, using a relatively inexpensive commodity CPU attached to a disk, our system can store and retrieve data with virtually no penalty for random disk requests and only a 15--20 requests. With such a minor performance penalty, there is no longer any reason not to include strong encryption and authentication in network file systems.",
    "author": [
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "William Freeman",
      "Benjamin C. Reed"
    ],
    "date": "Jan 15, 2002",
    "url": "https://ssrc.us/media/pubs/1c418abf401c2ced804f4ccff2b3c37f9647bd19.pdf",
    "bibTeX": {
      "@inproceedings": "miller-fast02",
      "author": "Ethan L. Miller and Darrell D. E. Long and William Freeman and Benjamin C. Reed",
      "title": "Strong Security for Network-Attached Storage",
      "booktitle": "Proceedings of the Conference on File and Storage Technologies (FAST)",
      "year": 2002,
      "month": "jan",
      "address": "Monterey",
      "organization": "Usenix Association",
      "pages": "1--13"
    }
  },
  {
    "id": 28,
    "title": "Data Placement Based on Seek Time Analysis of a MEMS-based Storage Device",
    "short_description": "",
    "full_content": "",
    "author": [
      "Euiseong Seo",
      "Seon-Yeong Park",
      "Bhuvan Urgaonkar",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 2005",
    "url": "https://ssrc.us/media/pubs/7b45b4b007e2744803712192a42a3baa4d62adde.pdf",
    "bibTeX": {
      "@inproceedings": "seo-mascots05",
      "author": "Euiseong Seo and Seon-Yeong Park and Bhuvan Urgaonkar and Darrell D. E. Long",
      "title": "Data Placement Based on Seek Time Analysis of a MEMS-based Storage Device",
      "booktitle": "Proceedings of the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2005,
      "month": "sep",
      "address": "Atlanta",
      "organization": "IEEE"
    }
  },
  {
    "id": 29,
    "title": "Who is more adaptive? ACME: adaptive caching using multiple experts",
    "short_description": "",
    "full_content": "The gap between CPU speeds and the speed of the technologies providing the data is increasing. As a result, latency and bandwidth to needed data is limited by the performance of the storage devices and the networks that connect them to the CPU. Distributed caching techniques are often used to reduce the penalties associated with such caching; however, such techniques need further development to be truly integrated into the network. This paper describes the preliminary design of an adaptive caching scheme using multiple experts, called ACME. ACME is used to manage the replacement policies within distributed caches to further improve the hit rates over static caching techniques. We propose the use of machine learning algorithms to rate and select the current best policies or mixtures of policies via weight updates based on their recent success, allowing each adaptive cache node to tune itself based on the workload it observes. Since no cache databases or synchronization messages are exchanged for adaptivity, the clusters composed of these nodes will be scalable and manageable. We show that static techniques are suboptimal when combined in networks of caches, providing potential for adaptivity to improve performance. : Caching, adaptive systems, clusters, adaptive caching, static caching",
    "author": [
      "Ismail Ari",
      "Ahmed Amer",
      "Ethan L. Miller",
      "Robert Gramacy",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 2002",
    "url": "https://ssrc.us/media/pubs/7f4e71177a2a7078b1848d7a0d6c8e98c4986cf1.pdf",
    "bibTeX": {
      "@incollection": "ari-carleton02",
      "author": "Ismail Ari and Ahmed Amer and Ethan L. Miller and Robert Gramacy and Scott A. Brandt and Darrell D. E. Long",
      "title": "ACME: Adaptive Caching using Multiple Experts",
      "booktitle": "Distributed Data \\& Structures 4 (Proceedings in Informatics 14)",
      "editor": "W. Litwin and G. L\\`evy",
      "publisher": "Carleton Scientific",
      "year": 2002,
      "month": "mar",
      "address": "Paris, France",
      "pages": "143--158"
    }
  },
  {
    "id": 30,
    "title": "File access prediction with adjustable accuracy",
    "short_description": "",
    "full_content": "We describe a novel on-line file access predictor, Recent Popularity, capable of rapid adaptation to workload changes while simultaneously predicting more events with greater accuracy than prior efforts. We distinguish the goal of predicting the most events accurately from the goal of offering the most accurate predictions (when declining to offer a prediction is acceptable). For this purpose we present two distinct measures of accuracy, general and specific accuracy, corresponding to these goals. We describe how our new predictor and an earlier effort, Noah, can trade the number of events predicted for prediction accuracy by modifying simple parameters. When prediction accuracy is strictly more important than the number of predictions offered, trace-based evaluation demonstrates error rates as low as 2 all file access events.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Jehan-François Pâris",
      "Randal C. Burns"
    ],
    "date": "Apr 1, 2002",
    "url": "https://ssrc.us/media/pubs/5cd3b23df47846a9021e668f6637cc4e8675109f.pdf",
    "bibTeX": {
      "@inproceedings": "amer-ipccc02",
      "author": "Ahmed Amer and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris and Randal C. Burns",
      "title": "File Access Prediction with Adjustable Accuracy",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2002,
      "month": "apr",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "131--140"
    }
  },
  {
    "id": 31,
    "title": "Group-based management of distributed file caches",
    "short_description": "",
    "full_content": "We describe how to manage distributed file system caches based upon groups of files that are accessed together. We use file access patterns to automatically construct dynamic groupings of files and then manage our cache by fetching groups, rather than single files. We present experimental results, based on trace-driven workloads, demonstrating that grouping improves cache performance. At the file system client, grouping can reduce LRU demand fetches by 50 to 60 At the server, cache hit rate improvements are much more pronounced, but vary widely (20 to over 1200 depending upon the capacity of intervening caches. Our treatment includes information theoretic results that justify our approach to file grouping.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Randal C. Burns"
    ],
    "date": "Jul 1, 2002",
    "url": "https://ssrc.us/media/pubs/d596362817aa990d06c1c497d4c2a5423ea5829d.pdf",
    "bibTeX": {
      "@inproceedings": "amer-icdcs02",
      "author": "Ahmed Amer and Darrell D. E. Long and Randal C. Burns",
      "title": "Group-Based Management of Distributed File Caches",
      "booktitle": "Proceedings of the Twenty-second International Conference on Distributed Computing Systems (ICDCS)",
      "year": 2002,
      "month": "jul",
      "address": "Vienna",
      "organization": "IEEE",
      "pages": "525--534"
    }
  },
  {
    "id": 32,
    "title": "Power conservation strategies for MEMS-based storage devices",
    "short_description": "",
    "full_content": "Power dissipation by storage systems in mobile computers accounts for a large percentage of the power consumed by the entire system. Reducing the power used by the storage device is crucial for reducing overall power consumption. A new class of secondary storage devices based on microelectromechanical systems (MEMS) promises to consume an order of magnitude less power with 10--20 times shorter latency and 10 times greater storage densities. We describe three strategies to reduce power consumption: aggressive spin-down, sequential request merging, and subsector accesses. We show that aggressive spin-down can save up to 50 by the device at the cost of increased response time. Merging of sequential requests can save up to 18 reduce response time by about 20 requests such as those for metadata can save 40 energy. Finally, we show that by applying all three power management strategies simultaneously the total power consumption of MEMS-based storage devices can be reduced by about 54 performance.",
    "author": [
      "Ying Lin",
      "Scott A. Brandt",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Oct 1, 2002",
    "url": "https://ssrc.us/media/pubs/6f89379bc0ae69a695406e103aaa488b6c385454.pdf",
    "bibTeX": {
      "@inproceedings": "lin-mascots02",
      "author": "Ying Lin and Scott A. Brandt and Darrell D. E. Long and Ethan L. Miller",
      "title": "Power Conservation Strategies for MEMS-based Storage Devices",
      "booktitle": "Proceedings of the Tenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2002,
      "month": "oct",
      "address": "Fort Worth",
      "organization": "IEEE",
      "pages": "53--62"
    }
  },
  {
    "id": 33,
    "title": "ACME: Adaptive Caching Using Multiple Experts",
    "short_description": "",
    "full_content": "The trend in cache design research is towards finding the single optimum replacement policy that performs better than any other proposed policy by using all the useful criteria at once. However, due to the variety of workloads and system topologies it is daunting, if not impossible, to summarize all this information into one magical value using any static formula. We propose a workload and topology adaptive cache management algorithm to address this problem. Based on proven machine learning techniques, this algorithm uses a weighted voting mechanism guided by a pool of cache replacement policies. Objects that collect the highest total vote from all policies stay in the cache. The policies that predict the workload well are rewarded by an increase in their weight and the policies that lead to wrong decisions are punished by a decrease in their weight. Weight adjustments of the replacement policies, or caching experts, are managed by extremely powerful but computationally simple machine learning algorithms. The scheme is different from hybrid criteria schemes and partitioned cache management policies because it is adaptive, and because it favors objects that are rated highly by many policies rather than simply favoring objects with high weight by a single, possibly complex policy.",
    "author": [
      "Ismail Ari",
      "Ahmed Amer",
      "Ethan L. Miller",
      "Robert Gramacy",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Dec 5, 2002",
    "url": "https://ssrc.us/media/pubs/96c7ffa6733e50e6fd43f14d4493118f07e369ab.pdf",
    "bibTeX": {
      "@incollection": "ari-carleton02",
      "author": "Ismail Ari and Ahmed Amer and Ethan L. Miller and Robert Gramacy and Scott A. Brandt and Darrell D. E. Long",
      "title": "ACME: Adaptive Caching using Multiple Experts",
      "booktitle": "Distributed Data \\& Structures 4 (Proceedings in Informatics 14)",
      "editor": "W. Litwin and G. L\\`evy",
      "publisher": "Carleton Scientific",
      "year": 2002,
      "month": "mar",
      "address": "Paris, France",
      "pages": "143--158"
    }
  },
  {
    "id": 34,
    "title": "A Variable Bandwidth Broadcasting Protocol For Video-onDemand",
    "short_description": "",
    "full_content": "We present the first broadcasting protocol that can alter the number of channels allocated to a given video without inconveniencing the viewer and without causing any temporary bandwidth surge. Our variable bandwidth broadcasting (VBB) protocol assigns to each video a minimum number of channels whose bandwidths are all equal to the video consumption rate. Additional channels can be assigned to the video at any time to reduce the customer waiting time or retaken to free server bandwidth. The cost of this additional flexibility is quite reasonable as the bandwidth requirements of our VBB fall between those of the fast broadcasting protocol and the new pagoda broadcasting protocol.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 2003",
    "url": "https://ssrc.us/media/pubs/18cd2bfc2e22ef31b7c0631b67be9263fdd2ed27.pdf",
    "bibTeX": {
      "@inproceedings": "paris-mmcn03",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "A Variable Bandwidth Broadcasting Protocol For Video-on-Demand",
      "booktitle": "Proceedings of the 2003 SPIE Multimedia Computing and Networking Conference (MMCN)",
      "year": 2003,
      "month": "jan",
      "address": "San Jose",
      "organization": "SPIE",
      "pages": "209--219"
    }
  },
  {
    "id": 35,
    "title": "Using Multiple Predictors to Improve the Accuracy of File Access Predictions",
    "short_description": "",
    "full_content": "Existing file access predictors keep track of previous file access patterns and rely on a single heuristic to predict which of the previous successors to the file being currently accessed is the most likely to be accessed next. We present here a novel composite predictor that applies multiple heuristics to this selection problem. As a result, it can make use of specialized heuristics that can make very accurate predictions when access patterns are observed to meet their particular criteria. Simulation results involving a total of seven file access traces indicate that our predictor delivers more correct predictions and less inaccurate guesses than predictors relying on a single heuristic for selecting a successor.",
    "author": [
      "Gary A. S. Whittle",
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Randal Burns"
    ],
    "date": "Apr 1, 2003",
    "url": "https://ssrc.us/media/pubs/a917ac198431394c2b99c74908fe451443399537.pdf",
    "bibTeX": {
      "@inproceedings": "whittle-mss03",
      "author": "Gary A. S. Whittle and Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long and Randal Burns",
      "title": "Using Multiple Predictors to Improve the Accuracy of File Access Predictions",
      "booktitle": "Proceedings of the Twentieth Symposium on Mass Storage Systems (MSS)",
      "year": 2003,
      "month": "apr",
      "address": "San Diego",
      "organization": "IEEE",
      "pages": "230--240"
    }
  },
  {
    "id": 36,
    "title": "Visualizing Cache Effects on I/O Workload Predictability",
    "short_description": "",
    "full_content": "We describe our experience graphically visualizing data access behavior, with a specific emphasis on visualizing the predictability of such accesses and the consistency of these observations at the block level. Such workloads are more frequently encountered after filtering through intervening cache levels and in this paper we demonstrate how such filtered workloads pose a problem for traditional caching schemes. We demonstrate how prior results are consistent across both file and disk access workloads. We also demonstrate how an aggregating cache based on predictive grouping can overcome such filtering effects. Our visualization tool provides an illustration of how file workloads remain predictable in the presence of intervening caches, explaining how the aggregating cache can remain effective under what would normally be considered adverse conditions. We further demonstrate how the same predictability remains true with physical block workloads.",
    "author": [
      "Ahmed Amer",
      "Alison Luo",
      "Newton Der",
      "Darrell D. E. Long",
      "Alex Pang"
    ],
    "date": "Apr 1, 2003",
    "url": "https://ssrc.us/media/pubs/eab92f87efdf3bf336a03603e7b8ba6889d5cb45.pdf",
    "bibTeX": {
      "@inproceedings": "amer-ipccc03",
      "author": "Ahmed Amer and Alison Luo and Newton Der and Darrell D. E. Long and Alex Pang",
      "title": "Visualizing Cache Effects on I/O Workload Predictability",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2003,
      "month": "apr",
      "address": "Phoenix",
      "organization": "IEEE",
      "pages": "417--424"
    }
  },
  {
    "id": 37,
    "title": "Efficient Metadata Management in a Large Distributed Storage System",
    "short_description": "",
    "full_content": "Existing file access predictors keep track of previous file access patterns and rely on a single heuristic to predict which of the previous successors to the file being currently accessed is the most likely to be accessed next. We present here a novel composite predictor that applies multiple heuristics to this selection problem. As a result, it can make use of specialized heuristics that can make very accurate predictions when access patterns are observed to meet their particular criteria. Simulation results involving a total of seven file access traces indicate that our predictor delivers more correct predictions and less inaccurate guesses than predictors relying on a single heuristic for selecting a successor.",
    "author": [
      "Scott A. Brandt",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Lan Xue"
    ],
    "date": "Apr 1, 2003",
    "url": "https://ssrc.us/media/pubs/a3d8d7cf4deff274fc35948573910760c593c5a9.pdf",
    "bibTeX": {
      "@inproceedings": "brandt-mss03",
      "author": "Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long and Lan Xue",
      "title": "Efficient Metadata Management in a Large Distributed Storage System",
      "booktitle": "Proceedings of the Twentieth Symposium on Mass Storage Systems (MSS)",
      "year": 2003,
      "month": "apr",
      "address": "San Diego",
      "organization": "IEEE",
      "pages": "290--298"
    }
  },
  {
    "id": 38,
    "title": "Reliability Mechanisms for Very large Storage Systems",
    "short_description": "",
    "full_content": "Reliability and availability are increasingly important in large-scale storage systems built from thousands of individual storage devices. Large systems must survive the failure of individual components; in systems with thousands of disks, even infrequent failures are likely in some device. We focus on two types of errors: nonrecoverable read errors and drive failures. We discuss mechanisms for detecting and recovering from such errors, introducing improved techniques for detecting errors in disk reads and fast recovery from disk failure. We show that simple RAID cannot guarantee sufficient reliability; our analysis examines the tradeoffs among other schemes between system availability and storage efficiency. Based on our data, we believe that two-way mirroring should be sufficient for most large storage systems. For those that need very high reliability, we recommend either three-way mirroring or mirroring combined with RAID.",
    "author": [
      "Qin Xin",
      "Ethan L. Miller",
      "Scott A. Brandt",
      "Darrell D. E. Long",
      "Thomas Schwarz",
      "Witold Litwin"
    ],
    "date": "Apr 15, 2003",
    "url": "https://ssrc.us/media/pubs/92ca6e61ab487a1947c85029cedcd02d32dd3f34.pdf",
    "bibTeX": {
      "@inproceedings": "xin-mss03",
      "author": "Qin Xin and Ethan L. Miller and Scott A. Brandt and Darrell D. E. Long and Thomas Schwarz and Witold Litwin",
      "title": "Reliability Mechanisms for Very Large Storage Systems",
      "booktitle": "Proceedings of the Twentieth Symposium on Mass Storage Systems (MSS)",
      "year": 2003,
      "month": "apr",
      "address": "San Diego",
      "organization": "IEEE",
      "pages": "146--156"
    }
  },
  {
    "id": 39,
    "title": "In-Place Reconstruction of Version Differences",
    "short_description": "",
    "full_content": "In-place reconstruction of differenced data allows information on devices with limited storage capacity to be updated efficiently over low-bandwidth channels. Differencing encodes a version of data compactly as a set of changes from a previous version. Transmitting updates to data as a version difference saves both time and bandwidth. In-place reconstruction rebuilds the new version of the data in the storage or memory the current version occupies--no scratch space is needed for a second version. By combining these technologies, we support highly mobile applications on space-constrained hardware. We present an algorithm that modifies a differentially encoded version to be in-place reconstructible. The algorithm trades a small amount of compression to achieve this property. Our treament includes experimental results that show our implementation to be efficient in space and time and verify that compression losses are small. Also, we give results on the computational complexity of performing this modification while minimizing lost compression. Terms---Differencing, differential compression, version management, data distribution, in-place reconstruction, mobile computing.",
    "author": [
      "Randal Burns",
      "Larry Stockmeyer",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2003",
    "url": "https://ssrc.us/media/pubs/2f301095bc2a766873f54153eaec6d83e10b142d.pdf",
    "bibTeX": {
      "@article": "burns-tkde03",
      "author": "Randal Burns and Larry Stockmeyer and Darrell Long",
      "title": "In-Place Reconstruction of Version Differences",
      "journal": "IEEE Transactions on Knowledge and Data Engineering",
      "volume": 15,
      "number": 4,
      "year": 2003,
      "month": "jul",
      "pages": "973--984"
    }
  },
  {
    "id": 40,
    "title": "A Stochastic Approach to File Access Prediction",
    "short_description": "",
    "full_content": "Most existing studies of file access prediction are experimental in nature and rely on trace driven simulation to predict the performance of the schemes being investigated. We present a first order Markov analysis of file access prediction, discuss its limitations and show how it can be used to estimate the performance of file access predictors, such as First Successor, Last Successor, Stable Successor and Best-k-out-of-n. We compare these analytical results with experimental measurements performed on several file traces and find out that specific workloads, and indeed individual files, can exhibit very different levels of non-stationarity. Overall, at least 60 percent of access requests appear to remain stable over at least a month. Terms: File access prediction, storage hierarchies, storage management.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 2003",
    "url": "https://ssrc.us/media/pubs/18d1d1827feea73154044eb088d7b10ab57ed69a.pdf",
    "bibTeX": {
      "@inproceedings": "paris-snapi03",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long",
      "title": "A Stochastic Approach to File Access Prediction",
      "booktitle": "Proceedings of the International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI)",
      "year": 2003,
      "month": "sep",
      "organization": "ACM"
    }
  },
  {
    "id": 41,
    "title": "Zone-based shortest positioning time first scheduling for MEMS-based storage devices",
    "short_description": "",
    "full_content": "Access latency to secondary storage devices is frequently a limiting factor in computer system performance. New storage technologies promise to provide greater storage densities at lower latencies than is currently obtainable with hard disk drives. MEMS-based storage devices use orthogonal magnetic or physical recording techniques and thousands of simultaneously active MEMS-based read-write tips to provide high-density low-latency non-volatile storage. These devices promise seek times 10--20 times faster than hard drives, storage densities 10 times greater, and power consumption an order of magnitude lower. Previous research has examined data layout and request ordering algorithms that are analogs of those developed for hard drives. We present an analytical model of MEMS device performance that motivates a computationally simple MEMS-based request scheduling algorithm called ZSPTF, which has average response times comparable to Shortest Positioning Time First (SPTF) but with response time variability comparable to Circular Scan (C-SCAN).",
    "author": [
      "Bo Hong",
      "Scott A. Brandt",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Karen A. Glocer",
      "Zachary N. J. Peterson"
    ],
    "date": "Oct 1, 2003",
    "url": "https://ssrc.us/media/pubs/e224295a36adce6779233ef1857f0d11049e19d3.pdf",
    "bibTeX": {
      "@inproceedings": "hong-mascots03",
      "author": "Bo Hong and Scott A. Brandt and Darrell D. E. Long and Ethan L. Miller and Karen A. Glocer and Zachary N. J. Peterson",
      "title": "Zone-Based Shortest Positioning Time First Scheduling for MEMS-Based Storage Devices",
      "booktitle": "Proceedings of the Eleventh International Symposium on Modeling, Analysis, and Simulation in Computer and Telecommunication Systems (MASCOTS)",
      "year": 2003,
      "month": "oct",
      "address": "Orlando",
      "organization": "IEEE",
      "pages": "104--113"
    }
  },
  {
    "id": 42,
    "title": "Managing flash crowds on the Internet",
    "short_description": "",
    "full_content": "A flash crowd is a surge in traffic to a particular Web site that causes the site to be virtually unreachable. We present a model of flash crowd events and evaluate the performance of various multi-level caching techniques suitable for managing these events. By using well-dispersed caches and with judicious choice of replacement algorithms we show reductions in client response times by as much as a factor of 25. We also show that these caches eliminate the server and network hot spots by distributing the load over the entire network.",
    "author": [
      "Ismail Ari",
      "Bo Hong",
      "Ethan L. Miller",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2003",
    "url": "https://ssrc.us/media/pubs/b6d9fb1b60f68f48bb8f6c32894bd02a135d12a5.pdf",
    "bibTeX": {
      "@inproceedings": "ari-mascots03",
      "author": "Ismail Ari and Bo Hong and Ethan L. Miller and Scott A. Brandt and Darrell D. E. Long",
      "title": "Managing Flash Crowds on the Internet",
      "booktitle": "Proceedings of the Eleventh International Symposium on Modeling, Analysis, and Simulation in Computer and Telecommunication Systems (MASCOTS)",
      "year": 2003,
      "month": "oct",
      "address": "Orlando",
      "organization": "IEEE",
      "pages": "246--249"
    }
  },
  {
    "id": 43,
    "title": "Identifying Stable File Access Patterns",
    "short_description": "",
    "full_content": "Parallel scientific applications require high-performance I/O support from underlying file systems. A comprehensive understanding of the expected workload is therefore essential for the design of high-performance parallel file systems. We re-examine the workload characteristics in parallel computing environments in the light of recent technology advances and new applications. We analyze application traces from a cluster with hundreds of nodes. On average, each application has only one or two typical request sizes. Large requests from several hundred kilobytes to several megabytes are very common. Although in some applications, small requests account for more than 90 are transferred by large requests. All of these applications show bursty access patterns. More than 65 within one millisecond in most applications. By running the same benchmark on different file models, we also find that the write throughput of using an individual output file for each node exceeds that of using a shared file for all nodes by a factor of 5. This indicates that current file systems are not well optimized for file sharing.",
    "author": [
      "Purvi Shah",
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2004",
    "url": "https://ssrc.us/media/pubs/e021012af63507f472049c007939b842aa2a5f10.pdf",
    "bibTeX": {
      "@inproceedings": "shah-mss04",
      "author": "Purvi Shah and Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long",
      "title": "Identifying Stable File Access Patterns",
      "booktitle": "Proceedings of the Twenty-first Symposium on Mass Storage Systems (MSS)",
      "year": 2004,
      "month": "apr",
      "address": "Goddard, Maryland",
      "organization": "NASA",
      "pages": "159--163"
    }
  },
  {
    "id": 44,
    "title": "File System Workload Analysis For Large Scientific Computing Applications",
    "short_description": "",
    "full_content": "The object-based storage model, in which files are made up of one or more data objects stored on self-contained Object-Based Storage Devices (OBSDs), is emerging as an architecture for distributed storage systems. The workload presented to the OBSDs will be quite different from that of general-purpose file systems, yet many distributed file systems employ general-purpose file systems as their underlying file system. We present OBFS, a small and highly efficient file system designed for use in OBSDs. Our experiments show that our user-level implementation of OBFS outperforms Linux Ext2 and Ext3 by a factor of two or three, and while OBFS is 1/25 the size of XFS, it provides only slightly lower read performance and up to 40 write performance.",
    "author": [
      "Feng Wang",
      "Qin Xin",
      "Bo Hong",
      "Scott A. Brandt",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Tyce T. McLarty"
    ],
    "date": "Apr 13, 2004",
    "url": "https://ssrc.us/media/pubs/d0b29be475da4315b7e3e7bb06a579fe3fc30b0e.pdf",
    "bibTeX": {
      "@inproceedings": "wang-mss04",
      "author": "Feng Wang and Qin Xin and Bo Hong and Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long and Tyce T. McLarty",
      "title": "File System Workload Analysis For Large Scale Scientific Computing Applications",
      "booktitle": "Proceedings of the Twenty-first Symposium on Mass Storage Systems (MSS)",
      "year": 2004,
      "month": "apr",
      "address": "Goddard, Maryland",
      "organization": "NASA",
      "pages": "139--152"
    }
  },
  {
    "id": 45,
    "title": "OBFS: A File System for Object-Based Storage Devices",
    "short_description": "",
    "full_content": "The object-based storage model, in which files are made up of one or more data objects stored on self-contained Object-Based Storage Devices (OBSDs), is emerging as an architecture for distributed storage systems. The workload presented to the OBSDs will be quite different from that of general-purpose file systems, yet many distributed file systems employ general-purpose file systems as their underlying file system. We present OBFS, a small and highly efficient file system designed for use in OBSDs. Our experiments show that our user-level implementation of OBFS outperforms Linux Ext2 and Ext3 by a factor of two or three, and while OBFS is 1/25 the size of XFS, it provides only slightly lower read performance and up to 40 write performance.",
    "author": [
      "Feng Wang",
      "Scott A. Brandt",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Apr 15, 2004",
    "url": "https://ssrc.us/media/pubs/791542036f88a466f05945aab74004a9cc7d5b9f.pdf",
    "bibTeX": {
      "@inproceedings": "wang-mss04b",
      "author": "Feng Wang and Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long",
      "title": "OBFS: A File System for Object-Based Storage Devices",
      "booktitle": "Proceedings of the Twenty-first Symposium on Mass Storage Systems (MSS)",
      "year": 2004,
      "month": "apr",
      "address": "Goddard, Maryland",
      "organization": "NASA",
      "pages": "283--300"
    }
  },
  {
    "id": 46,
    "title": "Reliability of MEMS-Based Storage Enclosures",
    "short_description": "",
    "full_content": "MEMS-based storage is a new, non-volatile storage technology currently under development. It promises fast data access, high throughput, high storage density, small physical size, low power consumption, and low entry costs. These properties make MEMS-based storage into a serious alternative to disk drives, in particular for mobile applications. The first generation of MEMS will only offer a fraction of the storage capacity of disks; therefore, we propose to integrate multiple MEMS devices into a MEMS storage enclosure, organizing them as a RAID Level 5 with multiple spares, to be used as the basic storage building block. This paper investigates the reliability of such an enclosure. We find that Mean Time To Failure is an inappropriate reliability metric for MEMS enclosures. We show that the reliability of the enclosures is appropriate for their economic lifetime if users choose not to replace failed MEMS storage components. In addition, we investigate the benefits of occasional, preventive maintenance of enclosures.",
    "author": [
      "Bo Hong",
      "Thomas J. E. Schwarz",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2004",
    "url": "https://ssrc.us/media/pubs/57a1adb40306db70c6d52376e389a836f60a29c1.pdf",
    "bibTeX": {
      "@inproceedings": "hong-mascots04",
      "author": "Bo Hong and Thomas J. E. Schwarz and Scott A. Brandt and Darrell D. E. Long",
      "title": "Reliability of MEMS-Based Storage Enclosures",
      "booktitle": "Proceedings of the Twelfth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2004,
      "month": "oct",
      "address": "Volendam, The Netherlands",
      "organization": "IEEE",
      "pages": "571--579"
    }
  },
  {
    "id": 47,
    "title": "Predicting When Not To Predict",
    "short_description": "",
    "full_content": "File prefetching based on previous file access patterns has been shown to be an effective means of reducing file system latency by implicitly loading caches with files that are likely to be needed in the near future. Mistaken prefetching requests can be very costly in terms of added performance overheads including increased latency and bandwidth consumption. Such costs of mispredictions are easily overlooked when considering access prediction algorithms only in terms of their accuracy, but we describe a novel algorithm that uses machine learning to not only improve overall prediction accuracy, but as a means to avoid these costly mispredictions. Our algorithm is fully adaptive to changing workloads, and is fully automated in its ability to refrain from offering predictions when they are likely to be mistaken. Our trace-based simulations show that our algorithm produces prediction accuracies of up to 98 cache hit ratios, application of this algorithm actually results in substantial reductions in unnecessary (and costly) I/O operations.",
    "author": [
      "Karl Brandt",
      "Darrell D. E. Long",
      "Ahmed Amer"
    ],
    "date": "Oct 1, 2004",
    "url": "https://ssrc.us/media/pubs/25bd1fcf849efacb1591a3f06488113089755cf4.pdf",
    "bibTeX": {
      "@inproceedings": "brandt-mascots04",
      "author": "Karl Brandt and Darrell D. E. Long and Ahmed Amer",
      "title": "Predicting When Not To Predict",
      "booktitle": "Proceedings of the Twelfth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2004,
      "month": "oct",
      "address": "Volendam, The Netherlands",
      "organization": "IEEE",
      "pages": "419--426"
    }
  },
  {
    "id": 48,
    "title": "Disk Scrubbing in Large Archival Storage Systems",
    "short_description": "",
    "full_content": "Large archival storage systems experience long periods of idleness broken up by rare data accesses. In such systems, disks may remain powered off for long periods of time. These systems can lose data for a variety of reasons, including failures at both the device level and the block level. To deal with these failures, we must detect them early enough to be able to use the redundancy built into the storage system. We propose a process called \"disk scrubbing\" in a system in which drives are periodically accessed to detect drive failure. By scrubbing all of the data stored on all of the disks, we can detect block failures and compensate for them by rebuilding the affected blocks. Our research shows how the scheduling of disk scrubbing affects overall system reliability, and that \"opportunistic\" scrubbing, in which the system scrubs disks only when they are powered on for other reasons, performs very well without the need to power on disks solely to check them.",
    "author": [
      "Thomas J. E. Schwarz",
      "Qin Xin",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Andrew Hospodor",
      "Spencer Ng"
    ],
    "date": "Oct 10, 2004",
    "url": "https://ssrc.us/media/pubs/d0a1a0161119593cfdb6235492f689bdf8c79b76.pdf",
    "bibTeX": {
      "@inproceedings": "schwarz-mascots04",
      "author": "Thomas J. E. Schwarz and Qin Xin and Ethan L. Miller and Darrell D. E. Long and Andrew Hospodor and Spencer Ng",
      "title": "Disk Scrubbing in Large Archival Storage Systems",
      "booktitle": "Proceedings of the Twelfth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2004,
      "month": "oct",
      "address": "Volendam, The Netherlands",
      "organization": "IEEE",
      "pages": "409--418"
    }
  },
  {
    "id": 49,
    "title": "Using MEMS-based Storage to Boost Disk Performance",
    "short_description": "",
    "full_content": "Non-volatile storage technologies such as flash memory, Magnetic RAM (MRAM), and MEMS-based storage are emerging as serious alternatives to disk drives. Among these, MEMS storage is predicted to be the least expensive and highest density, and at about 1 ms access times still considerably faster than hard disk drives. Like the other emerging non-volatile storage technologies, it will be highly suitable for small mobile devices but will, at least initially, be too expensive to replace hard drives entirely. Its non-volatility, dense storage, and high performance still makes it an ideal candidate for the secondary storage subsystem. We examine the use of MEMS storage in the storage hierarchy and show that using a technique called MEMS Caching Disk, we can achieve 30--49 by using only a small amount (3 capacity) of MEMS storage in conjunction with a standard hard drive. The resulting system is ideally suited for commercial packaging with a small MEMS device included as part of a standard disk controller or paired with a disk.",
    "author": [
      "Feng Wang",
      "Bo Hong",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Apr 11, 2005",
    "url": "https://ssrc.us/media/pubs/d8c4c91815536f170fc553a933e03f59f21ff7a6.pdf",
    "bibTeX": {
      "@inproceedings": "wang-msst05",
      "author": "Feng Wang and Bo Hong and Scott A. Brandt and Darrell D. E. Long",
      "title": "Using MEMS-based Storage to Boost Disk Performance",
      "booktitle": "Proceedings of the Twenty-second Conference on Mass Storage Systems and Technologies",
      "year": 2005,
      "month": "apr",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "202--209"
    }
  },
  {
    "id": 50,
    "title": "Deep Store: An Archival Storage System Architecture",
    "short_description": "",
    "full_content": "We present the Deep Store archival storage architecture, a large-scale storage system that stores immutable data efficiently and reliably for long periods of time. Archived data is stored across a cluster of nodes and recorded to hard disk. The design differentiates itself from traditional file systems by eliminating redundancy within and across files, distributing content for scalability, associating rich metadata with content, and using variable levels of replication based on the importance or degree of dependency of each piece of stored data. We evaluate the foundations of our design, including PRESIDIO, a virtual content-addressable storage framework with multiple methods for inter-file and intra-file compression that effectively addresses the data-dependent variability of data compression. We measure content and metadata storage efficiency, demonstrate the need for a variable-degree replication model, and provide preliminary results for storage performance.",
    "author": [
      "Lawrence L. You",
      "Kristal T. Pollack",
      "Darrell D. E. Long"
    ],
    "date": "Apr 15, 2005",
    "url": "https://ssrc.us/media/pubs/7015b2eb3c689db69837dddea1af32c8e30709d8.pdf",
    "bibTeX": {
      "@inproceedings": "you-icde05",
      "author": "Lawrence L. You and Kristal T. Pollack and Darrell D. E. Long",
      "title": "Deep Store: an Archival Storage System Architecture",
      "booktitle": "Proceedings of the Twenty-first International Conference on Data Engineering (ICDE)",
      "year": 2005,
      "month": "apr",
      "address": "Tokyo",
      "organization": "IEEE",
      "pages": "804--815"
    }
  },
  {
    "id": 51,
    "title": "Impact of Failure on Interconnection Networks in Large Storage Systems",
    "short_description": "",
    "full_content": "Recent advances in large-capacity, low-cost storage devices have led to active research in design of large-scale storage systems built from commodity devices for super-computing applications. Such storage systems, composed of thousands of storage devices, are required to provide high system bandwidth and petabyte-scale data storage. A robust network interconnection is essential to achieve high bandwidth, low latency, and reliable delivery during data transfers. However, failures, such as temporary link outages and node crashes, are inevitable. We discuss the impact of potential failures on network interconnections in very large-scale storage systems and analyze the trade-offs among several storage network topologies by simulations. Our results suggest that a good interconnect topology be essential to fault-tolerance of a petabyte-scale storage system.",
    "author": [
      "Qin Xin",
      "Ethan L. Miller",
      "Thomas J. E. Schwarz",
      "Darrell D. E. Long"
    ],
    "date": "Apr 20, 2005",
    "url": "https://ssrc.us/media/pubs/b5ba78d21ddfee517fc4b68648b8a8e0eae4b51c.pdf",
    "bibTeX": {
      "@inproceedings": "xin-msst05",
      "author": "Qin Xin and Ethan L. Miller and Thomas J. E. Schwarz and Darrell D. E. Long",
      "title": "Impact of Failure on Interconnection Networks in Large Storage Systems",
      "booktitle": "Proceedings of the Twenty-second Conference on Mass Storage Systems and Technologies",
      "year": 2005,
      "month": "apr",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "189--196"
    }
  },
  {
    "id": 52,
    "title": "Ceph: A Scalable Object-Based Storage System",
    "short_description": "",
    "full_content": "We have developed Ceph, a distributed file system that provides excellent performance, reliability, and scalability. Ceph maximizes the separation between data and metadata management by replacing allocation tables with a pseudo-random data distribution function (CRUSH) designed for heterogeneous and dynamic clusters of unreliable object storage devices (OSDs). We leverage device intelligence by distributing data replication, failure detection and recovery to semi-autonomous OSDs running a specialized local object file system. A dynamic distributed metadata cluster provides extremely efficient metadata management and seamlessly adapts to a wide range of general purpose and scientific computing file system workloads. Performance measurements under a variety of workloads show that Ceph has excellent I/O performance and scalable metadata management, supporting more than 250,000 metadata operations per second.",
    "author": [
      "Sage A. Weil",
      "Scott A. Brandt",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 2006",
    "url": "https://ssrc.us/media/pubs/80c09985415c15cb8be9ca91d85730027ad848e4.pdf",
    "bibTeX": {
      "@inproceedings": "weil-osdi06",
      "author": "Sage A. Weil and Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long",
      "title": "Ceph: A Scalable Object-Based Storage System",
      "booktitle": "Proceedings of the OSDI Workshop on Object Storage (WOS)",
      "year": 2006,
      "month": "nov",
      "address": "Seattle",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 53,
    "title": "Using MEMS-Based Storage in Computer Systems -- Device Modeling and Management",
    "short_description": "",
    "full_content": "MEMS-based storage is an emerging nonvolatile secondary storage technology. It promises high performance, high storage density, and low power consumption. With fundamentally different architectural designs from magnetic disk, MEMS-based storage exhibits unique two-dimensional positioning behaviors and efficient power state transitions. We model these low-level, device-specific properties of MEMS-based storage and present request scheduling algorithms and power management strategies that exploit the full potential of these devices. Our simulations show that MEMS-specific device management policies can significantly improve system performance and reduce power consumption. Categories and Subject Descriptors: D.4.2 [Storage Management] Secondary storage; D.4.8 [Performance] Modeling, Simulation General Terms: Algorithms, Management, Performance Additional Key Words and Phrases: MEMS-Based storage, request scheduling, power management",
    "author": [
      "Bo Hong",
      "Scott A. Brandt",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Ying Lin"
    ],
    "date": "May 1, 2006",
    "url": "https://ssrc.us/media/pubs/68a8dc82b2cfe3eec329a4f28bcf68b49513499c.pdf",
    "bibTeX": {
      "@article": "hong-tos06",
      "author": "Bo Hong and Scott A. Brandt and Darrell D. E. Long and Ethan L. Miller and Ying Lin",
      "title": "Using MEMS-Based Storage in Computer Systems -- Device Modeling and Management",
      "journal": "ACM Transactions on Storage",
      "volume": 2,
      "number": 2,
      "year": 2006,
      "month": "may",
      "pages": "139--160"
    }
  },
  {
    "id": 54,
    "title": "NVCache: Increasing the Effectiveness of Disk Spindown Algorithms with Caching",
    "short_description": "",
    "full_content": "Being one of the few mechanical components in a typical computer system, hard drives consume a significant amount of the overall power used by a computer. Spinning down a hard drive reduces its power consumption, but only works when no disk accesses occur, limiting overall effectiveness. We have designed and implemented a technique to extend disk spin-down times using a small non-volatile storage cache called NVCache, which contains a combination of caching techniques to service reads and writes while the hard disk is in low-power mode. We show that combining NVCache with an adaptive disk spin-down algorithm, a hard disk's power consumption can be reduced by up to 90",
    "author": [
      "Timothy Bisson",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 2006",
    "url": "https://ssrc.us/media/pubs/a08002805a8cadaf36fd9471a6e5fd62390979e1.pdf",
    "bibTeX": {
      "@inproceedings": "bisson-mascots06",
      "author": "Timothy Bisson and Scott A. Brandt and Darrell D. E. Long",
      "title": "NVCache: Increasing the Effectiveness of Disk Spin-down Algorithms with Caching",
      "booktitle": "Proceedings of the Fourteenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2006,
      "month": "sep",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "422--432"
    }
  },
  {
    "id": 55,
    "title": "Providing High Reliability in a Minimum Redundancy Archival Storage System",
    "short_description": "",
    "full_content": "Inter-file compression techniques store files as sets of references to data objects or chunks that can be shared among many files. While these techniques can achieve much better compression ratios than conventional intra-file compression methods such as Lempel-Ziv compression, they also reduce the reliability of the storage system because the loss of a few critical chunks can lead to the loss of many files. We show how to eliminate this problem by choosing for each chunk a replication level that is a function of the amount of data that would be lost if that chunk were lost. Experiments using actual archival data show that our technique can achieve significantly higher robustness than a conventional approach combining data mirroring and intra-file compression while requiring about half the storage space.",
    "author": [
      "Deepavali Bhagwat",
      "Kristal Pollack",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Thomas Schwarz, S.J.",
      "Jehan-François Pâris"
    ],
    "date": "Sep 13, 2006",
    "url": "https://ssrc.us/media/pubs/1bd42e936db567ec1c9a5d30b8ba7b46d4ad8a26.pdf",
    "bibTeX": {
      "@inproceedings": "bhagwat-mascots06",
      "author": "Deepavali Bhagwat and Kristal Pollack and Darrell D. E. Long and Ethan L. Miller and Thomas Schwarz, S.J. and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Providing High Reliability in a Minimum Redundancy Archival Storage System",
      "booktitle": "Proceedings of the Fourteenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2006,
      "month": "sep",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "412--421"
    }
  },
  {
    "id": 56,
    "title": "Adapting Predictions and Workloads for Power Management",
    "short_description": "",
    "full_content": "Power conservation in systems is critical for mobile, sensor network, and other power-constrained environments. While disk spin-down policies can contribute greatly to reducing the power consumption of the storage subsystem, the reshaping of the access workload can actively increase such energy savings. Traditionally reshaping of the access workload is a result of caches passively modifying the workload with the aim of increasing hit ratios and reducing access latency. In contrast, we present the a shifting predictive policy that actively reshapes the workload with the primary goal of conserving disk energy consumption. By reshaping the disk workload to explicitly lengthen idle periods, the disk can remain spun-down longer, saving more energy. We show that our approach can save up to 75 fixed-timeout spin-down policies. Our shifting algorithm dynamically shifts to the most energy efficient cache prefetching policy based on the current workload. This best shifting prefetching policy is shown to use 15 spin-down strategies and 5 the use of a fixed (non-shifting) prefetching policy.",
    "author": [
      "Jeffrey P. Rybczynski",
      "Darrell D. E. Long",
      "Ahmed Amer"
    ],
    "date": "Sep 14, 2006",
    "url": "https://ssrc.us/media/pubs/2a5d4c672e1a534d5bf1c0680837abcd2d7d775f.pdf",
    "bibTeX": {
      "@inproceedings": "rybczynski-mascots06",
      "author": "Jeffrey P. Rybczynski and Darrell D. E. Long and Ahmed Amer",
      "title": "Adapting Predictions and Workloads for Power Management",
      "booktitle": "Proceedings of the Fourteenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2006,
      "month": "sep",
      "address": "Monterey",
      "organization": "IEEE",
      "pages": "3--12"
    }
  },
  {
    "id": 58,
    "title": "Using Device Diversity to Protect Data against Batch-Correlated Disk Failures",
    "short_description": "",
    "full_content": "Batch-correlated failures result from the manifestation of a common defect in most, if not all, disk drives belonging to the same production batch. They are much less frequent than random disk failures but can cause catastrophic data losses even in systems that rely on mirroring or erasure codes to protect their data. We propose to reduce impact of batch-correlated failures on disk arrays by storing redundant copies of the same data on disks from different batches and, possibly, different manufacturers. The technique is especially attractive for mirrored organizations as it only requires that the two disks that hold copies of the same data never belong to the same production batch. We also show that even partial diversity can greatly increase the probability that the data stored in a RAID array will survive batch-correlated failures.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 2009",
    "url": "https://ssrc.us/media/pubs/31e34accfa6c62bd0d269bf6ca0ad60de631a7e9.pdf",
    "bibTeX": {
      "@inproceedings": "greenan-fast09",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Using Device Diversity to Protect Data against Batch-Correlated Disk Failures",
      "booktitle": "Poster Session of the USENIX Conference on File and Storage Technologies (FAST)",
      "year": 2009,
      "month": "feb",
      "address": "San Francisco",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 59,
    "title": "Ceph: A Scalable, High-Performance Distributed File System",
    "short_description": "",
    "full_content": "We have developed Ceph, a distributed file system that provides excellent performance, reliability, and scalability. Ceph maximizes the separation between data and metadata management by replacing allocation tables with a pseudo-random data distribution function (CRUSH) designed for heterogeneous and dynamic clusters of unreliable object storage devices (OSDs). We leverage device intelligence by distributing data replication, failure detection and recovery to semi-autonomous OSDs running a specialized local object file system. A dynamic distributed metadata cluster provides extremely efficient metadata management and seamlessly adapts to a wide range of general purpose and scientific computing file system workloads. Performance measurements under a variety of workloads show that Ceph has excellent I/O performance and scalable metadata management, supporting more than 250,000 metadata operations per second.",
    "author": [
      "Sage A. Weil",
      "Scott A. Brandt",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Carlos Maltzahn"
    ],
    "date": "Nov 6, 2006",
    "url": "https://ssrc.us/media/pubs/a7a45f7ef6849137936503f5b1cdabf103c5a661.pdf",
    "bibTeX": {
      "@inproceedings": "weil-osdi06",
      "author": "Sage A. Weil and Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long and Carlos Maltzahn",
      "title": "Ceph: A Scalable, High-Performance Distributed File System",
      "booktitle": "Proceedings of the Seventh Symposium on Operating Systems Design and Implementation (OSDI)",
      "year": 2006,
      "month": "nov",
      "address": "Seattle",
      "organization": "Usenix Association",
      "pages": "307--320"
    }
  },
  {
    "id": 60,
    "title": "Self-Adaptive Disk Arrays",
    "short_description": "",
    "full_content": "We present a disk array organization that adapts itself to successive disk failures. When all disks are operational, all data are mirrored on two disks. Whenever a disk fails, the array reorganizes itself, by selecting a disk containing redundant data and replacing these data by their exclusive or (XOR) with the other copy of the data contained on the disk that failed. This will protect the array against any single disk failure until the failed disk gets replaced and the array can revert to its original condition. Hence data will remain protected against the successive failures of up to one half of the original number of disks, provided that no critical disk failure happens while the array is reorganizing itself. As a result, our scheme achieves the same access times as a mirrored organization under normal operational conditions while having a much lower likelihood of loosing data under abnormal conditions. In addition, it tolerates much longer repair times than mirrored disk arrays.",
    "author": [
      "Jehan-François Pâris",
      "Thomas J. Schwarz, S.J.",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2006",
    "url": "https://ssrc.us/media/pubs/489b2ce20ab86e8c9e62d4fad4308e653417daea.pdf",
    "bibTeX": {
      "@inproceedings": "paris-sss06",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas J. Schwarz, S.J. and Darrell D. E. Long",
      "title": "Self-Adaptive Disk Arrays",
      "booktitle": "Proceedings of the Eighth International Symposium on Stabilization, Safety, and Security of Distributed Systems (SSS)",
      "year": 2006,
      "month": "nov",
      "address": "Dallas",
      "publisher": "Springer, Lecture Notes in Computer Science",
      "pages": "469--483"
    }
  },
  {
    "id": 61,
    "title": "Protecting Data Against Early Disk Failures",
    "short_description": "",
    "full_content": "Disk drives are known to fail at a higher rate during their first year of operation than during the remaining years of their useful lifetime. We propose to use the free space that normally exists on new disks to minimize the risk of data loss during that first year. Our technique applies to disk arrays that mirror their data on two disks. Whenever a disk fails, the array will reorganize itself by storing a new copy of the disk that failed on one or more disks that have free space. This will protect the data against any single disk failure until the failed disk gets replaced and the system reverts to its original state. A Markov analysis of the behavior of a small system consisting of two pairs of mirrored disks indicates that our technique can reduce the probability of a data loss during the first year of operation of the system by at least 75 percent provided the disks have 34 percent of spare space.",
    "author": [
      "Jehan-François Pâris",
      "Thomas J. Schwarz, S.J.",
      "Darrell D. E. Long"
    ],
    "date": "Dec 15, 2006",
    "url": "https://ssrc.us/media/pubs/417501a9844940233bea34b82988012342606779.pdf",
    "bibTeX": {
      "@inproceedings": "paris-i2ts06",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas J. Schwarz, S.J. and Darrell D. E. Long",
      "title": "Protecting Data Against Early Disk Failures",
      "booktitle": "Proceedings of the Fifth International Information and Telecommunication Technologies Symposium (I2TS)",
      "year": 2006,
      "month": "dec",
      "address": "Cuiab\\'a, Brazil",
      "pages": "113--119",
      "publisher": "IEEE"
    }
  },
  {
    "id": 62,
    "title": "Self-Adaptive Two-Dimensional RAID Arrays",
    "short_description": "",
    "full_content": "We propose increasing the survivability of data stored in two-dimensional RAID arrays by causing these arrays to reorganize themselves whenever they detect a disk failure. This reorganization will rebalance as much as possible the redundancy level of all stored data, thus reducing the potential impact of additional disk failures. It remains in effect until the failed disk gets repaired. We show how our technique can be applied to two-dimensional RAID arrays consisting of n^2 data disks and 2n parity disks and show how it can increase the mean time to data loss of the array by at least 200 percent as long as the reorganization process takes less than half the time it takes to replace a failed disk.",
    "author": [
      "Jehan-François Pâris",
      "Thomas J. Schwarz, S.J.",
      "Darrell D. E. Long"
    ],
    "date": "Apr 11, 2007",
    "url": "https://ssrc.us/media/pubs/2ecd1bbacf54c7173e461fabfeffbf9e83eb1573.pdf",
    "bibTeX": {
      "@inproceedings": "paris-ipccc07",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas J. Schwarz, S.J. and Darrell D. E. Long",
      "title": "Self-Adaptive Two-Dimensional RAID Arrays",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2007,
      "month": "apr",
      "address": "New Orleans",
      "organization": "IEEE"
    }
  },
  {
    "id": 63,
    "title": "A Hybrid Disk-Aware Spin-Down Algorithm with I/O Subsystem Support",
    "short_description": "",
    "full_content": "To offset the significant power demands of hard disk drives in computer systems, drives are typically powered down during idle periods. This saves power, but accelerates duty cycle consumption, leading to earlier drive failure. Hybrid disks with a small amount of non-volatile flash memory (NVCache) are coming on the market. We present four I/O subsystem enhancements that exploit the characteristics of hybrid disks to improve system performance: 1) Artificial Idle Periods, 2) a Read-Miss Cache, 3) Anticipatory Spin-Up, and 4) NVCache Write-Throttling. These enhancements reduce power consumption, duty cycling, NVCache block-erase impact, and the observed spin-up latency of a hybrid disk, resulting in lower power consumption, greater reliability, and faster I/O.",
    "author": [
      "Timothy Bisson",
      "Scott A. Brandt",
      "Darrell D. E. Long"
    ],
    "date": "Apr 11, 2007",
    "url": "https://ssrc.us/media/pubs/00bbfdfb354b8db0b4cc92bc9d44e447c48b19e1.pdf",
    "bibTeX": {
      "@inproceedings": "bisson-ipccc07",
      "author": "Timothy Bisson and Scott A. Brandt and Darrell D. E. Long",
      "title": "A Hybrid Disk-Aware Spin-Down Algorithm with I/O Subsystem Support",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2007,
      "month": "apr",
      "address": "New Orleans",
      "organization": "IEEE"
    }
  },
  {
    "id": 64,
    "title": "Quota Enforcement for High-Performance Distributed Storage Systems",
    "short_description": "",
    "full_content": "Storage systems manage quotas to ensure that no one user can use more than their share of storage, and that each user gets the storage they need. This is difficult for large, distributed systems, especially those used for high-performance computing applications, because resource allocation occurs on many nodes concurrently. While quota management is an important problem, no robust scalable solutions have been proposed to date. We present a solution that has less than 0.2 compared with not enforcing quota at all. It provides byte-level accuracy at all times, in the absence of failures and cheating. If nodes fail or cheat, we recover within a bounded period. In our scheme quota is enforced asynchronously by intelligent storage servers: storage clients contact a shared management service to obtain vouchers, which the clients can spend like cash at participating storage servers to allocate storage space. Like a digital cash system, the system periodically reconciles voucher usage to ensure that clients do not cheat by spending the same voucher at multiple storage servers. We report on a simulation study that validates this approach and evaluates its performance.",
    "author": [
      "Kristal T. Pollack",
      "Darrell D. E. Long",
      "Richard A. Golding",
      "Ralph A. Becker-Szendy",
      "Benjamin Reed"
    ],
    "date": "Sep 26, 2007",
    "url": "https://ssrc.us/media/pubs/810538e2f651e7a602c66f683e26baeb0e14f18f.pdf",
    "bibTeX": {
      "@inproceedings": "pollack-msst07",
      "author": "Kristal T. Pollack and Darrell D. E. Long and Richard A. Golding and Ralph A. Becker-Szendy and Benjamin Reed",
      "title": "Quota Enforcement for High-performance Distributed Storage Systems",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies",
      "year": 2007,
      "month": "sep",
      "address": "San Diego",
      "organization": "IEEE",
      "pages": "72--84"
    }
  },
  {
    "id": 65,
    "title": "Disaster Recovery Codes: Increasing Reliability with Large-Stripe Error Correction Codes",
    "short_description": "",
    "full_content": "Large-scale storage systems need to provide the right amount of redundancy in their storage scheme to protect client data. In particular, many high-performance systems require data protection that imposes minimal impact on performance; thus, such systems use mirroring to guard against data loss. Unfortunately, as the number of copies increases, mirroring becomes costly and contributes relatively little to the overall system reliability. Compared to mirroring, parity-based schemes are space-efficient, but incur greater update and degraded-mode read costs. An ideal data protection scheme should perform similarly to mirroring, while providing the space efficiency of a parity-based erasure code. Our goal is to increase the reliability of systems that currently mirror data for protection without impacting performance or space overhead. To this end, we propose the use of large parity codes across two-way mirrored reliability groups. The secondary reliability groups are defined across an arbitrarily large set of mirrored groups, necessitating a small amount of non-volatile RAM for parity. Since each parity element is stored in non-volatile RAM, our scheme drastically increases the mean time to data loss without impacting overall system performance.",
    "author": [
      "Kevin Greenan",
      "Ethan L. Miller",
      "Thomas Schwarz",
      "Darrell D. E. Long"
    ],
    "date": "Oct 29, 2007",
    "url": "https://ssrc.us/media/pubs/9ecab7b0d63fdb3455318123c3d7cd2e7fc1ca60.pdf",
    "bibTeX": {
      "@inproceedings": "greenan-storagess07",
      "author": "Kevin Greenan and Ethan L. Miller and Thomas Schwarz and Darrell D. E. Long",
      "title": "Disaster Recovery Codes: Increasing Reliability with Large-Stripe Error Correction Codes",
      "booktitle": "Proceedings of the Third International Workshop on Storage Security and Survivability (StorageSS)",
      "year": 2007,
      "month": "oct",
      "address": "Alexandria, Virginia",
      "organization": "ACM"
    }
  },
  {
    "id": 66,
    "title": "Ensuring Data Survival on Solid-State Storage Devices",
    "short_description": "",
    "full_content": "The emergence of more reliable, often much faster solid-stage storage devices is revolutionizing most aspects of data storage technology. Here we address the impact of these new storage devices on the techniques used to ensure data survival. In particular, we show that the higher bandwidth-to-capacity ratios of the new Storage Class Memory devices will make self- adaptive storage solutions much more attractive.",
    "author": [
      "Yangyang Huang",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Mar 1, 2010",
    "url": "https://ssrc.us/media/pubs/5177d775770337e41f3815753f0d2b42816ffe9a.pdf",
    "bibTeX": {
      "@inproceedings": "huang-nvmw10",
      "author": "Yangyang Huang and Darrell D. E. Long and Ethan L. Miller",
      "title": "Ensuring Data Survival on Solid-State Storage Devices",
      "booktitle": "Proceedings of the Non-Volatile Memories Workshop (NVMW)",
      "year": 2010,
      "month": "mar",
      "address": "San Diego",
      "publisher": "IEEE"
    }
  },
  {
    "id": 68,
    "title": "Increased Reliability with SSPiRAL Data Layouts",
    "short_description": "",
    "full_content": "We evaluate the reliability of storage system schemes consisting of an equal numbers of data disks and parity disks where each parity disk contains the exclusive or (XOR) of two or three of the data disks. These schemes are instances of Survivable Storage using Parity in Redundant Array Layouts (SSPiRAL). They have the same storage costs as mirrored organizations and use very simple parity schemes. Through a novel dynamic analysis of the likelihood of data losses, we show that these schemes are one hundred thousand to a million times less likely to lose data than a comparable mirrored organization. We also found that schemes where each parity disk contains the exclusive or of three data disks performed much better than schemes where each parity disk contains the exclusive or of only two data disks.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Jehan-François Pâris",
      "Thomas Schwarz"
    ],
    "date": "Sep 8, 2008",
    "url": "https://ssrc.us/media/pubs/83c957e36f53982d7ba0351e0677999b3764b689.pdf",
    "bibTeX": {
      "@inproceedings": "amer-mascots08",
      "author": "Ahmed Amer and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz",
      "title": "Increased Reliability with SSPiRAL Data Layouts",
      "booktitle": "Proceedings of the Sixteenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2008,
      "month": "sep",
      "address": "Baltimore",
      "organization": "IEEE",
      "pages": "189--198"
    }
  },
  {
    "id": 69,
    "title": "Secure Data Deduplication",
    "short_description": "",
    "full_content": "As the world moves to digital storage for archival purposes, there is an increasing demand for systems that can provide secure data storage in a cost-effective manner. By identifying common chunks of data both within and between files and storing them only once, deduplication can yield cost savings by increasing the utility of a given amount of storage. Unfortunately, deduplication exploits identical content, while encryption attempts to make all content appear random; the same content encrypted with two different keys results in very different ciphertext. Thus, combining the space efficiency of deduplication with the secrecy aspects of encryption is problematic. We have developed a solution that provides both data security and space efficiency in single-server storage and distributed storage systems. Encryption keys are generated in a consistent manner from the chunk data; thus, identical chunks will always encrypt to the same ciphertext. Furthermore, the keys cannot be deduced from the encrypted chunk data. Since the information each user needs to access and decrypt the chunks that make up a file is encrypted using a key known only to the user, even a full compromise of the system cannot reveal which chunks are used by which users.",
    "author": [
      "Mark W. Storer",
      "Kevin Greenan",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Oct 31, 2008",
    "url": "https://ssrc.us/media/pubs/9edc12c724b33d544db123004462843deb173edf.pdf",
    "bibTeX": {
      "@inproceedings": "storer-storagess08",
      "author": "Mark W. Storer and Kevin Greenan and Darrell D. E. Long and Ethan L. Miller",
      "title": "Secure Data Deduplication",
      "booktitle": "Proceedings of the Fourth International Workshop on Storage Security and Survivability (StorageSS)",
      "year": 2008,
      "month": "oct",
      "address": "Alexandria, Virginia",
      "organization": "ACM"
    }
  },
  {
    "id": 70,
    "title": "Logan: Automatic Management for Evolvable, Large-Scale, Archival Storage",
    "short_description": "",
    "full_content": "Archival storage systems designed to preserve scientific data, business data, and consumer data must maintain and safeguard tens to hundreds of petabytes of data on tens of thousands of media for decades. Such systems are currently designed in the same way as higher-performance, shorter-term storage systems, which have a useful lifetime but must be replaced in their entirety via a \"fork-lift\" upgrade. Thus, while existing solutions can provide good energy efficiency and relatively low cost, they do not adapt well to continuous improvements in technology, becoming less efficient relative to current technology as they age. In an archival storage environment, this paradigm implies an endless series of wholesale migrations and upgrades to remain efficient and up to date. Our approach, Logan, manages node addition, removal, and failure on a distributed network of intelligent storage appliances, allowing the system to gradually evolve as device technology advances. By automatically handling most of the common administration chores---integrating new devices into the system, managing groups of devices that work together to provide redundancy, and recovering from failed devices---Logan reduces management overhead and thus cost. Logan can also improve cost and space efficiency by identifying and decommissioning outdated devices, thus reducing space and power requirements for the archival storage system.",
    "author": [
      "Mark W. Storer",
      "Kevin M. Greenan",
      "Ian F. Adams",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Kaladhar Voruganti"
    ],
    "date": "Nov 17, 2008",
    "url": "https://ssrc.us/media/pubs/04fb9e71b120f3b6c53a6c4c69b690350ebc2897.pdf",
    "bibTeX": {
      "@inproceedings": "storer-pdsw08",
      "author": "Mark W. Storer and Kevin M. Greenan and Ian F. Adams and Ethan L. Miller and Darrell D. E. Long and Kaladhar Voruganti",
      "title": "Logan: Automatic Management for Evolvable, Large-Scale, Archival Storage Systems",
      "booktitle": "Proceedings of the Third Petascale Data Storage Workshop (PDSW)",
      "year": 2008,
      "month": "nov",
      "address": "Austin",
      "organization": "ACM"
    }
  },
  {
    "id": 71,
    "title": "A Spin-Up Saved is Energy Earned: Achieving Power-Efficient, Erasure-Coded Storage",
    "short_description": "",
    "full_content": "Storage accounts for a significant amount of a data center's ever increasing power budget. As a consequence, energy consumption has joined performance and reliability as a dominant metric in storage system design. In this paper, we show that the structure of an erasure code---which is generally used to provide data reliability---can be exploited to save power in a storage system. We define a novel technique in power-aware systems called power-aware coding and present generic techniques for reading, writing and activating devices in a power-aware, erasure-coded storage system. While our techniques have an effect on energy consumption, fault tolerance and performance, we focus on a few examples that illustrate the tradeoff between power efficiency and fault tolerance. Finally, we discuss open problems in the space of power-aware coding.",
    "author": [
      "Kevin M. Greenan",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Thomas J. E. Schwarz, S.J.",
      "Jay J. Wylie"
    ],
    "date": "Dec 7, 2008",
    "url": "https://ssrc.us/media/pubs/ec4209788a97532b732838320b54ee1e6434ae54.pdf",
    "bibTeX": {
      "@inproceedings": "greenan-hotdep08",
      "author": "Kevin M. Greenan and Darrell D. E. Long and Ethan L. Miller and Thomas J. E. Schwarz, S.J. and Jay J. Wylie",
      "title": "A Spin-Up Saved is Energy Earned: Achieving Power-Efficient, Erasure-Coded Storage",
      "booktitle": "Proceedings of Hot Topics in System Dependability (HotDep)",
      "year": 2008,
      "month": "dec",
      "address": "San Diego",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 72,
    "title": "Progressive Parity-Based Hardening of Data Stores",
    "short_description": "",
    "full_content": "We propose the use of parity-based redundant data layouts of increasing reliability as a means to progressively harden data archives. We evaluate the reliability of two such layouts and demonstrate how moving to layouts of higher parity degree offers a mechanism to progressively and dramatically increase the reliability of a multi-device data store. Specifically we propose that a data archive can be migrated to progressively more reliable layouts as the data ages, trading limited (and likely unrealized) increases in update costs for increased reliability. Our parity-based schemes are drawn from SSPiRAL (Survivable Storage using Parity in Redundant Array Layouts) that offer capacity efficiency equivalent to a straightforward mirroring arrangement. Our analysis shows our proposed schemes would utilize no additional physical resources and result in improvements to mean time to data loss of four to seven orders of magnitude.",
    "author": [
      "Ahmed Amer",
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Thomas Schwarz"
    ],
    "date": "Dec 14, 2008",
    "url": "https://ssrc.us/media/pubs/806db63f21c79d55881f1961f1d2238f1fddb222.pdf",
    "bibTeX": {
      "@inproceedings": "amer-ipccc08",
      "author": "Ahmed Amer and Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Thomas Schwarz",
      "title": "Progressive Parity-Based Hardening of Data Stores",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2008,
      "month": "dec",
      "address": "Austin",
      "organization": "IEEE",
      "pages": "34--42"
    }
  },
  {
    "id": 73,
    "title": "When MTTDLs Are Not Good Enough: Providing Better Estimates of Disk Array Reliability",
    "short_description": "",
    "full_content": "While mean time to data loss (MTTDL) provides an easy way to estimate the reliability of redundant disk arrays, it fails to take into account the relatively short lifetime of these arrays. We analyzed five different disk array organizations and compared the reliability estimates obtained using their mean times to data loss with the more exact values obtained by directly solving their corresponding Markov model. We observed that the conventional MTTDL approach generally provided a good estimate of the long-term reliability of arrays with the exception of non-repairable arrays while significantly underestimating the short-term reliability of these arrays.",
    "author": [
      "Jehan-François Pâris",
      "Thomas Schwarz",
      "Darrell D. E. Long",
      "Ahmed Amer"
    ],
    "date": "Dec 15, 2008",
    "url": "https://ssrc.us/media/pubs/b51479a2c778381473b9db678a69b70d893685da.pdf",
    "bibTeX": {
      "@inproceedings": "paris-i2ts08",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz and Darrell D. E. Long and Ahmed Amer",
      "title": "When MTTDLs Are Not Good Enough: Providing Better Estimates of Disk Array Reliability",
      "booktitle": "Proceedings of the Seventh International Information and Telecommunication Technologies Symposium (I2TS)",
      "year": 2008,
      "month": "dec",
      "address": "Foz do Igua\\c{c}u, Paran\\'a State, Brazil",
      "pages": "190--195",
      "publisher": "IEEE"
    }
  },
  {
    "id": 74,
    "title": "Protecting Against Rare Event Failures in Archival Systems",
    "short_description": "",
    "full_content": "Digital archives are growing rapidly, necessitating stronger reliability measures than RAID to avoid data loss from device failure. Mirroring, a popular solution, is too expensive over time. We present a compromise solution that uses multi-level redundancy coding to reduce the probability of data loss from multiple simultaneous device failures. This approach handles small-scale failures of one or two devices efficiently while still allowing the system to survive rare-event, larger-scale failures of four or more devices. In our approach, each disk is split into a set of fixed size disklets which are used to construct reliability stripes. To protect against rare event failures, reliability stripes are grouped into larger super-groups, each of which has a corresponding super-parity; super-parity is only used to recover data when disk failures overwhelm the redundancy in a single reliability stripe. Super-parity can be stored on a variety of devices such as NV-RAM and always-on disks to offset write bottlenecks while still keeping the number of active devices low. Our calculations of failure probabilities show that adding super-parity allows our system to absorb many more disk failures without data loss. Through discrete event simulation, we found that adding super-groups has a significant impact on mean time to data loss and that rebuilds are slow but not unmanageable. Finally, we showed that robustness against rare events can be achieved for a fraction of total system cost.",
    "author": [
      "Avani Wildani",
      "Thomas J. E. Schwarz",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Sep 21, 2009",
    "url": "https://ssrc.us/media/pubs/4970261ac6b15d9f3a3adbd7842a74b0ed50b44d.pdf",
    "bibTeX": {
      "@inproceedings": "wildani-mascots09",
      "author": "Avani Wildani and Thomas J. E. Schwarz and Ethan L. Miller and Darrell D. E. Long",
      "title": "Protecting Against Rare Event Failures in Archival Systems",
      "booktitle": "Proceedings of the Seventeenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2009,
      "month": "sep",
      "address": "London",
      "organization": "IEEE"
    }
  },
  {
    "id": 75,
    "title": "Using storage class memories to increase the reliability of two-dimensional RAID arrays",
    "short_description": "",
    "full_content": "Two-dimensional RAID arrays maintain separate row and column parities for all their disks. Depending on their organization, they can tolerate between two and three concurrent disk failures without losing any data. We propose to enhance the robustness of these arrays by replacing a small fraction of these drives with storage class memory devices, and demonstrate how such a pairing is several times more reliable than relying on conventional disks alone, or simply augmenting popular redundant layouts. Depending on the ratio of the failure rates of these two devices, the substitution can double or even triple the mean time to data loss (MTTDL) of each array.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Apr 17, 2009",
    "url": "https://ssrc.us/media/pubs/b5602a4af0428b1c2b2b1b03de68fc94e8aeff84.pdf",
    "bibTeX": {
      "@inproceedings": "paris-mascots09",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long",
      "title": "Using Storage Class Memories to Increase the Reliability of Two-dimensional RAID Arrays",
      "booktitle": "Proceedings of the Seventeenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2009,
      "month": "sep",
      "address": "London",
      "organization": "IEEE"
    }
  },
  {
    "id": 76,
    "title": "Reliability Modelling of Disk Subsystems with Probabilistic Model Checking",
    "short_description": "",
    "full_content": "",
    "author": [
      "K. Gopinath",
      "Jon Elerath",
      "Darrell Long"
    ],
    "date": "Sep 1, 2008",
    "url": "https://ssrc.us/media/pubs/b6074d5b97475e65a5c3082d4767797a479d108c.pdf",
    "bibTeX": {
      "@inproceedings": "greenan-qest08",
      "author": "K. Gopinath and Jon Elerath and Darrell Long",
      "title": "Reliability Modelling of Disk Subsystems with Probabilistic Model Checking",
      "booktitle": "Proceedings of the International Conference on Quantitative Evaluation of Systems (QEST)",
      "year": 2008,
      "month": "sep",
      "address": "St. Malo, France",
      "organization": "IEEE"
    }
  },
  {
    "id": 77,
    "title": "Maximizing Efficiency By Trading Storage for Computation",
    "short_description": "",
    "full_content": "Traditionally, computing has meant calculating results and then storing those results for later use. Unfortunately, committing large volumes of rarely used data to storage wastes space and energy, making it a very expensive strategy. Cloud computing, with its readily available and flexibly allocatable computing resources, suggests an alternative: storing the provenance data, and means to recomputing results as needed. While computation and storage are equivalent, finding the balance between the two that maximizes efficiency is difficult. One of the fundamental challenges of this issue is rooted in the knowledge gap separating the users and the cloud administrators---neither has a completely informed view. Users have a semantic understanding of their data, while administrators have an understanding of the cloud's underlying structure. We detail the user knowledge and system knowledge needed to construct a comprehensive cost model for analyzing the trade-off between storing a result and regenerating a result, allowing users and administrators to make an informed cost-benefit analysis.",
    "author": [
      "Ian F. Adams",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Jun 1, 2008",
    "url": "https://ssrc.us/media/pubs/148a4703eafdb2b708b144e8e49544d3476c4844.pdf",
    "bibTeX": {
      "@inproceedings": "storer-hotstorage08",
      "author": "Ian F. Adams and Darrell D. E. Long and Ethan L. Miller",
      "title": "Maximizing Efficiency By Trading Storage for Computation",
      "booktitle": "Proceedings of the USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage)",
      "year": 2008,
      "month": "jun",
      "address": "Boston",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 78,
    "title": "Building Flexible, Fault-Tolerant Flash-based Storage Systems",
    "short_description": "",
    "full_content": "Adding flash memory to the storage hierarchy has recently gained a great deal of attention in both industry and academia. Decreasing cost, low power utilization and improved performance has sparked this interest. Flash reliability is a weakness that must be overcome in order for the storage industry to fully adopt flash for persistent storage in mission-critical systems such as high-end storage controllers and low-power storage systems. We consider the unique reliability properties of NAND flash and present a high-level architecture for a reliable NAND flash memory storage system. The architecture manages erasure-coded stripes to increase reliability and operational lifetime of a flash memory-based storage system, while providing excellent write performance. Our analysis details the tradeoffs such a system can make, enabling the construction of highly reliable flash-based storage systems.",
    "author": [
      "Kevin M. Greenan",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Thomas J. E. Schwarz",
      "Avani Wildani"
    ],
    "date": "Jun 29, 2009",
    "url": "https://ssrc.us/media/pubs/85487ca09999d24c4f3d5c7148a8e1686d922147.pdf",
    "bibTeX": {
      "@inproceedings": "greenan-hotdep09",
      "author": "Kevin M. Greenan and Darrell D. E. Long and Ethan L. Miller and Thomas J. E. Schwarz and Avani Wildani",
      "title": "Building Flexible, Fault-Tolerant Flash-based Storage Systems",
      "booktitle": "Proceedings of Hot Topics in System Dependability (HotDep)",
      "year": 2009,
      "month": "jun",
      "address": "Lisbon",
      "organization": "IEEE"
    }
  },
  {
    "id": 79,
    "title": "Using Storage Class Memories to Increase the Reliability of Two-Dimensional RAID Arrays",
    "short_description": "",
    "full_content": "Digital archives are growing rapidly, necessitating stronger reliability measures than RAID to avoid data loss from device failure. Mirroring, a popular solution, is too expensive over time. We present a compromise solution that uses multi-level redundancy coding to reduce the probability of data loss from multiple simultaneous device failures. This approach handles small-scale failures of one or two devices efficiently while still allowing the system to survive rare-event, larger-scale failures of four or more devices. In our approach, each disk is split into a set of fixed size disklets which are used to construct reliability stripes. To protect against rare event failures, reliability stripes are grouped into larger super-groups, each of which has a corresponding super-parity; super-parity is only used to recover data when disk failures overwhelm the redundancy in a single reliability stripe. Super-parity can be stored on a variety of devices such as NV-RAM and always-on disks to offset write bottlenecks while still keeping the number of active devices low. Our calculations of failure probabilities show that adding super-parity allows our system to absorb many more disk failures without data loss. Through discrete event simulation, we found that adding super-groups has a significant impact on mean time to data loss and that rebuilds are slow but not unmanageable. Finally, we showed that robustness against rare events can be achieved for a fraction of total system cost.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Sep 21, 2009",
    "url": "https://ssrc.us/media/pubs/d4d1aadca8935513f3d409ef9cb5ed63b9087673.pdf",
    "bibTeX": {
      "@inproceedings": "paris-mascots09",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long",
      "title": "Using Storage Class Memories to Increase the Reliability of Two-dimensional RAID Arrays",
      "booktitle": "Proceedings of the Seventeenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2009,
      "month": "sep",
      "address": "London",
      "organization": "IEEE"
    }
  },
  {
    "id": 80,
    "title": "Extreme Binning: Scalable, Parallel Deduplication for Chunk-based File Backup",
    "short_description": "",
    "full_content": "Data deduplication is an essential and critical component of backup systems. Essential, because it reduces storage space requirements, and critical, because the performance of the entire backup operation depends on its throughput. Traditional backup workloads consist of large data streams with high locality, which existing deduplication techniques require to provide reasonable throughput. We present Extreme Binning, a scalable deduplication technique for non-traditional backup workloads that are made up of individual files with no locality among consecutive files in a given window of time. Due to lack of locality, existing techniques perform poorly on these workloads. Extreme Binning exploits file similarity instead of locality, and makes only one disk access for chunk lookup per file, which gives reasonable throughput. Multi-node backup systems built with Extreme Binning scale gracefully with the amount of input data; more backup nodes can be added to boost throughput. Each file is allocated using a stateless routing algorithm to only one node, allowing for maximum parallelization, and each backup node is autonomous with no dependency across nodes, making data management tasks robust with low overhead.",
    "author": [
      "Deepavali Bhagwat",
      "Kave Eshghi",
      "Darrell D. E. Long",
      "Mark Lillibridge"
    ],
    "date": "Sep 21, 2009",
    "url": "https://ssrc.us/media/pubs/b53c92a4725d8a656e4e9dcce3a7f03b5349d1e0.pdf",
    "bibTeX": {
      "@inproceedings": "bhagwat-mascots09",
      "author": "Deepavali Bhagwat and Kave Eshghi and Darrell D. E. Long and Mark Lillibridge",
      "title": "Extreme Binning: Scalable, Parallel Deduplication for Chunk-based File Backup",
      "booktitle": "Proceedings of the Seventeenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2009,
      "month": "sep",
      "address": "London",
      "organization": "IEEE",
      "pages": "237--245"
    }
  },
  {
    "id": 82,
    "title": "Evaluating the Impact of  Irrecoverable Read Errors on Disk Array Reliability",
    "short_description": "",
    "full_content": "We investigate the impact of irrecoverable read errors-also known as bad blocks-on the MTTDL of mirrored disks, RAID level 5 arrays and RAID level 6 arrays. Our study is based on the data collected by Bairavasundaram . from a population of 1.53 million disks over a period of 32 months. Our study indicates that irrecoverable read errors can reduce the mean time to data loss (MTTDL) of the three arrays by up to 99 percent, effectively canceling most of the benefits of fast disk repairs. It also shows the benefits of frequent scrubbing scans that map out bad blocks thus preventing future irrecoverable read errors. As an example, once-a-month scrubbing scans were found to improve the MTTDL of the three arrays by at least 300 percent compared to once-a-year scrubbing scans.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Thomas J. E. Schwarz"
    ],
    "date": "Nov 16, 2009",
    "url": "https://ssrc.us/media/pubs/d88bd54a99747e1c26b31f9501e872a9b9c5bf84.pdf",
    "bibTeX": {
      "@inproceedings": "paris-prdc09",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell D. E. Long and Thomas J. E. Schwarz",
      "title": "Evaluating the Impact of Irrecoverable Read Errors on Disk Array Reliability",
      "booktitle": "Proceedings of the IEEE Fifteenth Pacific Rim International Symposium on Dependable Computing (PRDC)",
      "year": 2009,
      "month": "nov",
      "address": "Shanghai, China",
      "organization": "IEEE"
    }
  },
  {
    "id": 83,
    "title": "Security Aware Partitioning for Efficient File System Search",
    "short_description": "",
    "full_content": "Index partitioning techniques---where indexes are broken into multiple distinct sub-indexes---are a proven way to improve metadata search speeds and scalability for large file systems, permitting early triage of the file system. A partitioned metadata index can rule out irrelevant files and quickly focus on files that are more likely to match the search criteria. Also, in a large file system that contains many users, a user's search should not include confidential files the user doesn't have permission to view. To meet these two parallel goals, we propose a new partitioning algorithm, Security Aware Partitioning, that integrates security with the partitioning method to enable efficient and secure file system search. In order to evaluate our claim of improved efficiency, we compare the results of Security Aware Partitioning to six other partitioning methods, including implementations of the metadata partitioning algorithms of SmartStore and Spyglass, two recent systems doing partitioned search in similar environments. We propose a general set of criteria for comparing partitioning algorithms, and use them to evaluate the partitioning algorithms. Our results show that Security Aware Partitioning can provide excellent search performance at a low computational cost to build indexes, O(n). Based on metrics such as information gain, we also conclude that expensive clustering algorithms do not offer enough benefit to make them worth the additional cost in time and memory.",
    "author": [
      "Aleatha Parker-Wood",
      "Christina Strong",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "May 6, 2010",
    "url": "https://ssrc.us/media/pubs/5503961c7ba8d36fed0d9146a9cb9276ad93f459.pdf",
    "bibTeX": {
      "@inproceedings": "parker-wood-msst10",
      "author": "Aleatha Parker-Wood and Christina Strong and Ethan L. Miller and Darrell D. E. Long",
      "title": "Security Aware Partitioning for Efficient File System Search",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies",
      "year": 2010,
      "month": "may",
      "address": "Incline Village, Nevada",
      "organization": "IEEE"
    }
  },
  {
    "id": 84,
    "title": "Design Issues for a Shingled Write Disk System",
    "short_description": "",
    "full_content": "If the data density of magnetic disks is to continue its current 30--50 the actively considered options, shingled writing is currently the most attractive one because it is the easiest to implement at the device level. Shingled write recording trades the inconvenience of the inability to update in-place for a much higher data density by a using a different write technique that overlaps the currently written track with the previous track. Random reads are still possible on such devices, but writes must be done largely sequentially. In this paper, we discuss possible changes to disk-based data structures that the adoption of shingled writing will require. We first explore disk structures that are optimized for large sequential writes with little or no sequential writing, even of metadata structures, while providing acceptable read performance. We also examine the usefulness of non-volatile RAM and the benefits of object-based interfaces in the context of shingled disks. Finally, through the analysis of recent device traces, we demonstrate the surprising stability of written device blocks, with general purpose workloads showing that more than 93 for more specialized workloads less than 0.5 would be needed to hold randomly updated blocks.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Jehan-François Pâris",
      "Thomas J. E. Schwarz"
    ],
    "date": "May 6, 2010",
    "url": "https://ssrc.us/media/pubs/8434c823fcb223a740f2dd59f213c4c5eb4d0960.pdf",
    "bibTeX": {
      "@inproceedings": "amer-msst10",
      "author": "Ahmed Amer and Darrell D. E. Long and Ethan L. Miller and Jehan-Fran\\c{c}ois P\\^aris and Thomas J. E. Schwarz",
      "title": "Design Issues for a Shingled Write Disk System",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies",
      "year": 2010,
      "month": "may",
      "address": "Incline Village, Nevada",
      "organization": "IEEE"
    }
  },
  {
    "id": 85,
    "title": "Improving Disk Array Reliability Through Expedited Scrubbing",
    "short_description": "",
    "full_content": "Disk scrubbing periodically scans the contents of a disk array to detect the presence of irrecoverable read errors and reconstitute the contents of the lost blocks using the built-in redundancy of the disk array. We address the issue of scheduling scrubbing runs in disk arrays that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole array whenever a disk failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of disk arrays over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twenty-four hour interval.",
    "author": [
      "Jehan-François Pâris",
      "Thomas J. E. Schwarz",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2010",
    "url": "https://ssrc.us/media/pubs/278033b73afed2d3a8fdb01ec46c3b79ce00d57b.pdf",
    "bibTeX": {
      "@inproceedings": "paris-nas10",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas J. E. Schwarz and Ahmed Amer and Darrell Long",
      "title": "Improving Disk Array Reliability Through Expedited Scrubbing",
      "booktitle": "Proceedings of the Fifth IEEE International Conference on Networking, Architecture, and Storage (NAS 2010)",
      "year": 2010,
      "month": "jul",
      "address": "Macau",
      "organization": "IEEE",
      "pages": "119--125"
    }
  },
  {
    "id": 87,
    "title": "Clasas: A Key-Store for the Cloud",
    "short_description": "",
    "full_content": "We propose Clasas (from the Castilian \"Claves seguras\" for \"secure keys\"), a key-store for distributed storage such in the Cloud. The security of Clasas derives from breaking keys into K shares and storing the key shares at many different sites. This provides both a probabilistic and a deterministic guarantee against an adversary trying to obtain keys. The probabilistic guarantee is based on a combinatorial explosion, which forces an adversary to subvert a very large portion of the storage sites for even a minute chance of obtaining a key. The deterministic guarantee stems from the use of LH* distributed linear hashing. Our use of the LH* addressing rules insures that no two key shares (belonging to the same key) are ever, even in transit, stored at the same site. Consequentially, an adversary has to subvert at least K sites. In addition, even an insider with extensive administrative privileges over many of the sites used for key storage is prevented from obtaining access to any key. Our key-store uses LH* or its scalable availability derivate, LH* _ RS to distribute key shares among a varying number of storage sites in a manner transparent to its users. While an adversary faces very high obstacles in obtaining a key, clients or authorized entities acting on their behalf can access keys with a very small number of messages, even if they do not know all sites where key shares are stored. This allows easy sharing of keys, re-keying, and key revocation.",
    "author": [
      "Thomas J. E. Schwarz",
      "Darrell D. E. Long"
    ],
    "date": "Aug 17, 2010",
    "url": "https://ssrc.us/media/pubs/6b56720e2d87218a4e0742c3dad7fd24dad83dd4.pdf",
    "bibTeX": {
      "@inproceedings": "schwarz-mascots10",
      "author": "Thomas J. E. Schwarz and Darrell Long",
      "title": "Clasas: A Key-Store for the Cloud",
      "booktitle": "Proceedings of the Eighteenth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2010,
      "month": "aug",
      "address": "Miami Beach",
      "organization": "IEEE"
    }
  },
  {
    "id": 88,
    "title": "Self-Adjusting Two-Failure Tolerant Disk Arrays",
    "short_description": "",
    "full_content": "We have presented a representation for a storage system with two failure tolerance based on flat XOR codes. We argue that this representation allows us to implement fast algorithm for the layout of very large, evolving disk arrays. Much needs to be done. Fast, but efficient algorithms for major changes in the disk array such as rack failure or insertion of new disks still need to be implemented and tested. Our goal is usually not to find an optimal layout (in a sense to be defined precisely), but one that is close to optimal. To assert that our algorithms perform at this level involves a more mathematical analysis of the consequences of failures in such an array to derive bounds on the robustness of optimal layouts, a task we have barely started. Nevertheless, the results we have indicate that the algorithms are quite effective and certainly fast and easy to implement. This presents definite progress over the true optimization (including looking for proven optimal designs) that can be done only for special, small cases and supports our pragmatic attitude.",
    "author": [
      "Ignacio Corderi",
      "Thomas Schwarz",
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Nov 15, 2010",
    "url": "https://ssrc.us/media/pubs/958ed1d376157c456f37cb2f7ab6a2e1e44b2fa1.pdf",
    "bibTeX": {
      "@inproceedings": "corderi-pdsw10",
      "author": "Ignacio Corderi and Thomas Schwarz and Ahmed Amer and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Self-Adjusting Two-Failure Tolerant Disk Arrays",
      "booktitle": "Proceedings of the Fifth Petascale Data Storage Workshop (PDSW)",
      "year": 2010,
      "month": "nov",
      "address": "New Orleans",
      "organization": "ACM"
    }
  },
  {
    "id": 89,
    "title": "Provenance Based Rebuild: Using Data Provenance to Improve Rebuild",
    "short_description": "",
    "full_content": "",
    "author": [
      "Kiran-Kumar Muniswamy-Reddy",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2011",
    "url": "https://ssrc.us/media/pubs/19130bd472435b4f8473a164f9ff73a964aa0120.pdf",
    "bibTeX": {
      "@inproceedings": "muniswamyreddy-mascots11",
      "author": "Kiran-Kumar Muniswamy-Reddy and Darrell D. E. Long",
      "title": "Provenance Based Rebuild: Using Data Provenance to Improve Rebuild",
      "booktitle": "Proceedings of the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2011,
      "month": "jul",
      "address": "Singapore",
      "organization": "IEEE"
    }
  },
  {
    "id": 90,
    "title": "Design and Evaluation of Oasis: An Active Storage Framework based on T10 OSD Standard",
    "short_description": "",
    "full_content": "In this paper, we present the design and performance evaluation of Oasis, an active storage framework for object-based storage systems that complies with the current T10 OSD standard. In contrast with previous work, Oasis has the following advantages. First, Oasis enables users to transparently process the OSD object and supports different processing granularity (from the single object to all the objects in the OSD) by extending the OSD object attribute page defined in the T10 OSD standard. Second, Oasis provides an easy and efficient way for users to manage the application functions in the OSD by using the existing OSD commands. Third, Oasis can authorize the execution of the application function in the OSD by enhancing the T10 OSD security protocol, allowing only authorized users to use the system. We evaluate the performance and scalability of our system implementation on Oasis by running three typical applications. The results indicate that active storage far outperforms the traditional object-based storage system in applications that filter data on the OSD. We also experiment with Java based applications and C based applications. Our experiments indicate that Java based applications may be bottlenecked for I/O-intensive applications, while for applications that do not heavily rely on the I/O operations, both Java based applications and C based applications achieve comparable performance. Our micro-benchmarks indicate that Oasis implementation overhead is minimal compared to the Intel OSD reference implementation, between 1.2 to 9.9",
    "author": [
      "Yulai Xie",
      "Kiran-Kumar Muniswamy-Reddy",
      "Dan Feng",
      "Darrell D. E. Long",
      "Yangwook Kang",
      "Zhongying Niu",
      "Zhipeng Tan"
    ],
    "date": "May 27, 2011",
    "url": "https://ssrc.us/media/pubs/a46f415ca8f67861f1e87147fd12f6ed5267d2f4.pdf",
    "bibTeX": {
      "@inproceedings": "xie-msst11",
      "author": "Yulai Xie and Kiran-Kumar Muniswamy-Reddy and Dan Feng and Darrell D. E. Long and Yangwook Kang and Zhongying Niu and Zhipeng Tan",
      "title": "Design and Evaluation of Oasis: An Active Storage Framework based on T10 OSD Standard",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2011,
      "month": "may",
      "address": "Denver, Colorado",
      "organization": "IEEE"
    }
  },
  {
    "id": 91,
    "title": "Compressing Provenance Graphs",
    "short_description": "",
    "full_content": "The provenance community has built a number of systems to collect provenance, most of which assume that provenance will be retained indefinitely. However, it is not cost-effective to retain provenance information inefficiently. Since provenance can be viewed as a graph, we note the similarities to web graphs and draw upon techniques from the web compression domain to provide our own novel and improved graph compression solutions for provenance graphs. Our preliminary results show that adapting web compression techniques results in a compression ratio of 2.12:1 to 2.71:1, which we can improve upon to reach ratios of up to 3.31:1.",
    "author": [
      "Yulai Xie",
      "Kiran-Kumar Muniswamy-Reddy",
      "Darrell D. E. Long",
      "Ahmed Amer",
      "Dan Feng",
      "Zhipeng Tan"
    ],
    "date": "Jun 20, 2011",
    "url": "https://ssrc.us/media/pubs/1b0fc490ef2493f79cc67f2409570e75f4f64b3d.pdf",
    "bibTeX": {
      "@inproceedings": "xie-tapp11",
      "author": "Yulai Xie and Kiran-Kumar Muniswamy-Reddy and Darrell D. E. Long and Ahmed Amer and Dan Feng and Zhipeng Tan",
      "title": "Compressing Provenance Graphs",
      "booktitle": "Proceedings of the Workshop on the Theory and Practice of Provenance (TaPP)",
      "year": 2011,
      "month": "jun",
      "address": "Crete",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 92,
    "title": "Tracking Emigrant File Data via Transient Provenance",
    "short_description": "",
    "full_content": "",
    "author": [
      "Stephanie N. Jones",
      "Christina R. Strong",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Jun 20, 2011",
    "url": "https://ssrc.us/media/pubs/5877b5c8b0dd2d70a70a54c4121cdecd283913ff.pdf",
    "bibTeX": {
      "@inproceedings": "jones-tapp11",
      "author": "Stephanie N. Jones and Christina R. Strong and Darrell D. E. Long and Ethan L. Miller",
      "title": "Tracking Emigrant File Data via Transient Provenance",
      "booktitle": "Proceedings of the Workshop on the Theory and Practice of Provenance (TaPP)",
      "year": 2011,
      "month": "jun",
      "address": "Crete",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 93,
    "title": "PRESIDIO: A Framework for Efficient Archival Data Storage",
    "short_description": "",
    "full_content": "The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, high-efficiency disk-based storage systems. However, managed disk storage cost is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy is for compressing data is, given diverse collections of data. To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficient storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object. The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data. Categories and Subject Descriptors: D.4.2 [Operating Systems]: Storage Management: E.2 [Data]: Data Storage Representations--Object representation; E.5 [Data]: Files--Organization/structure General Terms: Algorithms, Design Additional Key Words and Phrases: Archival storage systems, data compression, content-addressable storage, CAS, progressive compression Reference Format: You, L. L., Pollack, K. T., Long, D. D. E., and Gopinath, K. 2011. PRESIDIO: A framework for efficient archival data storage. ACM Trans. Storage 7, 2, Article 6 (July 2011), 60 pages. DOI = 10.1145/1970348.1970351 http://doi.acm.org/10.1145/1970348.1970351 This work was conducted by all the authors at the University of California, Santa Cruz, and includes part of L. L. You's dissertation. Authors' addresses: L. L. You, Google Inc., Mountain View, CA 94043; K. T. Pollack; email: kristal@cs.ucsc.edu; D. D. E. Long, Storage Systems Research Center, University of California, Santa Cruz, CA 95064; K. Gopinath, Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560 012, India; email: gopi@csa.iisc.ernet.in. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permission may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, USA, fax +1 (212) 869-0481, or permissions@acm.org. 2011 ACM 1553-3077/2011/07-ART6 10.00 DOI 10.1145/1970348.1970351 http://doi.acm.org/10.1145/1970348.1970351",
    "author": [
      "Lawrence L. You",
      "Kristal T. Pollack",
      "Darrell D. E. Long",
      "K. Gopinath"
    ],
    "date": "Jul 1, 2011",
    "url": "https://ssrc.us/media/pubs/33ef15dd54e7d4175fe554cff674cb6e386038c6.pdf",
    "bibTeX": {
      "@article": "you-tos11",
      "author": "Lawrence L. You and Kristal T. Pollack and Darrell D. E. Long and K. Gopinath",
      "title": "PRESIDIO: A Framework for Efficient Archival Data Storage",
      "journal": "ACM Transactions on Storage",
      "volume": 7,
      "number": 2,
      "year": 2011,
      "month": "jul",
      "pages": "1--60"
    }
  },
  {
    "id": 94,
    "title": "Los Alamos National Laboratory Interviews",
    "short_description": "",
    "full_content": "Efficient metadata management is a critical aspect of overall system performance in large distributed storage systems. Directory subtree partitioning and pure hashing are two common techniques used for managing metadata in such systems, but both suffer from bottlenecks at very high concurrent access rates. We present a new approach called Lazy Hybrid (LH) metadata management that combines the best aspects of these two approaches while avoiding their shortcomings.",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "Sep 12, 2011",
    "url": "https://ssrc.us/media/pubs/cd2f2a372894d912db4bac074535b77a0ebb5c38.pdf",
    "bibTeX": {
      "@misc": "lanl-interview",
      "author": "Darrell D. E. Long",
      "title": "Los Alamos National Laboratory Interviews",
      "year": 2015
    }
  },
  {
    "id": 95,
    "title": "Data Management and Layout for Shingled Magnetic Recording",
    "short_description": "",
    "full_content": "Ultimately the performance and success of a shingled write disk (SWD) will be determined by more than the physical hardware realized, but will depend on the data layouts employed, the workloads experienced, and the architecture of the overall system, including the level of interfaces provided by the devices to higher levels of system software. While we discuss several alternative layouts for use with SWD, we also discuss the dramatic implications of observed workloads. Example data access traces demonstrate the surprising stability of written device blocks, with a small fraction requiring multiple updates (the problematic operation for a shingled-write device). Specifically we discuss how general purpose workloads can show that more than 93",
    "author": [
      "Ahmed Amer",
      "JoAnne Holliday",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Jehan-François Pâris",
      "Thomas Schwarz, S.J."
    ],
    "date": "Oct 15, 2011",
    "url": "https://ssrc.us/media/pubs/fa661ead680269180527ae691db7e725299eacce.pdf",
    "bibTeX": {
      "@article": "amer-tmag11",
      "author": "Ahmed Amer and JoAnne Holliday and Darrell D. E. Long and Ethan L. Miller and Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz, S.J.",
      "title": "Data Management and Layout for Shingled Magnetic Recording",
      "journal": "IEEE Transactions on Magnetics",
      "volume": 47,
      "number": 10,
      "year": 2011,
      "month": "oct",
      "pages": "3691--3697"
    }
  },
  {
    "id": 96,
    "title": "Easing the Burdens of HPC File Management",
    "short_description": "",
    "full_content": "While the amount of data we can process and store grows, our ability to find data remains dependent upon our own memories more often than not. Manual metadata management is common among scientific users, consuming their time while not making use of the computing resources at hand. Our system design proposes to empower users with more powerful data finding tools, such as unified search spaces, provenance, and ranked file system search. By returning the responsibility of file management to the file system, we enable scientists to focus on their science without the need for a customized file organization scheme for their work.",
    "author": [
      "Stephanie N. Jones",
      "Christina R. Strong",
      "Aleatha Parker-Wood",
      "Alexandra Holloway",
      "Darrell D. E. Long"
    ],
    "date": "Nov 6, 2011",
    "url": "https://ssrc.us/media/pubs/ab95dce7cd57c2fb7654ca6b0c0e8b920472848a.pdf",
    "bibTeX": {
      "@inproceedings": "jones-pdsw11",
      "author": "Stephanie N. Jones and Christina R. Strong and Aleatha Parker-Wood and Alexandra Holloway and Darrell D. E. Long",
      "title": "Easing the Burdens of HPC File Management",
      "booktitle": "Proceedings of the Fifth Petascale Data Storage Workshop (PDSW)",
      "year": 2011,
      "month": "nov",
      "address": "Seattle",
      "organization": "ACM"
    }
  },
  {
    "id": 97,
    "title": "Horus: Fine-Grained Encryption-Based Security for Large-Scale Storage",
    "short_description": "",
    "full_content": "With the growing use of large-scale distributed systems, the likelihood that at least one node is compromised is increasing. Large-scale systems that process sensitive data such as geographic data with defense implications, drug modeling, nuclear explosion modeling, and private genomic data would benefit greatly from strong security for their storage. Nevertheless, many high performance computing (HPC), cloud, or secure content delivery network (SCDN) systems that handle such data still store them unencrypted or use simple encryption schemes, relying heavily on physical isolation to ensure confidentiality, providing little protection against compromised computers or malicious insiders. Moreover, current encryption solutions cannot efficiently provide fine-grained encryption for large datasets. Our approach, Horus, encrypts large datasets using keyed hash trees (KHTs) to generate different keys for each region of the dataset, providing fine-grained security: the key for one region cannot be used to access another region. Horus also reduces key management and distribution overhead while providing end-to-end data encryption and reducing the need to trust system operators or cloud service providers.",
    "author": [
      "Ranjana Rajendran",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Nov 13, 2011",
    "url": "https://ssrc.us/media/pubs/8951dfeb2e0dfaa788b496c0be1f1f260137ad7d.pdf",
    "bibTeX": {
      "@inproceedings": "rajendran-pdsw11",
      "author": "Ranjana Rajendran and Ethan L. Miller and Darrell D. E. Long",
      "title": "Horus: Fine-Grained Encryption-Based Security for Large-Scale Storage",
      "booktitle": "Proceedings of the Sixth Petascale Data Storage Workshop (PDSW)",
      "year": 2011,
      "month": "nov",
      "address": "Seattle",
      "organization": "ACM"
    }
  },
  {
    "id": 98,
    "title": "Combining Chunk Boundary and Chunk Signature Calculations for Deduplication",
    "short_description": "",
    "full_content": "Many modern, large-scale storage solutions offer deduplication, which can achieve impressive compression rates for many loads, especially for backups. When accepting new data for storage, deduplication checks whether parts of the data is already stored. If this is the case, then the system does not store that part of the new data but replaces it with a reference to the location where the data already resides. A typical deduplication system breaks data into chunks, hashes each chunk, and uses an index to see whether the chunk has already been stored. Variable chunk systems offer better compression, but process data byte-for-byte twice, first to calculate the chunk boundaries and then to calculate the hash. This limits the ingress bandwidth of a system. We propose a method to reuse the chunk boundary calculations in order to strengthen the collision resistance of the hash, allowing us to use a faster hashing method with fewer bytes or a much larger (256 times by adding two bytes) storage system with the same high assurance against chunk collision and resulting data loss.",
    "author": [
      "Witold Litwin",
      "Darrell D. E. Long",
      "Thomas Schwarz"
    ],
    "date": "Jan 1, 2012",
    "url": "https://ssrc.us/media/pubs/049bda82fad21815ebfdde1f3ca653287009b929.pdf",
    "bibTeX": {
      "@article": "litwin-lat12",
      "author": "Witold Litwin and Darrell Long and Thomas Schwarz",
      "title": "Combining Chunk Boundary and Chunk Signature Calculations for Deduplication",
      "journal": "IEEE Latin America Transactions",
      "volume": 10,
      "number": 1,
      "year": 2012,
      "month": "jan",
      "pages": "1305--1311"
    }
  },
  {
    "id": 99,
    "title": "Making Sense of File Systems Through Provenance and Rich Metadata",
    "short_description": "",
    "full_content": "",
    "author": [
      "Aleatha Parker-Wood",
      "Stephanie N. Jones",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2011",
    "url": "https://ssrc.us/media/pubs/32f01eab601b8048db6a76325e8f0db6dc6b4cfe.pdf",
    "bibTeX": {
      "@inproceedings": "parkerwood-pdsw11",
      "author": "Aleatha Parker-Wood and Stephanie N. Jones and Darrell D. E. Long",
      "title": "Making Sense of File Systems Through Provenance and Rich Metadata",
      "booktitle": "Proceedings of the Sixth Petascale Data Storage Workshop (PDSW)",
      "year": 2011,
      "month": "nov",
      "address": "Seattle",
      "organization": "ACM"
    }
  },
  {
    "id": 100,
    "title": "Computer Hard Drive Geolocation by HTTP Feature Extraction",
    "short_description": "",
    "full_content": "",
    "author": [
      "Austin Hendricks",
      "Joel Coffman",
      "Darrell D. E. Long"
    ],
    "date": "Dec 1, 2014",
    "url": "https://ssrc.us/media/pubs/c81a0bc54a2e58e00660bae349fe59bcfed8a7b2.pdf",
    "bibTeX": {
      "@inproceedings": "hendricks-acsac14",
      "author": "Austin Hendricks and Joel Coffman and Darrell D. E. Long",
      "title": "Computer Hard Drive Geolocation by HTTP Feature Extraction",
      "booktitle": "Proceedings of the Annual Computer Security Applications Conference (ACSAC)",
      "year": 2014,
      "month": "dec",
      "address": "New Orleans",
      "organization": "ACM"
    }
  },
  {
    "id": 101,
    "title": "Understanding Data Survivability in Archival Storage Systems",
    "short_description": "",
    "full_content": "Preserving data for a long period of time in the face of faults, large and small, is crucial for designing reliable archival storage systems. However, the survivability of data is different from the reliability of storage because typically, data are stored in more than one storage at a given moment. Previous studies of reliability ignore the former. We present a framework for relating data survivability and storage reliability, and use the framework to gauge the impact of rare but large-scale events on data survivability. We also present a method to track all copies of data and the condition of all the online and offline media, devices and systems on which they are stored uninterruptedly over the whole lifetime of the data. With this method, the survivability of the data can be closely monitored, and potential dangers can be handled in a timely manner. A better understanding of data survivability can be used in reducing unnecessary data replicas, thus reducing the cost.",
    "author": [
      "Yan Li",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Jun 4, 2012",
    "url": "https://ssrc.us/media/pubs/41ba07bff60028bf2d1ce5ed11f975340413f4cd.pdf",
    "bibTeX": {
      "@inproceedings": "li-systor12",
      "author": "Yan Li and Ethan L. Miller and Darrell D. E. Long",
      "title": "Understanding Data Survivability in Archival Storage Systems",
      "booktitle": "Proceedings of the Fifth Annual International Systems and Storage Conference (SYSTOR)",
      "year": 2012,
      "month": "jun",
      "address": "Haifa, Israel",
      "organization": "ACM"
    }
  },
  {
    "id": 102,
    "title": "Exploring Trusted Networking for Protected Applications",
    "short_description": "",
    "full_content": "",
    "author": [
      "Darrell D. E. Long",
      "Thomas J. E. Schwarz",
      "S. Felix Wu"
    ],
    "date": "Sep 1, 2009",
    "url": "https://ssrc.us/media/pubs/74023ead4c4af866237377b17a526f5e61e45ebd.pdf",
    "bibTeX": {
      "@inproceedings": "long-cns09",
      "author": "Darrell D. E. Long and Thomas J. E. Schwarz and S. Felix Wu",
      "title": "Exploring Trusted Networking for Protected Applications",
      "booktitle": "Proceedings of the Conference on Communications and Network Security (CNS)",
      "year": 2009,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 103,
    "title": "Emulating a Shingled Write Disk",
    "short_description": "",
    "full_content": "Shingled Magnetic Recording technology is expected to play a major role in the next generation of hard disk drives. But it introduces some unique challenges to system software researchers and prototype hardware is not readily available for the broader research community. It is crucial to work on system software in parallel to hardware manufacturing, to ensure successful and effective adoption of this technology. In this work, we present a novel Shingled Write Disk (SWD) emulator that uses a hard disk utilizing traditional Perpendicular Magnetic Recording (PMR) and emulates a Shingled Write Disk on top of it. We implemented the emulator as a pseudo block device driver and evaluated the performance overhead incurred by employing the emulator. The emulator has a slight overhead which is only measurable during pure sequential reads and writes. The moment disk head movement comes into picture, due to any random access, the emulator overhead becomes so insignificant as to become immeasurable.",
    "author": [
      "Rekha Pitchumani",
      "Andy Hospodor",
      "Ahmed Amer",
      "Yangwook Kang",
      "Ethan L. Miller",
      "Darrell Long"
    ],
    "date": "Aug 6, 2012",
    "url": "https://ssrc.us/media/pubs/8e8dfcab3d23cba2f60a8df9ffdf6c7442fa953b.pdf",
    "bibTeX": {
      "@inproceedings": "pitchumani-mascots12",
      "author": "Rekha Pitchumani and Andy Hospodor and Ahmed Amer and Yangwook Kang and Ethan L. Miller and Darrell Long",
      "title": "Emulating a Shingled Write Disk",
      "booktitle": "Proceedings of the Twentieth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
      "year": 2012,
      "month": "aug",
      "address": "Washington, DC",
      "organization": "IEEE"
    }
  },
  {
    "id": 104,
    "title": "PERSES: Data Layout for Low Impact Failures",
    "short_description": "",
    "full_content": "Growth in disk capacity continues to outpace advances in read speed and device reliability. This has led to storage systems spending increasing amounts of time in a degraded state while failed disks reconstruct. Users and applications that do not use the data on the failed or degraded drives are negligibly impacted by the failure, increasing the perceived performance of the system. We leverage this observation with PERSES, a statistical data allocation scheme to reduce the performance impact of failures by clustering data on disks such that data with high probability of co-access is placed on the same device as often as possible.",
    "author": [
      "Avani Wildani",
      "Ethan L. Miller",
      "Ian F. Adams",
      "Darrell D. E. Long"
    ],
    "date": "Sep 28, 2012",
    "url": "https://ssrc.us/media/pubs/269379e9c89be1f8ff0d24bf64c6fa30a6e15d47.pdf",
    "bibTeX": {
      "@inproceedings": "wildani-mascots14",
      "author": "Avani Wildani and Ethan L. Miller and Ian F. Adams and Darrell D. E. Long",
      "title": "PERSES: Data Layout for Low Impact Failures",
      "booktitle": "Proceedings of the Twenty-second International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2014)",
      "year": 2014,
      "month": "sep",
      "address": "Paris",
      "organization": "IEEE"
    }
  },
  {
    "id": 105,
    "title": "A Hybrid Approach for Efficient Provenance Storage",
    "short_description": "",
    "full_content": "Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the provenance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compression time and query performance when compared to other compression schemes. Categories and Subject Descriptors E.4[Coding and Information Theory]: Data compaction and compression; H.3.2[Information Storage and Retrieval]: Information Storage - File organization",
    "author": [
      "Yulai Xie",
      "Kiran-Kumar Muniswamy-Reddy",
      "Dan Feng",
      "Yan Li",
      "Darrell D. E. Long",
      "Zhipeng Tan",
      "Lei Chen"
    ],
    "date": "Oct 29, 2012",
    "url": "https://ssrc.us/media/pubs/1c86fd98ff7b0fe1da0298746532ce56be7914f3.pdf",
    "bibTeX": {
      "@inproceedings": "xie-cikm12",
      "author": "Yulai Xie and Kiran-Kumar Muniswamy-Reddy and Dan Feng and Yan Li and Darrell D. E. Long and Zhipeng Tan and Lei Chen",
      "title": "A Hybrid Approach for Efficient Provenance Storage",
      "booktitle": "Proceedings of International Conference on Information and Knowledge Management (CIKM)",
      "year": 2012,
      "month": "oct",
      "address": "Maui",
      "organization": "ACM"
    }
  },
  {
    "id": 106,
    "title": "Highly Reliable Two-Dimensional RAID Arrays for Archival Storage",
    "short_description": "",
    "full_content": "We present a two-dimensional RAID architecture that is specifically tailored to the needs of archival storage systems. Our proposal starts with a fairly conventional two-dimensional RAID architecture where each disk belongs to exactly one horizontal and one vertical RAID level 4 stripe. Once the array has been populated, we add a superparity device that contains the exclusive OR of all the contents of all horizontal-or vertical-parity disks. The new organization tolerates all triple disk failures and nearly all quadruple and quintuple disk failures. As a result, it provides mean times to data loss (MTTDLs) more than a hundred times better than those of sets of RAID level 6 stripes with equal capacity and similar parity overhead.",
    "author": [
      "Jehan-François Pâris",
      "Thomas Schwarz, S.J.",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Dec 1, 2012",
    "url": "https://ssrc.us/media/pubs/797505c7e2d985c98fc071183f30a6234e2f6e76.pdf",
    "bibTeX": {
      "@inproceedings": "paris-ipccc12",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz, S.J. and Ahmed Amer and Darrell D. E. Long",
      "title": "Highly Reliable Two-Dimensional RAID Arrays for Archival Storage",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2012,
      "month": "dec",
      "address": "Austin",
      "organization": "IEEE"
    }
  },
  {
    "id": 107,
    "title": "Improved Deduplication through Parallel Binning",
    "short_description": "",
    "full_content": "Many modern storage systems use deduplication in order to compress data by avoiding storing the same data twice. Deduplication needs to use data stored in the past, but accessing information about all data stored can cause a severe bottleneck. Similarity based deduplication only accesses information on past data that is likely to be similar and thus more likely to yield good deduplication. We present an adaptive deduplication strategy that extends Extreme Binning and investigate theoretically and experimentally the effects of the additional bin accesses.",
    "author": [
      "Zhike Zhang",
      "Deepavali Bhagwat",
      "Witold Litwin",
      "Darrell Long",
      "Thomas Schwarz, S.J."
    ],
    "date": "Dec 1, 2012",
    "url": "https://ssrc.us/media/pubs/e5318725a703fca005efbefc1df99c14d9f7a8d2.pdf",
    "bibTeX": {
      "@inproceedings": "zhang-ipccc12",
      "author": "Zhike Zhang and Deepavali Bhagwat and Witold Litwin and Darrell Long and Thomas Schwarz, S.J.",
      "title": "Improved Deduplication through Parallel Binning",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication (IPCCC)",
      "year": 2012,
      "month": "dec",
      "address": "Austin",
      "organization": "IEEE"
    }
  },
  {
    "id": 108,
    "title": "Examining Extended and Scientific Metadata for Scalable Index Designs",
    "short_description": "",
    "full_content": "While file system metadata is well characterized by a variety of workload studies, scientific metadata is much less well understood. We characterize scientific metadata, in order to better understand the implications for index design. Based on our findings, existing solutions for either file system or scientific search will not suffice for indexing a large scientific file system. We describe the problems with existing solutions, and suggest column stores as an alternative approach.",
    "author": [
      "Aleatha Parker-Wood",
      "Brian Madden",
      "Michael McThrow",
      "Darrell D. E. Long",
      "Ian F. Adams",
      "Avani Wildani"
    ],
    "date": "Dec 14, 2012",
    "url": "https://ssrc.us/media/pubs/dd9e7db2b76fc5e150eb442361114ed7b74cefe7.pdf",
    "bibTeX": {
      "@inproceedings": "parker-wood-systor13",
      "author": "Aleatha Parker-Wood and Brian Madden and Michael McThrow and Darrell D.E. Long and Ian F. Adams and Avani Wildani",
      "title": "Examining Extended and Scientific Metadata for Scalable Index Designs",
      "booktitle": "Proceedings of the Sixth Annual International Systems and Storage Conference (SYSTOR)",
      "year": 2013,
      "month": "jun",
      "address": "Haifa, Israel",
      "organization": "ACM"
    }
  },
  {
    "id": 109,
    "title": "Simple, Exact Placement of Data in Containers",
    "short_description": "",
    "full_content": "Storage and other systems frequently need to distribute objects equally over several sites or devices. While this is simple for a static system, organizing the distribution when additional containers (for example, hard drives, or web content delivery sites) become available is difficult. We present here a very simple scheme based on the factorial number system that allows equal, dynamic distribution of mirrored or replicated objects.",
    "author": [
      "Thomas Schwarz",
      "Ignacio Corderi",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Jan 28, 2013",
    "url": "https://ssrc.us/media/pubs/5c0b6915412ef0437316db7ff08edd464c1ae251.pdf",
    "bibTeX": {
      "@inproceedings": "schwarz-icnc13",
      "author": "Thomas Schwarz and Ignacio Corder\\'i and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Simple, Exact Placement of Data in Containers",
      "booktitle": "Proceedings of International Conference on Computing, Networking and Communications (ICNC)",
      "year": 2013,
      "month": "jan",
      "address": "San Diego",
      "organization": "IEEE"
    }
  },
  {
    "id": 110,
    "title": "Horus: Fine-Grained Encryption-Based Security for Large-Scale Storage",
    "short_description": "",
    "full_content": "With the growing use of large-scale distributed systems, the likelihood that at least one node is compromised is increasing. Large-scale systems that process sensitive data such as geographic data with defense implications, drug modeling, nuclear explosion modeling, and private genomic data would benefit greatly from strong security for their storage. Nevertheless, many high performance computing (HPC), cloud, or secure content delivery network (SCDN) systems that handle such data still store them unencrypted or use simple encryption schemes, relying heavily on physical isolation to ensure confidentiality, providing little protection against compromised computers or malicious insiders. Moreover, current encryption solutions cannot efficiently provide fine-grained encryption for large datasets. Our approach, Horus, encrypts large datasets using keyed hash trees (KHTs) to generate different keys for each region of the dataset, providing fine-grained security: the key for one region cannot be used to access another region.",
    "author": [
      "Yan Li",
      "Nakul Sanjay Dhotre",
      "Yasuhiro Ohara",
      "Thomas M. Kroeger",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Feb 13, 2013",
    "url": "https://ssrc.us/media/pubs/a7e9d6c768fb788a18a01408cd376898681a3b59.pdf",
    "bibTeX": {
      "@inproceedings": "li-fast13",
      "author": "Yan Li and Nakul Sanjay Dhotre and Yasuhiro Ohara and Thomas M. Kroeger and Ethan L. Miller and Darrell D. E. Long",
      "title": "Horus: Fine-Grained Encryption-Based Security for Large-Scale Storage",
      "booktitle": "Proceedings of the Conference on File and Storage Technologies (FAST)",
      "year": 2013,
      "month": "feb",
      "address": "San Jose",
      "organization": "Usenix Association",
      "pages": "147--160"
    }
  },
  {
    "id": 111,
    "title": "A Flexible Simulation Tool for Estimating Data Loss Risks in Storage Arrays",
    "short_description": "",
    "full_content": "Proteus is an open-source simulation program that can predict the risk of data loss in many disk array configurations, among which, mirrored disks, all levels of RAID arrays and various two-dimensional RAID arrays. It characterizes each array by five numbers, namely, the size n of the array, the number nf of simultaneous disk failures the array will always tolerate without data loss, and the respective fractions f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub> , f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub> and f <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sub> of simultaneous failures of n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">f</sub> + 1, n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">f</sub> + 2 and n <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">f</sub> + 3 disks that will not result in a data loss. As with any simulation tool, Proteus imposes no restriction on the distributions of failure and repair events. Our measurements have shown a surprisingly good agreement with the results obtained through analytical techniques and no measurable difference between values obtained assuming deterministic repair times and those assuming exponential repair times.",
    "author": [
      "Hsu-Wan Kao",
      "Jehan-François Pâris",
      "Thomas Schwarz, S.J.",
      "Darrell D. E. Long"
    ],
    "date": "May 8, 2013",
    "url": "https://ssrc.us/media/pubs/c4f125b292178426b9b8c88d9416da160fadc2ae.pdf",
    "bibTeX": {
      "@inproceedings": "kao-msst13",
      "author": "Hsu-Wan Kao and Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz, S.J. and Darrell D. E. Long",
      "title": "A Flexible Simulation Tool for Estimating Data Loss Risks in Storage Arrays",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2013,
      "month": "may",
      "address": "Long Beach",
      "organization": "IEEE"
    }
  },
  {
    "id": 112,
    "title": "Examining Extended and Scientific Metadata for Scalable Index Designs",
    "short_description": "",
    "full_content": "While file system metadata is well characterized by a variety of workload studies, scientific metadata is much less well understood. We characterize scientific metadata, in order to better understand the implications for index design. Based on our findings, existing solutions for either file system or scientific search will not suffice for indexing a large scientific file system. We describe the problems with existing solutions, and suggest column stores as an alternative approach.",
    "author": [
      "Aleatha Parker-Wood",
      "Brian Madden",
      "Michael McThrow",
      "Darrell D. E. Long",
      "Ian F. Adams",
      "Avani Wildani"
    ],
    "date": "Jun 30, 2013",
    "url": "https://ssrc.us/media/pubs/e7225d7288c8d36124427ea03981d19d12e220e5.pdf",
    "bibTeX": {
      "@inproceedings": "parker-wood-systor13",
      "author": "Aleatha Parker-Wood and Brian Madden and Michael McThrow and Darrell D. E. Long and Ian F. Adams and Avani Wildani",
      "title": "Examining Extended and Scientific Metadata for Scalable Index Designs",
      "booktitle": "Proceedings of the Sixth Annual International Systems and Storage Conference (SYSTOR)",
      "year": 2013,
      "month": "jun",
      "address": "Haifa, Israel",
      "organization": "ACM"
    }
  },
  {
    "id": 113,
    "title": "Evaluation of a Hybrid Approach for Efficient Provenance Storage",
    "short_description": "",
    "full_content": "Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the provenance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compression time and query performance when compared to other compression schemes. Categories and Subject Descriptors E.4[Coding and Information Theory]: Data compaction and compression; H.3.2[Information Storage and Retrieval]: Information Storage - File organization",
    "author": [
      "Yulai Xie",
      "Kiran-Kumar Muniswamy-Reddy",
      "Dan Feng",
      "Yan Li",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2013",
    "url": "https://ssrc.us/media/pubs/ad9b5bfc8371836c5499d5525493935d8adda5f9.pdf",
    "bibTeX": {
      "@article": "xie-tos13",
      "author": "Yulai Xie and Kiran-Kumar Muniswamy-Reddy and Dan Feng and Yan Li and Darrell D. E. Long",
      "title": "Evaluation of a Hybrid Approach for Efficient Provenance Storage",
      "journal": "ACM Transactions on Storage",
      "volume": 9,
      "number": 4,
      "year": 2013,
      "month": "nov"
    }
  },
  {
    "id": 114,
    "title": "Building JACK: Developing Metrics for Use in Multi-Objective Optimal Data Allocation Strategies",
    "short_description": "",
    "full_content": "",
    "author": [
      "Christina Strong",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Jan 7, 2014",
    "url": "https://ssrc.us/media/pubs/49d0443910c8167f38187eebf4ca8c6fd3e81127.pdf",
    "bibTeX": {
      "@techreport": "strong-techreport14",
      "author": "Christina Strong and Ahmed Amer and Darrell D. E. Long",
      "title": "Building {JACK",
      "institution": "University of California, Santa Cruz",
      "number": "UCSC-SSRC-14-01",
      "year": 2014,
      "month": "jan"
    }
  },
  {
    "id": 115,
    "title": "A File By Any Other Name: Managing File Names with Metadata",
    "short_description": "",
    "full_content": "",
    "author": [
      "Aleatha Parker-Wood",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Philippe Rigaux",
      "Andy Isaacson"
    ],
    "date": "Jun 10, 2014",
    "url": "https://ssrc.us/media/pubs/e6a4e47c11c5a468d9b7e33a5210e8fc57ddf159.pdf",
    "bibTeX": {
      "@inproceedings": "parker-wood-systor14",
      "author": "Aleatha Parker-Wood and Darrell D. E. Long and Ethan Miller and Philippe Rigaux and Andy Isaacson",
      "title": "A File By Any Other Name: Managing File Names with Metadata",
      "booktitle": "Proceedings of the Seventh Annual International Systems and Storage Conference (SYSTOR)",
      "year": 2014,
      "month": "jun",
      "address": "Haifa, Israel",
      "organization": "ACM"
    }
  },
  {
    "id": 116,
    "title": "PERSES: Data Layout for Low Impact Failures",
    "short_description": "",
    "full_content": "Growth in disk capacity continues to outpace advances in read speed and device reliability. This has led to storage systems spending increasing amounts of time in a degraded state while failed disks reconstruct. Users and applications that do not use the data on the failed or degraded drives are negligibly impacted by the failure, increasing the perceived performance of the system. We leverage this observation with PERSES, a statistical data allocation scheme to reduce the performance impact of failures by clustering data on disks such that data with high probability of co-access is placed on the same device as often as possible.",
    "author": [
      "Avani Wildani",
      "Ethan L. Miller",
      "Ian F. Adams",
      "Darrell D. E. Long"
    ],
    "date": "Sep 9, 2014",
    "url": "https://ssrc.us/media/pubs/db9e477f8d70c6439364112f4bb7d7a9ca95a617.pdf",
    "bibTeX": {
      "@inproceedings": "wildani-mascots14",
      "author": "Avani Wildani and Ethan L. Miller and Ian F. Adams and Darrell D. E. Long",
      "title": "PERSES: Data Layout for Low Impact Failures",
      "booktitle": "Proceedings of the Twenty-second International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2014)",
      "year": 2014,
      "month": "sep",
      "address": "Paris",
      "organization": "IEEE"
    }
  },
  {
    "id": 117,
    "title": "Which Storage Device is Greenest? Modeling the Energy Cost of I/O Workloads",
    "short_description": "",
    "full_content": "The performance requirements and amount of work of an I/O workload affect the number of storage devices and the run time needed by the workload, and should be included in the calculation of the cost or energy consumption of storage devices. This paper introduces models to calculate the cost and energy consumption of storage devices for running a variety of workloads, categorized by their dominant requirements. Measurements of two latest hard disk and solid-state drive (SSD) are included to illustrate the models in practice. Contrary to common belief, SSD is not the energy efficient choice for many workloads.",
    "author": [
      "Yan Li",
      "Darrell D. E. Long"
    ],
    "date": "Sep 10, 2014",
    "url": "https://ssrc.us/media/pubs/7d16df559585d26c07cea39fb6fe33503007438a.pdf",
    "bibTeX": {
      "@inproceedings": "li-mascots14",
      "author": "Yan Li and Darrell D. E. Long",
      "title": "Which Storage Device is Greenest? Modeling the Energy Cost of I/O Workloads",
      "booktitle": "Proceedings of the Twenty-second International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2014)",
      "year": 2014,
      "month": "sep",
      "address": "Paris",
      "organization": "IEEE"
    }
  },
  {
    "id": 118,
    "title": "Protecting RAID Arrays Against Unexpectedly High Disk Failure Rates",
    "short_description": "",
    "full_content": "Disk failure rates vary so widely among different makes and models that designing storage solutions for the worst case scenario is a losing proposition. The approach we propose here is to design our storage solutions for the most probable case while incorporating in our design the option of adding extra redundancy when we find out that its disks are less reliable than expected. To illustrate our proposal, we show how to increase the reliability of existing two-dimensional disk arrays with n <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> data elements and 2n parity elements by adding n additional parity elements that will mirror the contents of half the existing parity elements. Our approach offers the three advantages of being easy to deploy, not affecting the complexity of parity calculations, and providing a five-year reliability of 99.999 percent in the face of catastrophic levels of data loss where the array would lose up to a quarter of its storage capacity in a year.",
    "author": [
      "Jehan-François Pâris",
      "Thomas J. E. Schwarz",
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2014",
    "url": "https://ssrc.us/media/pubs/b7e8714a7341963723c7a64ae40a801cfb8c7187.pdf",
    "bibTeX": {
      "@inproceedings": "paris-prdc14",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas J. E. Schwarz and Ahmed Amer and Darrell Long",
      "title": "Protecting RAID Arrays Against Unexpectedly High Disk Failure Rates",
      "booktitle": "Proceedings of the Twentieth IEEE Pacific Rim International Symposium on Dependable Computing (PRDC 2014)",
      "year": 2014,
      "month": "nov",
      "address": "Singapore",
      "organization": "IEEE"
    }
  },
  {
    "id": 119,
    "title": "Self-Repairing Disk Arrays",
    "short_description": "",
    "full_content": "As the prices of magnetic storage continue to decrease, the cost of replacing failed disks becomes increasingly dominated by the cost of the service call itself. We propose to eliminate these calls by building disk arrays that contain enough spare disks to operate without any human intervention during their whole lifetime. To evaluate the feasibility of this approach, we simulated the behavior of two-dimensional disk arrays with n parity disks and n(n-1)/2 data disks under realistic failure and repair assumptions. Our conclusion is that having n(n+1)/2 spare disks is more than enough to achieve a 99.999 percent probability of not losing data over four years. We also observe that the same objectives cannot be reached with RAID level 6 organizations and would require RAID stripes that could tolerate triple disk failures.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Thomas Schwarz"
    ],
    "date": "Jan 21, 2015",
    "url": "",
    "bibTeX": {
      "@inproceedings": "paris-adapt15",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell Long and Thomas Schwarz",
      "title": "Self-Repairing Disk Arrays",
      "booktitle": "Proceedings of the Fifth International Workshop on Adaptive Self-tuning Computing Systems (ADAPT)",
      "year": 2015,
      "month": "jan",
      "address": "Amsterdam, The Netherlands",
      "publisher": "ACM"
    }
  },
  {
    "id": 120,
    "title": "Even or Odd: A Simple Graphical Authentication System",
    "short_description": "",
    "full_content": "Many portable devices need a simple authentication system to protect them from being used by an unauthenticated person such as a thief. The security of traditional methods such as pin codes or passwords is limited by shoulder surfing where a casual or intentional observer observes an authentication session and derives all information necessary for authentication. Graphical authentication systems have been developed to forestall this attack. We present here an especially simple variant of a graphical authentication system based on the capacity of humans to recognize faces well. In our challenge-response scheme, a user is presented with a row of typically three faces and needs to decide whether the number of \"friends\" is even or odd. We present here an analysis of security and usability of this scheme. Keywords--- Authentication, Usability, Graphical Passwords",
    "author": [
      "Nicolás López",
      "Matías Rodríguez",
      "Catalina Fellegi",
      "Darrell Long",
      "Thomas Schwarz, S.J."
    ],
    "date": "Mar 1, 2015",
    "url": "https://ssrc.us/media/pubs/a13df07eff9a3a0add4b629872905bc9b853af7c.pdf",
    "bibTeX": {
      "@article": "lopez-lat15",
      "author": "Nicol\\'as L\\'opez and Mat\\'{\\i}as Rodr\\'{\\i}guez and Catalina Fellegi and Darrell Long and Thomas Schwarz, S.J.",
      "title": "Even or Odd: A Simple Graphical Authentication System",
      "journal": "IEEE Latin America Transactions",
      "volume": 13,
      "number": 3,
      "year": 2015,
      "month": "mar",
      "pages": "804--809"
    }
  },
  {
    "id": 121,
    "title": "ASCAR: Automating Contention Management for High-Performance Storage Systems",
    "short_description": "",
    "full_content": "High-performance parallel storage systems, such as those used by supercomputers and data centers, can suffer from performance degradation when a large number of clients are contending for limited resources, like bandwidth. These contentions lower the efficiency of the system and cause unwanted speed variances. ASCAR (Automatic Storage Contention Alleviation and Reduction) is a storage traffic management system for improving the bandwidth utilization and fairness of resource allocation. ASCAR regulates I/O traffic from the clients using a rule-based algorithm that controls the congestion window and rate limit. The rule-based client controllers are fast responding to burst I/O because no runtime coordination between clients or with a central coordinator is needed; they are also autonomous so the system has no scale-out bottleneck. Evaluation shows that the ASCAR prototype can improve the throughput of all tested workloads - some by as much as 35%.",
    "author": [
      "Yan Li",
      "Xiaoyuan Lu",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Jun 2, 2015",
    "url": "https://ssrc.us/media/pubs/35501b12a19824cf2f4cf48eb65ceb1445b28c0e.pdf",
    "bibTeX": {
      "@inproceedings": "li-msst15",
      "author": "Yan Li and Xiaoyuan Lu and Ethan L. Miller and Darrell D. E. Long",
      "title": "ASCAR: Automating Contention Management for High-Performance Storage Systems",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2015,
      "month": "may",
      "address": "Santa Clara University",
      "organization": "IEEE"
    }
  },
  {
    "id": 122,
    "title": "Classifying Data to Reduce Long Term Data Movement in Shingled Write Disks",
    "short_description": "",
    "full_content": "Shingled Magnetic Recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the entire band is reclaimed, and a poor band compaction algorithm could result in spending a lot of time moving blocks over the lifetime of the device. We propose using write frequency to separate blocks to reduce data movement and develop a band compaction algorithm that implements this heuristic. We demonstrate how our algorithm results in improved data management, resulting in an up to 47% reduction in required data movements when compared to naive approaches to band management.",
    "author": [
      "Stephanie N. Jones",
      "Ahmed Amer",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Rekha Pitchumani",
      "Christina R. Strong"
    ],
    "date": "Jun 4, 2015",
    "url": "https://ssrc.us/media/pubs/beab0307cea322529784d702dc70dcbdad9a171c.pdf",
    "bibTeX": {
      "@article": "jones-tos16",
      "author": "Stephanie N. Jones and Ahmed Amer and Ethan L. Miller and Darrell D. E. Long and Rekha Pitchumani and Christina R. Strong",
      "title": "Classifying Data to Reduce Long-Term Data Movement in Shingled Write Disks",
      "journal": "ACM Transactions on Storage",
      "volume": 12,
      "number": 1,
      "year": 2016,
      "month": "feb"
    }
  },
  {
    "id": 123,
    "title": "Percival: A Searchable Secret Split Datastore",
    "short_description": "",
    "full_content": "Maintaining information privacy is challenging when sharing data across a distributed long-term datastore. In such applications, secret splitting the data across independent sites has been shown to be a superior alternative to fixed-key encryption; it improves reliability, reduces the risk of insider threat, and removes the issues surrounding key management. However, the inherent security of such a datastore normally precludes it from being directly searched without reassembling the data; this, however, is neither computationally feasible nor without risk since reassembly introduces a single point of compromise. As a result, the secret-split data must be pre-indexed in some way in order to facilitate searching. To meet these needs, we developed Percival: a novel system that enables searching a secret-split datastore while maintaining information privacy. The system leverages salted hashing, performed within hardware security modules, to access prerecorded queries that have been secret split and stored in a distributed environment. When testing Percival on a corpus of approximately one million files, we found that the average search operation completed in less than one second.",
    "author": [
      "Joel C. Frank",
      "Shayna M. Frank",
      "Lincoln A. Thurlow",
      "Thomas M. Kroeger",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Jun 4, 2015",
    "url": "https://ssrc.us/media/pubs/b3ecd7f064f22088266ca2a52935d1f840151d0f.pdf",
    "bibTeX": {
      "@inproceedings": "frank-msst15",
      "author": "Joel C. Frank and Shayna M. Frank and Lincoln A. Thurlow and Thomas M. Kroeger and Ethan L. Miller and Darrell D. E. Long",
      "title": "Percival: A Searchable Secret Split Datastore",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2015,
      "month": "may",
      "address": "Santa Clara University",
      "organization": "IEEE"
    }
  },
  {
    "id": 124,
    "title": "Classifying Data to Reduce Long-Term Data Movement in Shingled Write Disks",
    "short_description": "",
    "full_content": "Shingled Magnetic Recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the entire band is reclaimed, and a poor band compaction algorithm could result in spending a lot of time moving blocks over the lifetime of the device. We propose using write frequency to separate blocks to reduce data movement and develop a band compaction algorithm that implements this heuristic. We demonstrate how our algorithm results in improved data management, resulting in an up to 47% reduction in required data movements when compared to naive approaches to band management.",
    "author": [
      "Stephanie N. Jones",
      "Ahmed Amer",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Rekha Pitchumani",
      "Christina R. Strong"
    ],
    "date": "Feb 1, 2016",
    "url": "https://ssrc.us/media/pubs/3a6a2a2a5904469af04d266995dc84b0eec89ee2.pdf",
    "bibTeX": {
      "@article": "jones-tos16",
      "author": "Stephanie N. Jones and Ahmed Amer and Ethan L. Miller and Darrell D. E. Long and Rekha Pitchumani and Christina R. Strong",
      "title": "Classifying Data to Reduce Long-Term Data Movement in Shingled Write Disks",
      "journal": "ACM Transactions on Storage",
      "volume": 12,
      "number": 1,
      "year": 2016,
      "month": "feb"
    }
  },
  {
    "id": 125,
    "title": "Oasis: an Active Storage Framework for Object Storage Platform",
    "short_description": "",
    "full_content": "The network bottleneck incurred by big data process and transfer has increasingly become a severe problem in today's data center and cloud. Exploring and exploiting the advantages of both the scalable object storage architecture and intelligent active storage technology are one of the ways to address this challenge. In this paper, we present the design and performance evaluation of Oasis, an active storage framework for object-based storage platform such as Seagate Kinetic. The basic idea behind Oasis is to leverage the OSD's processing capability to run data intensive applications locally. In contrast with previous work, Oasis has the following advantages. First, Oasis enables users to transparently process the OSD object and supports different processing granularity. Second, Oasis can ensure the integrity of execution code using signature scheme and provide the access control for the code execution in the OSD by enhancing the existing OSD security protocol. Third, Oasis can partition the computation task between host and OSD dynamically according to the OSD workload status. Our work on Oasis can be integrated into Kinetic object storage platform seamlessly. Experimental results on widely-used real world applications demonstrate the performance and efficiency of our system.",
    "author": [
      "Yulai Xie",
      "Dan Feng",
      "Yan Li",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 2016",
    "url": "https://ssrc.us/media/pubs/a08002805a8cadaf36fd9471a6e5fd62390979e1.pdf",
    "bibTeX": {
      "@article": "xie-fgcs16",
      "author": "Yulai Xie and Dan Feng and Yan Li and Darrell D. E. Long",
      "title": "Oasis: an Active Storage Framework for Object Storage Platform",
      "journal": "Future Generation Computer Systems",
      "volume": 56,
      "year": 2016,
      "month": "mar",
      "pages": "746--758"
    }
  },
  {
    "id": 126,
    "title": "Effects of Prolonged Media Usage and Long-term Planning on Archival Systems",
    "short_description": "",
    "full_content": "In archival systems, storage media are often replaced much earlier than their expected service life in exchange for other benefits of new media, such as higher capacity, bandwidth, and I/O operations per second, or lower costs. In an era of decreasing media density growth rates, retiring media early by considering only short-term benefits while discarding potential long-term cost benefits could have a negative long-term impact on an archival system's economics. To extend an archival system's life, at low cost, while limiting performance degradation, we suggest extending media lifetime past manufacturer recommendations as well as increasing the horizon for planning and provisioning future media purchases. We present a cost-benefit analysis of the impact of prolonged media usage and long-term planning. Through Monte Carlo simulation, we simulate the behavior of an archival system using tapes, hard disk drives (HDDs), solid state devices (SSDs), and Blu-ray discs. We show that leaving older media in the archival system makes economic sense for SSDs without significantly affecting reliability; we show cost improvements of approximately 10% for SSDs for a low annual media density growth rate, such as 5%, which would have been a loss of 35%, for a high annual media density rate, such as 20%. We show that, for SSDs and hard disks, the optimal planning time of an archival system is at least as long as the media service life. Combining prolonged media usage with an extended planning horizon reduced costs by 15% for a system using SSDs.",
    "author": [
      "Preeti Gupta",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "David S.H. Rosenthal",
      "Avani Wildani"
    ],
    "date": "May 5, 2016",
    "url": "https://ssrc.us/media/pubs/e6d80e26fce5d8d5ec549c3f0939aadd133e530a.pdf",
    "bibTeX": {
      "@inproceedings": "gupta-msst16",
      "author": "Preeti Gupta and Darrell D. E. Long and Ethan L. Miller and David S.H. Rosenthal and Avani Wildani",
      "title": "Effects of Prolonged Media Usage and Long-term Planning on Archival Systems",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2016,
      "month": "may",
      "organization": "IEEE"
    }
  },
  {
    "id": 127,
    "title": "Pilot: A Framework that Understands How to Do Performance Benchmarks The Right Way",
    "short_description": "",
    "full_content": "Carrying out even the simplest performance benchmark requires considerable knowledge of statistics and computer systems, and painstakingly following many error-prone steps, which are distinct skill sets yet essential for getting statistically valid results. As a result, many performance measurements in peer-reviewed publications are flawed. Among many problems, they fall short in one or more of the following requirements: accuracy, precision, comparability, repeatability, and control of overhead. This is a serious problem because poor performance measurements misguide system design and optimization. We propose a collection of algorithms and heuristics to automate these steps. They cover the collection, storing, analysis, and comparison of performance measurements. We also implement these methods as a readily-usable open source software framework called Pilot, which can help to reduce human error and shorten benchmark time. Evaluation of Pilot with various benchmarks show that it can reduce the cost and complexity of running benchmarks, and can produce better measurement results. Index Terms: computer performance; performance analysis; software performance; performance evaluation; system performance; measurement techniques; heuristic algorithms",
    "author": [
      "Yan Li",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Yash Gupta"
    ],
    "date": "Sep 19, 2016",
    "url": "https://ssrc.us/media/pubs/1fc262bbb19b5ff0ec48ec4912d9f75c15917ce7.pdf",
    "bibTeX": {
      "@inproceedings": "li-mascots16",
      "author": "Yan Li and Ethan L. Miller and Darrell D. E. Long and Yash Gupta",
      "title": "Pilot: A Framework that Understands How to Do Performance Benchmarks the Right Way",
      "booktitle": "Proceedings of the Twenty-fourth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2016)",
      "year": 2016,
      "month": "sep",
      "address": "London",
      "organization": "IEEE"
    }
  },
  {
    "id": 128,
    "title": "RESAR: Reliable Storage at Exabyte Scale",
    "short_description": "",
    "full_content": "Stored data needs to be protected against device failure and irrecoverable sector read errors, yet doing so at exabyte scale can be challenging given the large number of failures that must be handled. We have developed RESAR (Robust, Efficient, Scalable, Autonomous, Reliable) storage, an approach to storage system redundancy that only uses XOR-based parity and employs a graph to lay out data and parity. The RESAR layout offers greater robustness and higher flexibility for repair at the same overhead as a declustered version of RAID 6. For instance, a RESAR-based layout with 16 data disklets per stripe has about 50 times lower probability of suffering data loss in the presence of a fixed number of failures than a corresponding RAID 6 organization. RESAR uses a layer of virtual storage elements to achieve better manageability, a broader potential for energy savings, as well as easier adoption of heterogeneous storage devices.",
    "author": [
      "Thomas Schwarz",
      "Ahmed Amer",
      "Thomas Kroeger",
      "Ethan L. Miller",
      "Darrell Long",
      "Jehan-François Pâris"
    ],
    "date": "Sep 21, 2016",
    "url": "https://ssrc.us/media/pubs/495396c0b29a40d17d4c6e7bc2627abce18ffc1c.pdf",
    "bibTeX": {
      "@inproceedings": "schwarz-mascots16",
      "author": "Thomas Schwarz and Ahmed Amer and Thomas Kroeger and Ethan L. Miller and Darrell Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "RESAR: Reliable Storage at Exabyte Scale",
      "booktitle": "Proceedings of the Twenty-fourth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2016)",
      "year": 2016,
      "month": "sep",
      "address": "London",
      "organization": "IEEE"
    }
  },
  {
    "id": 129,
    "title": "CAPES: Unsupervised Storage Performance Tuning Using Neural Network-Based Deep Reinforcement Learning",
    "short_description": "",
    "full_content": "Parameter tuning is an important task of storage performance optimization. Current practice usually involves numerous tweak-benchmark cycles that are slow and costly. To address this issue, we developed CAPES, a model-less deep reinforcement learning-based unsupervised parameter tuning system driven by a deep neural network (DNN). It is designed to find the optimal values of tunable parameters in computer systems, from a simple client-server system to a large data center, where human tuning can be costly and often cannot achieve optimal performance. CAPES takes periodic measurements of a target computer system's state, and trains a DNN which uses Q-learning to suggest changes to the system's current parameter values. CAPES is minimally intrusive, and can be deployed into a production system to collect training data and suggest tuning actions during the system's daily operation. Evaluation of a prototype on a Lustre file system demonstrates an increase in I/O throughput up to 45",
    "author": [
      "Yan Li",
      "Kenneth Chang",
      "Oceane Bel",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Nov 13, 2017",
    "url": "https://ssrc.us/media/pubs/ea175a9f07f9f172005c62cf0514051fa08ff0f1.pdf",
    "bibTeX": {
      "@inproceedings": "li-sc17",
      "author": "Yan Li and Kenneth Chang and Oceane Bel and Ethan L. Miller and Darrell D. E. Long",
      "title": "CAPES: Unsupervised Storage Performance Tuning Using Neural Network-Based Deep Reinforcement Learning",
      "booktitle": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '17)",
      "year": 2017,
      "month": "nov",
      "publisher": "ACM"
    }
  },
  {
    "id": 130,
    "title": "Twizzler: An Operating System for Next-Generation Memory Hierarchies",
    "short_description": "",
    "full_content": "The introduction of NVDIMMs (truly non-volatile and directly accessible) requires us to rethink all levels of the system stack, from processor features to applications. Operating systems, too, must evolve to support new I/O models for applications accessing persistent data. We developed Twizzler, an operating system for next-generation memory hierarchies. Twizzler is designed to provide applications with direct access to persistent storage while providing mechanisms for cross-object pointers, removing itself from the common access path to persistent data, and providing fine-grained security and recoverability. Twizzler removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application.",
    "author": [
      "Daniel Bittman",
      "Darrell D. E. Long",
      "Ethan L. Miller",
      "Peter Alvaro"
    ],
    "date": "Jul 1, 2019",
    "url": "https://ssrc.us/media/pubs/39d6b0bee42492871fd9d2a1a41fcc56add986cc.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-atc19-poster",
      "author": "Daniel Bittman and Darrell D. E. Long and Ethan L. Miller and Peter Alvaro",
      "title": "Twizzler: An Operating System for Next-Generation Memory Hierarchies",
      "booktitle": "Poster Session of the USENIX Annual Technical Conference (ATC)",
      "year": 2019,
      "month": "jul",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 131,
    "title": "Inkpack: A Secure, Data-Exposure Resistant Storage System",
    "short_description": "",
    "full_content": "Removing hard drives from a data center may expose sensitive data, such as encryption keys or passwords. To prevent exposure, data centers have security policies in place to physically secure drives in the system, and securely delete data from drives that are removed. Despite advances in security technology and best practices, implementation of these security measures is often done incorrectly. We anticipate that physical security will fail, and fixing the issue after the failure is costly and ineffective. [1]This research was supported by the NSF under grant IIP-1266400 and by the industrial members of the NSF IUCRC Center for Research in Storage Systems. We propose Inkpack, a protocol that prevents an attacker from reading data from a drive removed from the data center even if the attacker has the user key linked to the data. An implementation of this protocol encrypts data, and secret splits the key over a number of drives. Recovering the key requires communicating with other drives, thereby denying access to the data if a few drives have been removed. Inkpack also requires the system to verify the validity of individual drives before normal operation. A prototype created within the Ceph storage system executed individual key split, key rebuild, and drive validation operations in 100--150 s. We also show that our protocol is sensitive to small data write overheads, demonstrating potential performance gains if implemented on smart solid state storage devices, and propose a solution to increase performance.",
    "author": [
      "Oceane Bel",
      "Kenneth Chang",
      "Daniel Bittman",
      "Ethan L. Miller",
      "Darrell D. E. Long",
      "Hiroshi Isozaki"
    ],
    "date": "Jun 4, 2018",
    "url": "https://ssrc.us/media/pubs/63636bdd45dc0eda47f6db142582fefb1dcc3216.pdf",
    "bibTeX": {
      "@inproceedings": "bel-systor18",
      "author": "Oceane Bel and Kenneth Chang and Daniel Bittman and Ethan L. Miller and Darrell D. E. Long and Hiroshi Isozaki",
      "title": "Inkpack: A Secure, Data-Exposure Resistant Storage System",
      "booktitle": "Proceedings of the Eleventh Annual International Systems and Storage Conference (SYSTOR)",
      "year": 2018,
      "month": "jun",
      "address": "Haifa, Israel",
      "organization": "ACM"
    }
  },
  {
    "id": 132,
    "title": "Designing Data Structures to Minimize Bit Flips on NVM",
    "short_description": "",
    "full_content": "New byte-addressable non-volatile memory (BNVM) technologies such as phase change memory (PCM) enable the construction of systems with large persistent memories, improving reliability and potentially reducing power consumption. However, BNVM technologies only support a limited number of lifetime writes per cell and consume most of their power when flipping a bit's state during a write. We develop a framework for using the number of bit flips as the measure of 'goodness' for a range of hardware and software techniques. We confirm that approaches with the fewest writes often have more bit flips than those optimized to reduce bit flipping. We were able to reduce the number of bits flipped by up to 3.56x over standard implementations of the same data structures with negligible overhead.",
    "author": [
      "Daniel Bittman",
      "Matthew Gray",
      "Justin Raizes",
      "Sinjoni Mukhopadhyay",
      "Matthew Bryson",
      "Peter Alvaro",
      "Darrell Long",
      "Ethan L. Miller"
    ],
    "date": "Aug 28, 2018",
    "url": "https://ssrc.us/media/pubs/22231b2cf1a7af7c758716f9ed669d47b30228ad.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-nvmsa18",
      "author": "Daniel Bittman and Matthew Gray and Justin Raizes and Sinjoni Mukhopadhyay and Matthew Bryson and Peter Alvaro and Darrell Long and Ethan L. Miller",
      "title": "Designing Data Structures to Minimize Bit Flips on NVM",
      "booktitle": "Proceedings of the Seventh IEEE Non-Volatile Memory Systems and Applications Symposium (NVMSA '18)",
      "year": 2018,
      "month": "aug",
      "address": "Hakodate, Japan",
      "organization": "IEEE"
    }
  },
  {
    "id": 133,
    "title": "Pagoda: A Hybrid Approach to Enable Efficient Real-time Provenance Based Intrusion Detection in Big Data Environments",
    "short_description": "",
    "full_content": "Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the provenance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compression time and query performance when compared to other compression schemes. Categories and Subject Descriptors E.4[Coding and Information Theory]: Data compaction and compression; H.3.2[Information Storage and Retrieval]: Information Storage - File organization",
    "author": [
      "Yulai Xie",
      "Dan Feng",
      "Yuchong Hu",
      "Yan Li",
      "Staunton Sample",
      "Darrell D. E. Long"
    ],
    "date": "Aug 29, 2018",
    "url": "https://ssrc.us/media/pubs/ada6e507e18591586bcfb4a69a137673505e36cb.pdf",
    "bibTeX": {
      "@article": "xie-tdsc18",
      "author": "Yulai Xie and Dan Feng and Yuchong Hu and Yan Li and Staunton Sample and Darrell Long",
      "title": "Pagoda: A Hybrid Approach to Enable Efficient Real-time Provenance Based Intrusion Detection in Big Data Environments",
      "journal": "IEEE Transactions on Dependable and Secure Computing",
      "year": 2018,
      "month": "aug"
    }
  },
  {
    "id": 134,
    "title": "Efficient Reconstruction Techniques for Disaster Recovery in Secret-Split Datastores",
    "short_description": "",
    "full_content": "Increasingly, archival systems rely on authentication-based techniques that leverage secret-splitting rather than encryption to secure data for long-term storage. Secret-splitting data across multiple independent repositories reduces complexities in key management, eliminates the need for updates due to encryption algorithm deprecation over time, and reduces the risk of insider compromise. While reconstruction of stored data objects is straightforward if a user-maintained index is available, the system must also support disaster recovery for index-free datastores. Designing a mechanism for efficient index-free reconstruction that does not increase the risk of attacker compromise is a challenge. Reconstruction requires the association of chunks that make up an object, which is the kind of information attackers can use to identify chunks they must steal to illicitly obtain data. We propose two new techniques, the Set-Subset reconstruction and Secret-Split Secure Hash reconstruction, which allow chunks of data to be correlated and quickly reconstructed. Both techniques operate on the entire collection of chunks in the archive. While they can efficiently rebuild an entire archive, they are inefficient and impractical for rebuilding single objects, making them useless for attackers that do not have access to all of the data. These techniques can each be tuned to trade-off between reconstruction performance and security, reducing overall runtime from O(N^T) (for N objects requiring T recombined chunks each to return the original object) to between O(N) and O(N^2). These runtimes are practical for archives containing as many as 10^7 objects. Larger archives can run these techniques with manageable runtimes by dividing themselves into smaller collections and running the algorithms separately on each collection.",
    "author": [
      "Sinjoni Mukhopadhyay",
      "Joel Frank",
      "Daniel Bittman",
      "Darrell Long",
      "Ethan L. Miller"
    ],
    "date": "Sep 25, 2018",
    "url": "https://ssrc.us/media/pubs/9b7193806988f1c135c7ce6321bb7359f0b00d84.pdf",
    "bibTeX": {
      "@inproceedings": "mukhopadhyay-mascots18",
      "author": "Sinjoni Mukhopadhyay and Joel Frank and Daniel Bittman and Darrell Long and Ethan L. Miller",
      "title": "Efficient Reconstruction Techniques for Disaster Recovery in Secret-Split Datastores",
      "booktitle": "Proceedings of the Twenty-sixth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2018)",
      "year": 2018,
      "month": "sep",
      "address": "Milwaukee",
      "organization": "IEEE"
    }
  },
  {
    "id": 135,
    "title": "Using Simulation to Design Scalable and Cost-Efficient Archival Storage Systems",
    "short_description": "",
    "full_content": "The need for reliable and cost-effective data storage grows as digital information becomes increasingly ubiquitous. Archival systems must store valuable data for years while adapting to changing user needs, capacity, and performance requirements. Storage devices differ in terms of performance, capacity, reliability, acquisition cost, power consumption, and the rates at which their features change over time. Today, numerous alternative and emerging storage technologies may become viable for use in archival storage systems as their cost, performance, and reliability change. Differences between storage devices also complicate the choice of the best storage technology to use for an archival system. We have designed a simulator that models the capacity, performance, acquisition cost, and power cost of archival systems. Our simulator compares four storage technologies that exhibit different cost and performance characteristics: tape, optical disc, hard disk, and NAND flash SSD. We evaluate the total cost of ownership for each storage technology within an archival system, and we explore the effect that prospective technological advancements and growth rates over time may have on the relative cost and viability of each storage technology. We show that, in order for each storage technology to remain cost-effective in archival systems over time, it must increase in capacity at least as quickly as the demand for storage overall. We compare trends in storage technologies to suggest developments that could minimize the long-term total cost of ownership for archival systems. We show that hard disks and flash could become cost-competitive with tape-based archives by adopting new designs to minimize infrastructure and electricity costs.",
    "author": [
      "James Byron",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Sep 26, 2018",
    "url": "https://ssrc.us/media/pubs/5afa96a4fbf610b4d45e9c41d4fca786aa59de32.pdf",
    "bibTeX": {
      "@inproceedings": "byron-mascots18",
      "author": "James Byron and Ethan L. Miller and Darrell D. E. Long",
      "title": "Using Simulation to Design Scalable and Cost-Efficient Archival Storage Systems",
      "booktitle": "Proceedings of the Twenty-sixth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2018)",
      "year": 2018,
      "month": "sep",
      "address": "Milwaukee",
      "organization": "IEEE"
    }
  },
  {
    "id": 136,
    "title": "Optimizing Systems for Byte-Addressable NVM by Reducing Bit Flipping",
    "short_description": "",
    "full_content": "New byte-addressable non-volatile memory (BNVM) technologies such as phase change memory (PCM) enable the construction of systems with large persistent memories, improving reliability and potentially reducing power consumption. However, BNVM technologies only support a limited number of lifetime writes per cell and consume most of their power when flipping a bit's state during a write; thus, PCM controllers only rewrite a cell's contents when the cell's value has changed. Prior research has assumed that reducing the number of words written is a good proxy for reducing the number of bits modified, but a recent study has suggested that this assumption may not be valid. Our research confirms that approaches with the fewest writes often have more bit flips than those optimized to reduce bit flipping. To test the effectiveness of bit flip reduction, we built a framework that uses the number of bits flipped over time as the measure of \"goodness\" and modified a cycle-accurate simulator to count bits flipped during program execution. We implemented several modifications to common data structures designed to reduce power consumption and increase memory lifetime by reducing the number of bits modified by operations on several data structures: linked lists, hash tables, and red-black trees. We were able to reduce the number of bits flipped by up to 3.56x over standard implementations of the same data structures with negligible overhead. We measured the number of bits flipped by memory allocation and stack frame saves and found that careful data placement in the stack can reduce bit flips significantly. These changes require no hardware modifications and neither significantly reduce performance nor increase code complexity, making them attractive for designing systems optimized for BNVM.",
    "author": [
      "Daniel Bittman",
      "Darrell D. E. Long",
      "Peter Alvaro",
      "Ethan L. Miller"
    ],
    "date": "Feb 25, 2019",
    "url": "https://ssrc.us/media/pubs/24b1890968914309db6e6962267747f55c7b7dc9.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-fast19",
      "author": "Daniel Bittman and Darrell D. E. Long and Peter Alvaro and Ethan L. Miller",
      "title": "Optimizing Systems for Byte-Addressable NVM by Reducing Bit Flipping",
      "booktitle": "Proceedings of the Conference on File and Storage Technologies (FAST)",
      "year": 2019,
      "month": "feb",
      "address": "San Jose",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 137,
    "title": "Efficient Provenance Management via Clustering and Hybrid Storage in Big Data Environments",
    "short_description": "",
    "full_content": "Provenance is a type of metadata that records the creation and transformation of data objects. It has been applied to a wide variety of areas such as security, search, and experimental documentation. However, provenance usually has a vast amount of data with its rapid growth rate which hinders the effective extraction and application of provenance. This paper proposes an efficient provenance management system via clustering and hybrid storage. Specifically, we propose a Provenance-Based Label Propagation Algorithm which is able to regularize and cluster a large number of irregular provenance. Then, we use separate physical storage mediums, such as SSD and HDD, to store hot and cold data separately, and implement a hot/cold scheduling scheme which can update and schedule data between them automatically. Besides, we implement a feedback mechanism which can locate and compress the rarely used cold data according to the query request. The experimental test shows that the system can significantly improve provenance query performance with a small run-time overhead.",
    "author": [
      "Die Hu",
      "Dan Feng",
      "Yulai Xie",
      "Gongming Xu",
      "Xinrui Gu",
      "Darrell D. E. Long"
    ],
    "date": "Mar 25, 2019",
    "url": "",
    "bibTeX": {
      "@article": "hu-tbd20",
      "author": "Die Hu and Dan Feng and Yulai Xie and Gongming Xu and Xinrui Gu and Darrell Long",
      "title": "Efficient Provenance Management via Clustering and Hybrid Storage in Big Data Environments",
      "journal": "IEEE Transactions on Big Data",
      "volume": 6,
      "number": 4,
      "year": 2020,
      "month": "dec",
      "pages": "792--803"
    }
  },
  {
    "id": 138,
    "title": "The Flipside: A Bit Flip Saved is Power and Lifetime Earned",
    "short_description": "",
    "full_content": "",
    "author": [
      "Daniel Bittman",
      "Peter Alvaro",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Mar 1, 2017",
    "url": "https://ssrc.us/media/pubs/6d9e1139b7972d9484f1f93f463035a1f529333c.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-nvmw17",
      "author": "Daniel Bittman and Peter Alvaro and Darrell D. E. Long and Ethan L. Miller",
      "title": "The Flipside: A Bit Flip Saved is Power and Lifetime Earned",
      "booktitle": "Proceedings of the Non-Volatile Memories Workshop (NVMW)",
      "year": 2017,
      "month": "mar",
      "address": "San Diego",
      "publisher": "IEEE"
    }
  },
  {
    "id": 139,
    "title": "A Tale of Two Abstractions: The Case for Object Space",
    "short_description": "",
    "full_content": "The increasing availability of byte-addressable non-volatile memory on the system bus provides an opportunity to dramatically simplify application interaction with persistent data. However, software and hardware leverage different abstractions: software operating on persistent data structures requires \"global\" pointers that remain valid after a process terminates, while hardware requires that a diverse set of devices all have the same mappings they need for bulk transfers to and from memory, and that they be able to do so for a potentially heterogeneous memory system. Both abstractions must be implemented in a way that is efficient using existing hardware. We propose to abstract physical memory into an object space, which maps objects to physical memory, while providing applications with a way to refer to data that may have a lifetime longer than the processes accessing it. This approach reduces the coordination required for access to multiple types of memory while improving hardware security and enabling more hardware autonomy. We describe how we can use existing hardware support to implement these abstractions, both for applications and for the OS and devices, and show that the performance penalty for this approach is minimal.",
    "author": [
      "Daniel Bittman",
      "Peter Alvaro",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Jul 8, 2019",
    "url": "https://ssrc.us/media/pubs/fb2174c4c08b9b7cc8ac45f4a97dcb04eecc56be.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-hotstorage19",
      "author": "Daniel Bittman and Peter Alvaro and Darrell D. E. Long and Ethan L. Miller",
      "title": "A Tale of Two Abstractions: The Case for Object Space",
      "booktitle": "Proceedings of the Eleventh Hot Topics in Storage and File Systems (Hot Storage 2019)",
      "year": 2019,
      "month": "jul",
      "address": "Renton",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 140,
    "title": "Artifice: A Deniable Steganographic File System",
    "short_description": "",
    "full_content": "The challenge of deniability for sensitive data can be a life or death issue depending on location. Plausible deniability directly impacts groups such as democracy advocates relaying information in repressive regimes, journalists covering human rights stories in a war zone, and NGO workers hiding food shipment schedules from violent militias. All of these users would benefit from a plausibly deniable data storage system. Previous deniable storage solutions only offer pieces of an implementable solution. Artifice is the first tunable, operationally secure, self repairing, and fully deniable steganographic file system. A set of data blocks to be hidden are combined with entropy blocks through error correcting codes to produce a set of obfuscated carrier blocks that are indistinguishable from other pseudorandom blocks on the disk.",
    "author": [
      "Austen Barker",
      "Staunton Sample",
      "Yash Gupta",
      "Anastasia McTaggart",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Aug 13, 2019",
    "url": "https://ssrc.us/media/pubs/b24a5e0bae67ea4d648789f05461ab7281e4b44a.pdf",
    "bibTeX": {
      "@inproceedings": "barker-fast17-poster",
      "author": "Austen Barker and Staunton Sample and Yash Gupta and Anastasia McTaggart and Ethan L. Miller and Darrell D. E. Long",
      "title": "Artifice: A Deniable Steganographic File System",
      "booktitle": "Poster Session of the USENIX Conference on File and Storage Technologies (FAST)",
      "year": 2017,
      "month": "feb",
      "address": "Santa Clara",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 141,
    "title": "A Docker Container Anomaly Monitoring System Based on Optimized Isolation Forest",
    "short_description": "",
    "full_content": "Container-based virtualization has gradually become a main solution in today's cloud computing environments. Detecting and analyzing anomaly in containers present a major challenge for cloud vendors and users. This paper proposes an online container anomaly detection system by monitoring and analyzing multidimensional resource metrics of the containers based on the optimized isolation forest algorithm. To improve the detection accuracy, it assigns each resource metric a weight and changes the random feature selection in the isolation forest algorithm to the weighted feature selection according to the resource bias of the container. In addition, it can identify abnormal resource metrics and automatically adjust the monitoring period to reduce the monitoring delay and system overhead. Moreover, it can locate the cause of the anomalies via analyzing and exploring the container log. The experimental results demonstrate the performance and efficiency of the system on detecting the typical anomalies in containers in both simulated and real cloud environments.",
    "author": [
      "Yulai Xie",
      "Zhuping Zhou",
      "Kai Huang",
      "Gongming Xu",
      "Dan Feng",
      "Darrell Long"
    ],
    "date": "Aug 20, 2019",
    "url": "https://ssrc.us/media/pubs/328563c30a4f38c3860bf200194ba490cba5592f.pdf",
    "bibTeX": {
      "@article": "xie-tcc18",
      "author": "Yulai Xie and Zhuping Zhou and Kai Huang and Gongming Xu and Dan Feng and Darrell Long",
      "title": "A Docker Container Anomaly Monitoring System Based on Optimized Isolation Forest",
      "journal": "IEEE Transactions on Cloud Computing",
      "year": 2018,
      "month": "sep"
    }
  },
  {
    "id": 142,
    "title": "P-Gaussian: Provenance-Based Gaussian Distribution for Detecting Intrusion Behavior Variants Using High Efficient and Real Time Memory Databases",
    "short_description": "",
    "full_content": "It is increasingly important and a big challenge to detect intrusion behavior variants in today's world. Previous host-based intrusion detection methods typically explore the sequence of system calls or unix shell commands to detect the intrusion behavior. This article abstracts the detection of intrusion behavior variants as the comparison between different sequences when the sequence order or length transforms. To overcome the impact of sequence transformation on the detection accuracy, we propose P-Gaussian, a provenance-based Gaussian distribution detection scheme which comprises two key design features: (1) it utilizes provenance to describe and identify intrusion behavior variants, and eliminates the impact of sequence order transformation on the detection accuracy. (2) it adopts Gaussian distribution principle to accurately compute the similarity between intrusion behavior and its variant, and eliminates the impact of intrusion behavior sequence length increase on the detection accuracy. To improve the detection performance, P-Gaussian employs a Redis memory database with multiple Redis instances and multiple threads to enable the parallelism of provenance processing in multi-core environments. It also classifies hot and cold provenance to provide high-efficient long-term forensic analysis. Experimental results on widely-used real world applications demonstrate the performance and efficiency of our system.",
    "author": [
      "Yulai Xie",
      "Yafeng Wu",
      "Dan Feng",
      "Darrell D. E. Long"
    ],
    "date": "Dec 17, 2019",
    "url": "",
    "bibTeX": {
      "@article": "xie-tdsc19",
      "author": "Yulai Xie and Yafeng Wu and Dan Feng and Darrell Long",
      "title": "P-Gaussian: Provenance-Based Gaussian Distribution for Detecting Intrusion Behavior Variants Using High Efficient and Real Time Memory Databases",
      "journal": "IEEE Transactions on Dependable and Secure Computing",
      "year": 2019,
      "month": "dec"
    }
  },
  {
    "id": 143,
    "title": "Real-time Prediction of Docker Container Resource Load Based on A Hybrid Model of ARIMA and Triple Exponential Smoothing",
    "short_description": "",
    "full_content": "More and more enterprises are beginning to use Docker containers to build cloud platforms. Predicting the resource usage of container workload has been an important and challenging problem to improve the performance of cloud computing platform. The existing prediction models either incur large time overhead or have insufficient accuracy. This article proposes a hybrid model of the ARIMA and triple exponential smoothing. It can accurately predict both linear and nonlinear relationships in the container resource load sequence. To deal with the dynamic Docker container resource load, the weighting values of the two single models in the hybrid model are chosen according to the sum of squares of their predicted errors for a period of time. We also design and implement a real-time prediction system that consists of the collection, storage, prediction of Docker container resource load data and scheduling optimization of CPU and memory resource usage based on predicted values. The experimental results show that the predicting accuracy of the hybrid model improves by 52.64, 20.15, and 203.72 percent on average compared to the ARIMA, the triple exponential smoothing model and ANN+SaDE model respectively with a small time overhead.",
    "author": [
      "Yulai Xie",
      "Minpeng Jin",
      "Zhuping Zou",
      "Gongming Xu",
      "Dan Feng",
      "Wenmao Liu",
      "Darrell D. E. Long"
    ],
    "date": "Apr 22, 2020",
    "url": "",
    "bibTeX": {
      "@article": "xie-tcc20",
      "author": "Yulai Xie and Minpeng Jin and Zhuping Zou and Gongming Xu and Dan Feng and Wenmao Liu and Darrell Long",
      "title": "Real-time Prediction of Docker Container Resource Load Based on A Hybrid Model of ARIMA and Triple Exponential Smoothing",
      "journal": "IEEE Transactions on Cloud Computing",
      "year": 2020,
      "month": "apr"
    }
  },
  {
    "id": 144,
    "title": "Twizzler: a Data-Centric OS for Non-Volatile Memory",
    "short_description": "",
    "full_content": "Byte-addressable, non-volatile memory ( ) presents an opportunity to rethink the entire system stack. We present , an operating system redesign for this near-future. removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. provides a clean-slate programming model for persistent data, realizing the vision of in a world of persistent RAM@. We show that is simpler, more extensible, and more secure than existing I/O models and implementations by building software for and evaluating it on DIMMs. Most persistent pointer operations in impose less than 0.5,ns added latency. operations are up to 13x faster than , and SQLite queries are up to 4.2x faster than on PMDK@. YCSB workloads ran 1.1--2.9x faster on than on native and -optimized SQLite backends.",
    "author": [
      "Daniel Bittman",
      "Peter Alvaro",
      "Pankaj Mehra",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Jul 7, 2020",
    "url": "https://ssrc.us/media/pubs/0e45877f839d4b9e6b6d87de5e97c023b8cbdd9c.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-atc21",
      "author": "Daniel Bittman and Peter Alvaro and Pankaj Mehra and Darrell D. E. Long and Ethan L. Miller",
      "title": "Twizzler: A Data-Centric OS for Non-Volatile Memory",
      "booktitle": "Proceedings of the USENIX Annual Technical Conference (ATC)",
      "year": 2021,
      "month": "jul",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 145,
    "title": "Artifice: Data in Disguise",
    "short_description": "",
    "full_content": "The challenge of deniability for sensitive data can be a life or death issue depending on location. Plausible deniability directly impacts groups such as democracy advocates relaying information in repressive regimes, journalists covering human rights stories in a war zone, and NGO workers hiding food shipment schedules from violent militias. All of these users would benefit from a plausibly deniable data storage system. Artifice presents a truly deniable storage solution through its use of external entropy and error correcting codes, while providing better reliability than other deniable storage systems.",
    "author": [
      "Austen Barker",
      "Yash Gupta",
      "Sabrina Au",
      "Eugene Chou",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2020",
    "url": "https://ssrc.us/media/pubs/f9f0f1e811d466113531bbac73f3cab0f2611bf6.pdf",
    "bibTeX": {
      "@inproceedings": "barker-msst20",
      "author": "Austen Barker and Yash Gupta and Sabrina Au and Eugene Chou and Ethan L. Miller and Darrell D. E. Long",
      "title": "Artifice: Data in Disguise",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2020,
      "month": "oct",
      "address": "Santa Clara University",
      "organization": "IEEE"
    }
  },
  {
    "id": 146,
    "title": "Measuring the Cost of Reliability in Archival Systems",
    "short_description": "",
    "full_content": "",
    "author": [
      "James Byron",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2020",
    "url": "https://ssrc.us/media/pubs/916f5482d2c0dfd400923bd43a0f6299f3aa4a81.pdf",
    "bibTeX": {
      "@inproceedings": "byron-msst20",
      "author": "James Byron and Ethan L. Miller and Darrell D. E. Long",
      "title": "Measuring the Cost of Reliability in Archival Systems",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2020,
      "month": "oct",
      "address": "Santa Clara University",
      "organization": "IEEE"
    }
  },
  {
    "id": 147,
    "title": "Geomancy: Automated Performance Enhancement through Data Layout Optimization",
    "short_description": "",
    "full_content": "Large distributed storage systems such as high-performance computing (HPC) systems used by national or international laboratories require sufficient performance and scale for demanding scientific workloads and must handle shifting workloads with ease. Ideally, data is placed in locations to optimize performance, but the size and complexity of large storage systems inhibit rapid effective restructuring of data layouts to maintain performance as workloads shift. To address these issues, we have developed , a tool that models the placement of data within a distributed storage system and reacts to drops in performance. Using a combination of machine learning techniques suitable for temporal modeling, determines when and where a bottleneck may happen due to changing workloads and suggests changes in the layout that mitigate or prevent them. Our approach to optimizing throughput offers benefits for storage systems such as avoiding potential bottlenecks and increasing overall I/O throughput from 11",
    "author": [
      "Oceane Bel",
      "Kenneth Chang",
      "Nathan R. Tallent",
      "Dirk Duellmann",
      "Ethan L. Miller",
      "Faisal Nawab",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 2020",
    "url": "https://ssrc.us/media/pubs/8ae7913294f888870063952e4a4bf25272f43ec8.pdf",
    "bibTeX": {
      "@inproceedings": "bel-msst20",
      "author": "Oceane Bel and Kenneth Chang and Nathan R. Tallent and Dirk Duellmann and Ethan L. Miller and Faisal Nawab and Darrell D. E. Long",
      "title": "Geomancy: Automated Performance Enhancement through Data Layout Optimization",
      "booktitle": "Proceedings of the Conference on Mass Storage Systems and Technologies (MSST)",
      "year": 2020,
      "month": "oct",
      "address": "Santa Clara University",
      "organization": "IEEE"
    }
  },
  {
    "id": 148,
    "title": "Twizzler: A Data-centric OS for Non-volatile Memory",
    "short_description": "",
    "full_content": "Byte-addressable, non-volatile memory ( ) presents an opportunity to rethink the entire system stack. We present , an operating system redesign for this near-future. removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. provides a clean-slate programming model for persistent data, realizing the vision of in a world of persistent RAM@. We show that is simpler, more extensible, and more secure than existing I/O models and implementations by building software for and evaluating it on DIMMs. Most persistent pointer operations in impose less than 0.5,ns added latency. operations are up to 13x faster than , and SQLite queries are up to 4.2x faster than on PMDK@. YCSB workloads ran 1.1--2.9x faster on than on native and -optimized SQLite backends.",
    "author": [
      "Daniel Bittman",
      "Peter Alvaro",
      "Pankaj Mehra",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "Jun 1, 2021",
    "url": "https://ssrc.us/media/pubs/8a6e23e0c8e3ad1d0f2813cb326abd60f2387a35.pdf",
    "bibTeX": {
      "@inproceedings": "bittman-atc21",
      "author": "Daniel Bittman and Peter Alvaro and Pankaj Mehra and Darrell D. E. Long and Ethan L. Miller",
      "title": "Twizzler: A Data-Centric OS for Non-Volatile Memory",
      "booktitle": "Proceedings of the USENIX Annual Technical Conference (ATC)",
      "year": 2021,
      "month": "jul",
      "organization": "Usenix Association"
    }
  },
  {
    "id": 149,
    "title": "A Multiple Snapshot Attack on Deniable Storage Systems",
    "short_description": "",
    "full_content": "Multiple snapshot attacks, where an adversary is able to gain access to two or more images of a disk, have often been proposed in the deniable storage system literature; however, there have been no concrete attacks proposed or carried out. This paper describes a multiple snapshot attack for use against deniable storage systems, using Artifice as an example system. We designed and implemented the first multiple snapshot attack against a deniable storage system.",
    "author": [
      "Kyle Fredrickson",
      "Austen Barker",
      "Darrell D. E. Long"
    ],
    "date": "Oct 9, 2021",
    "url": "https://ssrc.us/media/pubs/b5d2cdef04702983d608a8f3ca2994db1fc22f2d.pdf",
    "bibTeX": {
      "@inproceedings": "fredrickson-mascots21",
      "author": "Kyle Fredrickson and Austen Barker and Darrell Long",
      "title": "A Multiple Snapshot Attack on Deniable Storage Systems",
      "booktitle": "Proceedings of the Twenty-ninth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 2021)",
      "year": 2021,
      "month": "nov",
      "organization": "IEEE"
    }
  },
  {
    "id": 150,
    "title": "Lethe: Secure Deletion by Addition",
    "short_description": "",
    "full_content": "Modern data privacy regulations such as GDPR, CCPA, and CDPA stipulate that data pertaining to a user must be deleted without undue delay upon the user's request. Existing systems are not designed to comply with these regulations and can leave traces of deleted data for indeterminate periods of time, often as long as months. We developed Lethe to address these problems by providing fine-grained secure deletion on any system and any storage medium, provided that Lethe has access to a fixed, small amount of securely-deletable storage. Lethe achieves this using keyed hash forests (KHFs), extensions of keyed hash trees (KHTs), structured to serve as efficient representations of encryption key hierarchies. By using a KHF as a regulator for data access, Lethe provides its secure deletion not by removing the KHF, but by adding a new KHF that only grants access to still-valid data.",
    "author": [
      "Eugene Chou",
      "Leo Conrad-Shah",
      "Austen Barker",
      "Andrew Quinn",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "May 8, 2023",
    "url": "https://ssrc.us/media/pubs/ded7bf9486d2d05ce6eb53026b887aa6869b4a0b.pdf",
    "bibTeX": {
      "@inproceedings": "chou-cheops23",
      "author": "Eugene Chou and Leo Conrad-Shah and Austen Barker and Andrew Quinn and Ethan L. Miller and Darrell D. E. Long",
      "title": "Lethe: Secure Deletion by Addition",
      "booktitle": "Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems (CHEOPS '23)",
      "year": 2023,
      "month": "may",
      "address": "Rome",
      "organization": "ACM"
    }
  },
  {
    "id": 151,
    "title": "Avoiding Surprise in an Era of Global Technology Advances",
    "short_description": "",
    "full_content": "",
    "author": [
      "Ruth A. David",
      "Steven R.J. Brueck",
      "Stephen W. Drew",
      "Alan H. Epstein",
      "Robert A. Furman",
      "Sharon Glotzer",
      "Christopher C. Green",
      "Diane E. Griffin",
      "J. Jerome Holton",
      "Michael R. Ladisch",
      "Darrell Long",
      "Frederick R. Lopez",
      "Richard M. Osgood",
      "Stewart D. Personick",
      "Alton D. Romig",
      "S. Shankar Sasrty",
      "James B. Smith",
      "Camillo J. Taylor",
      "Diane Wiley"
    ],
    "date": "2005",
    "url": "https://scholar.google.com/scholar?q=Avoiding%20Surprise%20in%20an%20Era%20of%20Global%20Technology%20Advances",
    "bibTeX": {
      "@book": "david-nrc05",
      "author": "Ruth A. David and Steven R.J. Brueck and Stephen W. Drew and Alan H. Epstein and Robert A. Furman and Sharon Glotzer and Christopher C. Green and Diane E. Griffin and J. Jerome Holton and Michael R. Ladisch and Darrell Long and Frederick R. Lopez and Richard M. Osgood and Stewart D. Personick and Alton D. Romig and S. Shankar Sasrty and James B. Smith and Camillo J. Taylor and Diane Wiley",
      "title": "Avoiding Surprise in an Era of Global Technology Advances",
      "publisher": "National Academies Press",
      "year": 2005,
      "address": "Washington, DC"
    }
  },
  {
    "id": 152,
    "title": "Theory of Finite Automata",
    "short_description": "",
    "full_content": "",
    "author": [
      "John L. Carroll",
      "Darrell D. E. Long"
    ],
    "date": "1989",
    "url": "https://scholar.google.com/scholar?q=Theory%20of%20Finite%20Automata",
    "bibTeX": {
      "@book": "carroll-prentice89",
      "author": "John L. Carroll and Darrell D. E. Long",
      "title": "Theory of Finite Automata",
      "publisher": "Prentice-Hall",
      "year": 1989,
      "address": "Englewood Cliffs, New Jersey"
    }
  },
  {
    "id": 153,
    "title": "Video-on-Demand Broadcasting Protocols",
    "short_description": "",
    "full_content": "Broadcasting protocols are proved to be efficient for transmitting most of the popular videos in video-on-demand systems. We propose a generalized analytical approach to evaluate the efficiency of the broadcasting protocols and derive the theoretical lower bandwidth requirement bound for any periodic broadcasting protocols. By means of the proposed analytical tool-temporal-bandwidth map, the approach can be used to direct the design of periodic broadcasting protocols to achieve different goals, e.g., server bandwidth requirement, client waiting time, client I/O bandwidth requirement etc. As the most important performance index in a VOD system is the required server bandwidth, we give the solution to achieve the optimal bandwidth efficiency given client waiting time requirement and the length of the video. To take into account the popular compressed video with variable bit rate, the optimal approach is applied readily to the VBR videos and can achieve zero loss and best bandwidth efficiency. We give a proof why existing techniques such as smoothing and prefetching is not necessary and in some cases inefficient in broadcasting protocols. We also discuss how broadcasting schemes can be tailored to support true and interactive VOD service. An insightful comparison between broadcasting and multicasting schemes is also given.",
    "author": [
      "Steven W. Carter",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "2000",
    "url": "https://scholar.google.com/scholar?q=Video-on-Demand%20Broadcasting%20Protocols",
    "bibTeX": {
      "@incollection": "carter-academic00",
      "author": "Steven W. Carter and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Video-on-Demand Broadcasting Protocols",
      "booktitle": "Multimedia Systems and Techniques",
      "editor": "Borko Furht",
      "publisher": "Kluwer Academic",
      "year": 2000,
      "address": "Boston"
    }
  },
  {
    "id": 154,
    "title": "Simulation Modeling of Weak-Consistency Protocols",
    "short_description": "",
    "full_content": "Weak-consistency replication protocols can be used to build wide-area services that are scalable, fault-tolerant, and useful for mobile computer systems. We have evaluated the timestamped anti-entropy protocol to determine how well it meets these goals. In this protocol, pairs of replicas periodically exchange update messages; in this way updates eventually propagate to all replicas. In this paper we present results from a detailed simulation analysis of the fault tolerance and the consistency provided by this protocol. The protocol is extremely robust in the face of site and network failure, and it scales well to large numbers of replicas. : wide-area communication, scalability, replication, fault tolerance.",
    "author": [
      "Richard A. Golding",
      "Darrell D. E. Long"
    ],
    "date": "1999",
    "url": "https://scholar.google.com/scholar?q=Simulation%20Modeling%20of%20Weak-Consistency%20Protocols",
    "bibTeX": {
      "@incollection": "golding-gordon99",
      "author": "Richard A. Golding and Darrell D. E. Long",
      "title": "Simulation Modeling of Weak-Consistency Protocols",
      "booktitle": "Network Systems Design",
      "editor": "E. Gelenbe and K. Bagchi and G. Zobrist",
      "publisher": "Gordon and Breach Science Publishers",
      "year": 1999,
      "pages": "161--186"
    }
  },
  {
    "id": 155,
    "title": "Using Simulation to Evaluate Consistency Protocols for Replicated Data",
    "short_description": "",
    "full_content": "The need for reliable and cost-effective data storage grows as digital information becomes increasingly ubiquitous. Archival systems must store valuable data for years while adapting to changing user needs, capacity, and performance requirements. Storage devices differ in terms of performance, capacity, reliability, acquisition cost, power consumption, and the rates at which their features change over time. Today, numerous alternative and emerging storage technologies may become viable for use in archival storage systems as their cost, performance, and reliability change. Differences between storage devices also complicate the choice of the best storage technology to use for an archival system. We have designed a simulator that models the capacity, performance, acquisition cost, and power cost of archival systems. Our simulator compares four storage technologies that exhibit different cost and performance characteristics: tape, optical disc, hard disk, and NAND flash SSD. We evaluate the total cost of ownership for each storage technology within an archival system, and we explore the effect that prospective technological advancements and growth rates over time may have on the relative cost and viability of each storage technology. We show that, in order for each storage technology to remain cost-effective in archival systems over time, it must increase in capacity at least as quickly as the demand for storage overall. We compare trends in storage technologies to suggest developments that could minimize the long-term total cost of ownership for archival systems. We show that hard disks and flash could become cost-competitive with tape-based archives by adopting new designs to minimize infrastructure and electricity costs.",
    "author": [
      "Darrell D. E. Long",
      "John L. Carroll"
    ],
    "date": "1995",
    "url": "https://scholar.google.com/scholar?q=Using%20Simulation%20to%20Evaluate%20Consistency%20Protocols%20for%20Replicated%20Data",
    "bibTeX": {
      "@incollection": "long-ablex95",
      "author": "Darrell D. E. Long and John L. Carroll",
      "title": "Using Simulation to Evaluate Consistency Protocols for Replicated Data",
      "booktitle": "Progress in Simulation II",
      "editor": "G. W. Zobrist and J. V. Leonard",
      "publisher": "Ablex Publishing",
      "year": 1995
    }
  },
  {
    "id": 156,
    "title": "A Novel Hybrid Model for Docker Container Workload Prediction",
    "short_description": "",
    "full_content": "The emergence of containers dramatically simplifies and facilitates the development and deployment of applications. More and more enterprises deploy their applications on the container cloud platform. For cloud service providers, an effective container workload prediction method is a must to achieve efficient utilization of cloud resources. However, the existing methods are either rarely based on container load characteristics or cannot make accurate real-time predictions. In this paper, we propose a Docker container workload proactive prediction method using a hybrid model combining triple exponential smoothing and long short-term memory (LSTM), which not only can capture both short-term and long-term dependencies in container resource time series but also smooth the container resource utilization data. In order to improve the prediction accuracy of the hybrid model, those two single models are combined using the mean absolute percentage error (MAPE) method. Besides, we design a real-time Docker workload prediction system for the hybrid model. Our experiments show that the mean absolute percentage error of the hybrid model is decreased by an average of 3.24%, 12.18%, 13.42%, 43.45%, and 50.69% compared with the LSTM, the triple exponential smoothing, ES-ARIMA, Bayesian Ridge Regression and BiLSTM with an acceptable time and computational cost overhead.",
    "author": [
      "Liangkang Zhang",
      "Yulai Xie",
      "Minpeng Jin",
      "Pan Zhou",
      "Gongming Xu",
      "Yafeng Wu",
      "Dan Feng",
      "Darrell Long"
    ],
    "date": "February 2023",
    "url": "https://ieeexplore.ieee.org/document/10052731",
    "bibTeX": {
      "@article": "zhang-tnsm",
      "author": "Liangkang Zhang and Yulai Xie and Minpeng Jin and Pan Zhou and Gongming Xu and Yafeng Wu and Dan Feng and Darrell Long",
      "title": "A Novel Hybrid Model for Docker Container Workload Prediction",
      "journal": "IEEE Transactions on Network and Service Management",
      "volume": 20,
      "number": 3,
      "month": "feb",
      "year": 2023
    }
  },
  {
    "id": 157,
    "title": "Paradise: Real-time, Generalized, and Distributed Provenance-Based Intrusion Detection",
    "short_description": "",
    "full_content": "Identifying intrusion from massive and multi-source logs accurately and in real-time presents challenges for today's users. This paper presents Paradise, a real-time, generalized, and distributed provenance-based intrusion detection method. Paradise introduces a novel extract strategy to prune and extract process feature vectors from provenance dependencies at the system log level, and it stores them in high-efficiency memory databases. Using this strategy, Paradise does not depend on the specific operating system type or provenance collection framework. Provenance-based dependencies are calculated independently during the detection phase, thus, Paradise can negotiate all detection results from multiple detectors without extra communication overhead between detectors. Paradise also employs an efficient load-balanced distribution scheme that enhances the Kafka architecture to efficiently distribute provenance graph feature vectors to the detectors.",
    "author": [
      "Yafeng Wu",
      "Yulai Xie",
      "Xuelong Liao",
      "Pan Zhou",
      "Dan Feng",
      "Lin Wu",
      "Xuan Li",
      "Avani Wildani",
      "Darrell Long"
    ],
    "url": "https://ieeexplore.ieee.org/document/9739913",
    "bibTeX": {
      "@article": "wu-tdsc",
      "author": "Yafeng Wu and Yulai Xie and Xuelong Liao and Pan Zhou and Dan Feng and Lin Wu and Xuan Li and Avani Wildani and Darrell Long",
      "title": "Paradise: Real-time, Generalized, and Distributed Provenance-Based Intrusion Detection",
      "journal": "IEEE Transactions on Dependable and Secure Computing",
      "volume": 20,
      "number": 2,
      "month": "mar",
      "year": 2022
    }
  },
  {
    "id": 158,
    "title": "Rethinking the Adversary and Operational Characteristics of Deniable Storage",
    "short_description": "",
    "full_content": "Aim: With the widespread adoption of disk encryption technologies, it has become common for adversaries to employ coercive tactics to force users to surrender encryption keys. For some users, this creates a need for hidden volumes that provide plausible deniability, the ability to deny the existence of sensitive information. Previous deniable storage solutions only offer pieces of an implementable solution that do not take into account more advanced adversaries, such as intelligence agencies, and operational concerns. Specifically, they do not address an adversary that is familiar with the design characteristics of any deniable system. Methods: We evaluated existing threat models and deniable storage system designs to produce a new, stronger threat model and identified design characteristics necessary in a plausibly deniable storage system. To better explore the implications of this stronger adversary, we developed Artifice, the first tunable, operationally secure, self repairing, and fully deniable storage system. Results: With Artifice, hidden data blocks are split with an information dispersal algorithm such as Shamir Secret Sharing to produce a set of obfuscated carrier blocks that are indistinguishable from other pseudorandom blocks on the disk. The blocks are then stored in unallocated space of an existing file system. The erasure correcting capabilities of an information dispersal algorithm allow Artifice to self repair damage caused by writes to the public file system. Unlike preceding systems, Artifice addresses problems regarding flash storage devices and multiple snapshot attacks through simple block allocation schemes and operational security measures. To hide the user’s ability to run a deniable system and prevent information leakage, a user accesses Artifice through a separate OS stored on an external Linux live disk. Conclusion: In this paper, we present a stronger adversary model and show that our proposed design addresses the primary weaknesses of existing approaches to deniable storage under this stronger assumed adversary.",
    "author": [
      "Austen Barker",
      "Yash Gupta",
      "James Hughes",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "May 27, 2021",
    "url": "https://scholar.google.com/scholar?q=Rethinking%20the%20Adversary%20and%20Operational%20Characteristics%20of%20Deniable%20Storage",
    "bibTeX": {
      "@article": "barker-jsss21",
      "author": "Austen Barker and Yash Gupta and James Hughes and Ethan L. Miller and Darrell D. E. Long",
      "title": "Rethinking the Adversary and Operational Characteristics of Deniable Storage",
      "journal": "Journal of Surveillance, Security and Safety",
      "volume": 2,
      "year": 2021,
      "month": "may"
    }
  },
  {
    "id": 159,
    "title": "Valet: Efficient Data Placement on Modern SSDs",
    "short_description": "",
    "full_content": "Valet leverages userspace shim layers to add placement hints for application data, delivering up to 2-4x write throughput over filesystems and comparable or better performance than application-specific solutions, with up to 6x lower tail latency. Valet generates dynamic placement hints, remapping application data to modern SSDs with zero modifications to the application, the filesystem, or the kernel. To the authors' knowledge, this is the first work to present a generalized theory of data placement, showcasing affinity and lifetime as the important parameters over temperature-based approaches of the past. Valet is fast: achieving 2-6 times higher write throughput, up to 6 times lower latency, and reduced garbage overhead over filesystems and application backends.",
    "author": [
      "Devashish R. Purandare",
      "Peter Alvaro",
      "Avani Wildani",
      "Darrell D. E. Long",
      "Ethan L. Miller"
    ],
    "date": "November 2025",
    "url": "https://scholar.google.com/scholar?q=Valet%3A%20Efficient%20Data%20Placement%20on%20Modern%20SSDs",
    "bibTeX": {
      "@inproceedings": "purandare-socc25",
      "author": "Devashish R. Purandare and Peter Alvaro and Avani Wildani and Darrell D. E. Long and Ethan L. Miller",
      "title": "Valet: Efficient Data Placement on Modern SSDs",
      "booktitle": "Proceedings of the Sixteenth Symposium on Cloud Computing (SOCC '25)",
      "year": 2025,
      "month": "nov",
      "organization": "ACM"
    }
  },
  {
    "id": 160,
    "title": "Sparta: Practical Anonymity with Long-Term Resistance to Traffic Analysis",
    "short_description": "",
    "full_content": "Existing metadata-private messaging systems are either non-scalable or vulnerable to long-term traffic analysis. Approaches that mitigate traffic analysis attacks often suffer from unrealistic and unimplementable assumptions or impose system-wide bandwidth restrictions, degrading usability, and performance. In this work, we present a new model for metadata-private communication systems---deferred retrieval---that guarantees traffic analysis resistance under realistic, implementable user assumptions. We introduce Sparta systems, practical and scalable instantiations of deferred retrieval that are distributable, achieve high throughput, and support multiple concurrent conversations without message loss. Specifically, we present three Sparta constructions optimized for different scenarios: (i) low-latency, (ii) high-throughput in shared-memory environments (multi-thread implementations), and (iii) high throughput in shared-nothing (distributed) environments. Our low latency Sparta supports latencies of less than one millisecond, while our high-throughput Sparta can scale to deliver over 700,000 100B messages per second on a single 48-core server.",
    "author": [
      "Kyle Fredrickson",
      "Ioannis Demertzis",
      "James P. Hughes",
      "Darrell D. E. Long"
    ],
    "date": "May 2025",
    "url": "https://scholar.google.com/scholar?q=Sparta%3A%20Practical%20Anonymity%20with%20Long-Term%20Resistance%20to%20Traffic%20Analysis",
    "bibTeX": {
      "@inproceedings": "fredrickson-sp25",
      "author": "Kyle Fredrickson and Ioannis Demertzis and James P. Hughes and Darrell D. E. Long",
      "title": "Sparta: Practical Anonymity with Long-Term Resistance to Traffic Analysis",
      "booktitle": "Proceedings of the Forty-fifth IEEE Symposium on Security and Privacy (S\\&P)",
      "year": 2025,
      "month": "may",
      "address": "San Francisco",
      "organization": "IEEE"
    }
  },
  {
    "id": 161,
    "title": "Accurate Generation of I/O Workloads Using Generative Adversarial Networks",
    "short_description": "",
    "full_content": "It is essential to utilize a large number of I/O workloads to analyze commodity system performance or simulate scientific phenomena in high-performance scientific computing. I/O traces are often unavailable at scale due to trace storage overhead, privacy concerns, and the performance impact of trace instrumentation. We study how to generate sufficiently representative I/O workloads using Generative Adversarial Networks (GANs). The best GAN architecture can generate I/O workloads with maximum mean discrepancy (MMD) as low as 0.015-0.05, which implies the synthetic I/O workloads have successfully learned the potential distribution of real I/O traces. We demonstrate that the performance similarity between the original I/O trace and the generated I/O workload through trace replay can be 90.36",
    "author": [
      "Heyu Zhang",
      "Zhen Yang",
      "Yulai Xie",
      "Yafeng Wu",
      "Jiakun Li",
      "Dan Feng",
      "Avani Wildani",
      "Darrell Long"
    ],
    "date": "November 2024",
    "url": "https://scholar.google.com/scholar?q=Accurate%20Generation%20of%20I%2FO%20Workloads%20Using%20Generative%20Adversarial%20Networks",
    "bibTeX": {
      "@inproceedings": "zhang-nas24",
      "author": "Heyu Zhang and Zhen Yang and Yulai Xie and Yafeng Wu and Jiakun Li and Dan Feng and Avani Wildani and Darrell Long",
      "title": "Accurate Generation of I/O Workloads Using Generative Adversarial Networks",
      "booktitle": "Proceedings of the Seventeenth IEEE International Conference on Networking, Architecture, and Storage (NAS 2024)",
      "year": 2024,
      "month": "nov",
      "address": "Guangzhou, China",
      "organization": "IEEE"
    }
  },
  {
    "id": 162,
    "title": "WinnowML: Stable Feature Selection for Maximizing Prediction Accuracy of Time-based System Modeling",
    "short_description": "",
    "full_content": "Online deep learning (ODL) has become an important methodology for modeling time-based performance of computer systems. An open problem is the intelligent selection of features from raw workload traces of computer systems. The best methods are overly sensitive to noisy data, causing frequent feature changes and re-training. Using all available features inflates training time and introduces model artifacts if some features should have been dropped. We present WinnowML, a method for automatically determining the most relevant feature subset for a predictive time-series model. WinnowML combines existing feature ranking algorithms and a history of each feature's ranking to iteratively rank a feature set to lower prediction error and maximize long term relevance. From this ranked feature set, the most relevant and stable subset is selected to train a model. Experimentally, we show how WinnowML can lower a model's mean absolute relative error up to 42% on average compared to the closest performing approach. Additionally, we lower the fluctuation in feature ranking and selection up to 65%. We also demonstrate how to combine WinnowML and a model search tool to provide improvements in performance of up to 14.5% when compared to using all the feature available.",
    "author": [
      "Oceane Bel",
      "Sinjoni Mukhopadhyay",
      "Nathan Tallent",
      "Faisal Nawab",
      "Darrell Long"
    ],
    "date": "December 2021",
    "url": "https://scholar.google.com/scholar?q=WinnowML%3A%20Stable%20Feature%20Selection%20for%20Maximizing%20Prediction%20Accuracy%20of%20Time-based%20System%20Modeling",
    "bibTeX": {
      "@inproceedings": "bel-bigdata21",
      "author": "Oceane Bel and Sinjoni Mukhopadhyay and Nathan Tallent and Faisal Nawab and Darrell Long",
      "title": "WinnowML: Stable Feature Selection for Maximizing Prediction Accuracy of Time-based System Modeling",
      "booktitle": "IEEE International Conference on Big Data (Big Data)",
      "year": 2021,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 163,
    "title": "Introducing SeaOS",
    "short_description": "",
    "full_content": "SeaOS is a simple operating system developed for fast prototyping of research concepts. SeaOS implements a POSIX-like interface in less than 30,000 lines of C kernel code. Its userspace is complete enough to be self-hosting and includes ports of gcc, bash, grub, binutils, and coreutils. SeaOS runs on modern x86 and x86_64 architectures and is designed to support our lab's operating system research in scheduling, virtualization and security architectures.",
    "author": [
      "Daniel Bittman",
      "D.J. Capelis",
      "Darrell Long"
    ],
    "date": "May 2014",
    "url": "https://scholar.google.com/scholar?q=Introducing%20SeaOS",
    "bibTeX": {
      "@inproceedings": "bittman-icisa14",
      "author": "Daniel Bittman and D.J. Capelis and Darrell Long",
      "title": "Introducing SeaOS",
      "booktitle": "Proceedings of the 2014 International Conference on Information Science and Applications (ICISA)",
      "year": 2014,
      "month": "may",
      "address": "Seoul, Korea",
      "pages": "1--3",
      "organization": "IEEE"
    }
  },
  {
    "id": 164,
    "title": "Future File Systems",
    "short_description": "",
    "full_content": "",
    "author": [
      "David Pease",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 2008",
    "url": "https://scholar.google.com/scholar?q=Future%20File%20Systems",
    "bibTeX": {
      "@techreport": "pease-tr08",
      "author": "David Pease and Darrell D. E. Long",
      "title": "Future File Systems",
      "institution": "University of California, Santa Cruz",
      "year": 2008,
      "month": "sep"
    }
  },
  {
    "id": 165,
    "title": "Experimentally Evaluating In-Place Delta Reconstruction",
    "short_description": "",
    "full_content": "",
    "author": [
      "Randal Burns",
      "Larry Stockmeyer",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 1999",
    "url": "https://scholar.google.com/scholar?q=Experimentally%20Evaluating%20In-Place%20Delta%20Reconstruction",
    "bibTeX": {
      "@inproceedings": "kulkarni-dcc99",
      "author": "Randal Burns and Larry Stockmeyer and Darrell D. E. Long",
      "title": "Experimentally Evaluating In-Place Delta Reconstruction",
      "booktitle": "Proceedings of the Data Compression Conference (DCC)",
      "year": 1999,
      "month": "mar",
      "address": "Snowbird",
      "organization": "IEEE"
    }
  },
  {
    "id": 166,
    "title": "A Technique for Managing Mirrored Disks",
    "short_description": "",
    "full_content": "When data is replicated, an access protocol must be chosen to insure the presentation of a consistent view of the data. Protocols based on quorum consensus provide good availability with the added benefit of mutual exclusion. Of the protocols based on quorum consensus, the dynamic voting protocols provide the highest known availability. We describe a dynamic voting protocol that does not need the instantaneous state information required by the same performance as the original dynamic voting in the asymptotic case, and quickly converges to it for realistic access rates. Our protocol does this at a cost in network similar to that of statistic majority consensus voting. The first realistic analysis of the availability afforded by dynamic voting protocols is presented, taking the access frequency into account. The analysis confirms our hypothesis that delaying state information does not appreciably affect availability. Discrete event simulation is used to confirm and to extend the results we obtain using analytic models.",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 1988",
    "url": "https://scholar.google.com/scholar?q=A%20Technique%20for%20Managing%20Mirrored%20Disks",
    "bibTeX": {
      "@inproceedings": "long-srdsds88",
      "author": "Darrell D. E. Long",
      "title": "A Technique for Managing Mirrored Disks",
      "booktitle": "Proceedings of the Symposium on Reliability in Distributed Software and Database Systems (SRDSDS)",
      "year": 1988,
      "month": "oct",
      "address": "Los Angeles",
      "organization": "IEEE"
    }
  },
  {
    "id": 167,
    "title": "REINAS: the Real-Time Environmental Information Network and Analysis System",
    "short_description": "",
    "full_content": "The Real-Time Environmental Information Network and Analysis System (REINAS) is a distributed system supporting the conduct of regional environmental science research at the desk top. Continuous real-time data acquired from dispersed sensors is stored in a logically integrated but physically distributed data base. An integrated problem-solving environment is under development which supports visualization and modeling. REINAS is intended to provide insight into historical, current, and predicted oceanographic and meteorological conditions. REINAS permits both collaborative and single-user scientific work in a distributed environment. abstract : distributed data base, heterogeneous data base, real-time data base, temporal data base, scientific data base, geographical information system, environmental monitoring.",
    "author": [
      "Darrell D. E. Long",
      "Patrick E. Mantey",
      "Craig M. Wittenbrink",
      "Theodore R. Haining",
      "Bruce R. Montague"
    ],
    "date": "Mar 1, 1995",
    "url": "https://scholar.google.com/scholar?q=REINAS%3A%20the%20Real-Time%20Environmental%20Information%20Network%20and%20Analysis%20System",
    "bibTeX": {
      "@inproceedings": "long-compass95",
      "author": "Darrell D. E. Long and Patrick E. Mantey and Craig M. Wittenbrink and Theodore R. Haining and Bruce R. Montague",
      "title": "REINAS: the Real-Time Environmental Information Network and Analysis System",
      "booktitle": "Proceedings of the Tenth Annual Conference on Computer Assurance (COMPASS)",
      "year": 1995,
      "month": "mar",
      "address": "Gaithersburg, Maryland",
      "publisher": "IEEE"
    }
  },
  {
    "id": 168,
    "title": "Analysis of Caching Algorithms for Distributed File Systems",
    "short_description": "",
    "full_content": "When picking a cache replacement policy for file systems, LRU (Least Recently Used) has always been the obvious choice, because of the temporal locality found in programs and data. However, in the case of Sun NFS servers much of the locality is filtered out by the client cache. It has been conjectured that this filtering of locality by the client caches render LRU ineffective as a server cache replacement policy. This study disproves the conjecture by simulating a NFS server cache with real worm traces. Traces were taken of NFS read and write requests sent to an NFS server. These traces were then run through a cache simulator using six different cache replacement policies: Least Recently Used (LRU), Least Frequently Used (LFU), Frequency Based Replacement (FBR), First In First Out (FIFO), Random (RAND), and Optimal (OPT). RAND and OPT were used to provide lower and upper bounds on performance. Results show that LRU is an effective NFS server cache replacement policy and frequency based tend to exhibit erratic behavior in the presence of temporal locality and sequentially accessed files.",
    "author": [
      "Benjamin Reed",
      "Darrell D. E. Long"
    ],
    "date": "July 1996",
    "url": "https://scholar.google.com/scholar?q=Analysis%20of%20Caching%20Algorithms%20for%20Distributed%20File%20Systems",
    "bibTeX": {
      "@article": "reed-osr96",
      "author": "Benjamin Reed and Darrell D. E. Long",
      "title": "Analysis of Caching Algorithms for Distributed File Systems",
      "journal": "ACM SIGOPS Operating Systems Review",
      "volume": 30,
      "number": 3,
      "year": 1996,
      "month": "jul",
      "pages": "12--17"
    }
  },
  {
    "id": 169,
    "title": "REINAS: Real-Time Environmental Information Network",
    "short_description": "",
    "full_content": "*Abstract The Real-Time Environmental Information Network and Analysis System (REINAS) is a continuing engineering research and development program with the goal of designing, developing and testing an operational prototype system for data acquisition, data management, and visualization. This system is to support the real-time utilization of advanced instrumentation in environmental science. Current applications are in meteorology and oceanography; others envisioned include geophysics, hydrology, air pollution, etc. The REINAS project has been funded by a Department of Defense University Research Initiative through the Office of Naval Research. Participating institutions include the Naval Postgraduate School (NPS), the Monterey Bay Aquarium Research Institute (MBARI) and the Baskin Center for Computer Engineering and Information Sciences of the University of California, Santa Cruz (UCSC). The REINAS system has been designed for regional real-time environmental monitoring and analysis. It is a modern system, integrated into the Internet, for conducting interactive real-time coastal air/ocean science. The database design of REINAS is independent of specific database technology and is designed to support operational scientific needs throughput the entire scientific data life-cycle. During the previous phases a survey of available technologies was made, selections of the those to be used in the prototype system were made, and detailed architecture of REINAS and experimentation with subsystems for data collection, data management, processing, and visualization were started. This report documents the status of REINAS in Phase IV -- the Experimentation and System Verification Phase. *Acknowledgements We wish to acknowledge the dedicated work of all the UCSC faculty, students and staff, and especially our partners from MBARI, NPS and other interested groups. It is difficult to name all those who participated in the REINAS project, but the following contributed directly to the work of Phase IV in 1994 and 1995: .2in itemize UCSC Faculty and Staff: Patrick E. Mantey, Daniel M. Fernandez, Harwood G. Kolsky, Glen G. Langdon, Suresh Lodha, J.J. Garcia-Luna, Darrell D. E. Long, Andrew Muir, Alex T. Pang, Stephen Petersen, Craig M. Wittenbrink. .1in NPS Faculty and Staff: Richard Lind, Paul Hirshberg, Wendell A. Nuss, Jeffrey Paduan. .1in MBARI Staff: Bruce R. Gritton, Francisco Chavez. itemize *.2in Students who worked on the REINAS Project during Phase IV: *.2in itemize System and DBMS Architecture: Bruce R. Montague, Eric C. Rosen, Linda Begley, Jose Mendoza, Theodore R. Haining, Carles Pi-Sunyer. .1in Instrumentation: Eric C. Rosen, Catherine Tornabene, Robert Sheehan, John Wiederhold, Sume Biak, Amelia Graf, Ted Goodman, Dan Harris, Mike Allen, Suzana Djurcilov. .1in Instrument Interfaces: Michelle D. Abram, Jimmy Chan, Ted Dunn, Scott Lin, Chris Thomas, David K. Schreiber. .1in Networking Research: Chane L. Fullmer, Jochen Behrens, Hans-Peter Dommel, Brian Levine, Dean Long, Shree Murthy, Clay Shields. .1in Visualization System: Naim Alper, Jeffrey J. Furman, Tom H. Goodman, Elijah C. Saxon. .1in Modeling: Cheng Tang. .1in Data Compression: William W. Macy, Robert J. Antonucci, David Kulp, James Spring, Yi Zhou. itemize",
    "author": [
      "Daniel M. Fernandez",
      "Patrick E. Mantey",
      "Darrell D. E. Long",
      "Eric Rosen",
      "Craig M. Wittenbrink"
    ],
    "date": "May 1, 1996",
    "url": "https://scholar.google.com/scholar?q=REINAS%3A%20Real-Time%20Environmental%20Information%20Network",
    "bibTeX": {
      "@inproceedings": "fernandez-cps96",
      "author": "Daniel M. Fernandez and Patrick E. Mantey and Darrell D. E. Long and Eric Rosen and Craig M. Wittenbrink",
      "title": "REINAS: Real-Time Environmental Information Network",
      "booktitle": "Proceedings of the International Symposium on Computer-Aided Control System Design",
      "year": 1996,
      "month": "may",
      "publisher": "IEEE"
    }
  },
  {
    "id": 170,
    "title": "A Note On Bit-Mapped Free Sector Management",
    "short_description": "",
    "full_content": "",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "April 1993",
    "url": "https://scholar.google.com/scholar?q=A%20Note%20On%20Bit-Mapped%20Free%20Sector%20Management",
    "bibTeX": {
      "@article": "long-osr93",
      "author": "Darrell D. E. Long",
      "title": "A Note On Bit-Mapped Free Sector Management",
      "journal": "Operating Systems Review",
      "volume": 27,
      "number": 2,
      "year": 1993,
      "month": "apr",
      "pages": "7--9"
    }
  },
  {
    "id": 172,
    "title": "Using MEMS-Based Storage in Computer Systems -- MEMS Storage Architectures",
    "short_description": "",
    "full_content": "As an emerging nonvolatile secondary storage technology, MEMS-based storage exhibits several desirable properties including high performance, high storage volumic density, low power consumption, low entry cost, and small form factor. However, MEMS-based storage provides a limited amount of storage per device and is likely to be more expensive than magnetic disk. Systems designers will therefore need to make trade-offs to achieve well-balanced designs. We present an architecture in which MEMS devices are organized into MEMS storage enclosures with online spares. Such enclosures are proven to be highly reliable storage building bricks, with no maintenance during their economic lifetimes. We also demonstrate the effectiveness of using MEMS as another layer in the storage hierarchy, bridging the cost and performance gap between MEMS storage and disk. We show that using MEMS as a disk cache can significantly improve system performance and cost-performance ratio. General Terms: Economics, Management, Performance, Reliability Additional Key Words and Phrases: MEMS-based storage, storage enclosures, maintenance strategies, economic lifetime, hybrid storage devices, cost-performance",
    "author": [
      "Bo Hong",
      "Feng Wang",
      "Scott A. Brandt",
      "Darrell D. E. Long",
      "Thomas J. E. Schwarz"
    ],
    "date": "Feb 1, 2006",
    "url": "https://escholarship.org/content/qt8076d22w/qt8076d22w.pdf",
    "bibTeX": {
      "@article": "hong-tos06a",
      "author": "Bo Hong and Feng Wang and Scott A. Brandt and Darrell D. E. Long and Thomas J. E. Schwarz",
      "title": "Using MEMS-Based Storage in Computer Systems -- MEMS Storage Architectures",
      "journal": "ACM Transactions on Storage",
      "volume": 2,
      "number": 1,
      "year": 2006,
      "month": "feb",
      "pages": "1--21"
    }
  },
  {
    "id": 173,
    "title": "Authenticating Network Attached Storage",
    "short_description": "",
    "full_content": "flushleft We present an architecture for network-authenticated disks that implements distributed file systems without file servers or encryption. Our system provides network clients with direct network access to remote storage. flushleft The need to access anything from anywhere has increased the role of distributed file servers in computing. Distributed file systems provide local file system semantics for access to remote storage. This allows network clients to incorporate the remote storage into their local file system. File semantics are well understood by users and applications, making distributed file servers a convenient tool in developing distributed applications. As the role played by distributed file systems expands, problems with their design become increasingly evident. Faster clients, high-bandwidth connections, and larger drive capacities increase the demand on file servers. Although it would seem that the I/O capacity of the system storage devices would limit network file server performance, in actuality, file servers frequently are CPU bound. Riedel and Gibson discovered that, even with low overall CPU utilization, burst loads were sufficiently intense to overuse the server . In addition to the performance problems of distributed network file systems, security also presents a problem. Applications that rely on distributed file systems should not be compromised by security weaknesses of the file systems on which they are built. Local file systems have a single kernel that restricts access to file data, but because a distributed file system involves multiple servers and clients, it cannot rely on a single kernel to restrict access. The security risk is even greater since the network that connects servers and clients may also pose a threat. The authenticated network-attached disks we present address these problems by providing an architecture based on one-way hash functions that make available mutual authentication of the network disks and the clients. This architecture obviates the need for more performance-intensive authentication methods such as public-key encryption and Kerberos , but does not preclude their use. The authentication protocol used by the network storage is very simple and flexible, and allows keys to be created and managed using existing authentication systems.",
    "author": [
      "Benjamin C. Reed",
      "Darrell D. E. Long",
      "Edward G. Chron",
      "Randal C. Burns"
    ],
    "date": "Jan 1, 2000",
    "url": "https://ieeexplore.ieee.org/document/820053/",
    "bibTeX": {
      "@article": "reed-micro00",
      "author": "Benjamin C. Reed and Darrell D. E. Long and Edward G. Chron and Randal C. Burns",
      "title": "Authenticating Network Attached Storage",
      "journal": "IEEE Micro",
      "volume": 20,
      "number": 1,
      "year": 2000,
      "month": "jan",
      "pages": "49--57"
    }
  },
  {
    "id": 174,
    "title": "Improving Bandwidth Efficiency of Video-on-Demand Servers",
    "short_description": "",
    "full_content": "Video-on-demand (VOD) servers have a limited amount of bandwidth with which to service client requests. Conventional VOD servers dedicate a unique stream of data for each client, and that strategy can quickly allocate all of the available bandwidth on the server. We describe a system called stream tapping that allows clients to \"tap\" into existing streams on the VOD server. By using existing streams as much as possible, clients can reduce the amount of new bandwidth they require, and that allows more clients to use the server at once, reducing client latency. Stream tapping uses less than 20 by a conventional VOD server for popular videos, and it performs better than many other strategies designed to improve VOD servers. : Video-on-demand -- Efficiency -- Bandwidth",
    "author": [
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1999",
    "url": "https://escholarship.org/content/qt9xj6z13f/qt9xj6z13f_noSplash_e1050e26548b547def3fb1dd310a1307.pdf",
    "bibTeX": {
      "@article": "carter-cn99",
      "author": "Steven W. Carter and Darrell D. E. Long",
      "title": "Improving Bandwidth Efficiency of Video-on-Demand Servers",
      "journal": "Computer Networks",
      "volume": 31,
      "number": "1--2",
      "year": 1999,
      "month": "jan",
      "pages": "99--111"
    }
  },
  {
    "id": 175,
    "title": "REINAS: A Real-time System for Managing Environmental Data",
    "short_description": "",
    "full_content": "Managing scientific data is a challenging task, and many of the problems it presents have yet to be adequately solved. The Real-time Environmental Information Network and Analysis System (REINAS) is an attempt to develop an operational solution to the problem of collecting and distributing environmental data in a real-time context, as well as supporting data acquisition, verification, retrieval, visualization and long-term data maintenance. The system is built around one or more databases and has been developed to support both real-time and retrospective regional scale environmental science. Continuous real-time data is acquired from dispersed sensors and input to a logically integrated but physically distributed system. In such a system, the database can provide a powerful structure to handle data management, but current database technology has difficulty meeting the performance requirements that a large real-time environmental system demands. This discussion will describe the REINAS architecture in some detail, including challenges that were addressed in the construction of an operational system, with real users requiring real-time and retrospective access to a variety of structured environmental data, while supporting continuous real-time collection of data into the database.",
    "author": [
      "Eric C. Rosen",
      "Theodore R. Haining",
      "Darrell D. E. Long",
      "Patrick E. Mantey"
    ],
    "date": "Jan 1, 1998",
    "url": "https://escholarship.org/uc/item/34x467zf",
    "bibTeX": {
      "@article": "rosen-jseke98",
      "author": "Eric C. Rosen and Theodore R. Haining and Darrell D. E. Long and Patrick E. Mantey",
      "title": "REINAS: A Real-time System for Managing Environmental Data",
      "journal": "Journal of Software Engineering and Knowledge Engineering",
      "volume": 8,
      "number": 1,
      "year": 1998,
      "pages": "35--53"
    }
  },
  {
    "id": 176,
    "title": "Swift/RAID: A Distributed RAID System",
    "short_description": "",
    "full_content": "The Swift I/O architecture is designed to provide high data rates in support of multimedia type applications in general purpose distributed environments through the use of distributed striping. Striping techniques place sections of a single logical data space onto multiple physical devices. The original Swift prototype was designed to validate the architecture, but did not provide fault tolerance. We have implemented a new prototype of the Swift architecture that provides fault tolerance in the distributed environment in the same manner as RAID levels 4 and 5. RAID (Redundant Arrays of Inexpensive Disks) techniques have recently been widely used to increase both performance and fault tolerance of disk storage systems. The new Swift/RAID implementation manages all communication using a distributed transfer plan executor/ which isolates all communication code from the rest of Swift. The transfer plan executor is implemented as a distributed finite state machine which decodes and executes a set of reliable data transfer operations. This approach enabled us to easily investigate alternative architectures and communications protocols. Providing fault tolerance comes at a cost, since computing and administering parity data impacts Swift/RAID data rates. For a five node system, in one typical performance benchmark, Swift/RAID level 5 obtained 87 write throughput. Swift/RAID level 4 obtained 92 and 34",
    "author": [
      "Darrell D. E. Long",
      "Bruce R. Montague",
      "Luis-Felipe Cabrera"
    ],
    "date": "Jan 1, 1994",
    "url": "https://escholarship.org/uc/item/5dp278wc",
    "bibTeX": {
      "@article": "long-cs94",
      "author": "Darrell D. E. Long and Bruce R. Montague and Luis-Felipe Cabrera",
      "title": "Swift/RAID: A Distributed RAID System",
      "journal": "Computing Systems",
      "volume": 7,
      "number": 3,
      "year": 1994,
      "pages": "333--359"
    }
  },
  {
    "id": 177,
    "title": "Swift: Using Distributed Disk Striping to Provide High I/O Data Rates",
    "short_description": "",
    "full_content": "We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly.",
    "author": [
      "Luis-Felipe Cabrera",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1991",
    "url": "https://www.usenix.org/legacy/publications/compsystems/1991/fall_cabrera.pdf",
    "bibTeX": {
      "@article": "cabrera-cs91",
      "author": "Luis-Felipe Cabrera and Darrell D. E. Long",
      "title": "Swift: Using Distributed Disk Striping to Provide High I/O Data Rates",
      "journal": "Computing Systems",
      "volume": 4,
      "number": 4,
      "year": 1991,
      "pages": "405--436"
    }
  },
  {
    "id": 178,
    "title": "Accessing Replicated Data in a Large-Scale Distributed System",
    "short_description": "",
    "full_content": "Replicating a data object improves the availability of the data, and can improve access latency by locating copies of the object near to their use. When accessing replicated objects across an internetwork, the time to access different replicas is non-uniform. Further, the probability that a particular replica is inaccessible is much higher in an internetwork than in a local-area network (LAN) because of partitions and the many intermediate hosts and networks that can fail. We report three replica-accessing algorithms which can be tuned to minimize either access latency or the number of messages sent. These algorithms assume only an unreliable datagram mechanism for communicating with replicas. Our work extends previous investigations into the performance of replication algorithms by assuming unreliable communication. We have investigated the performance of these algorithms by measuring the communication behavior of the Internet, and by building discrete-event simulations based on our measurements. We find that almost all message failures are either transient or due to long-term host failure, so that retrying messages a few times adds only a small amount to the overall message traffic while improving both access latency as long as the probability of message failure is small. Moreover, the algorithms which retry messages on failure provide significantly improved availability over those which do not.",
    "author": [
      "Richard A. Golding",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1991",
    "url": "https://crss.us/media/pubs/c92408e66e20a15d78c3888c31c4b90e761320f1.pdf",
    "bibTeX": {
      "@article": "golding-ijcs91",
      "author": "Richard A. Golding and Darrell D. E. Long",
      "title": "Accessing Replicated Data in a Large-Scale Distributed System",
      "journal": "International Journal in Computer Simulation",
      "volume": 1,
      "number": 4,
      "year": 1991,
      "pages": "347--372"
    }
  },
  {
    "id": 179,
    "title": "The Performance of Available Copy Protocols for the Management of Replicated Data",
    "short_description": "",
    "full_content": "Available copy protocols guarantee the consistency of replicated data objects against any combination of non-Byzantine failures that do not result in partial communication failures. While the original available copy protocol assumed instantaneous detection of failures and instantaneous propagation of this information, more realistic protocols that do not rely on these assumptions have been devised. Two such protocols are investigated in this paper: a naive available copy (NAC) protocol that does not maintain any state information, and an optimistic available copy (OAC) protocol that only maintains state information at write and recovery times. Markov models are used to compare the performance of these two protocols with that of the original available copy protocol. These protocols are shown to perform nearly as well as the original available copy protocol, which is shown to perform much better than quorum consensus protocols.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 1990",
    "url": "https://ssrc.us/media/pubs/91119e35f3253892d6ea5a9edfc12be0e54261c1.pdf",
    "bibTeX": {
      "@article": "paris-pe90",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "The Performance of Available Copy Protocols for the Management of Replicated Data",
      "journal": "Performance Evaluation",
      "volume": 11,
      "number": 1,
      "year": 1990,
      "month": "apr",
      "pages": "9--30"
    }
  },
  {
    "id": 180,
    "title": "Disk Failure Prediction in Heterogeneous Environments",
    "short_description": "",
    "full_content": "Recent studies have shown the benefits of using SMART attributes to predict disk failures in homogeneous populations of disks from the same make and model. We address here the case of data centers with more heterogeneous disk populations, such as the ones described in the BackBlaze datasets, and propose to build global disk failure predictors that would apply to disks of all makes and models. Our first challenge was the large number of SMART parameters that were missing for most makes and models in many disk instances of our dataset. As a result, we had to discard the SMART attributes that were missing in at least 90 percent of the disks, which left us with 21 SMART attributes. We then applied a Reverse Arrangement Test to these attributes to select the strongest disk failure indicators. We investigated three different machine learning models (Decision Trees, Neural Networks, and Logistic Regression) using the 2015 BackBlaze data to train and validate our predictors. Our best model was a decision tree that identified true failure events among the disks that tested positive for at least one of our failure indicators. We then used the 2016 BackBlaze data to evaluate its performance. Our results show that our decision tree identifies at least 52 percent of all disk failures and makes nearly all its predictions several days ahead: no more than 2.45 percent of the predicted failures occur within one day or two of the prediction. Finally, we compared the performance of our predictor with those of the RAIDShield and the original BackBlaze predictor. We found out that RAIDShield could predict at most 18 percent of disk failures, that is, 34 percent fewer failures than our decision tree while the BackBlaze predictor predicted 60 percent of disk failures but generated 4 to 5 false alarms per correct prediction.",
    "author": [
      "Carlos A. Rincon",
      "Jehan-François Pâris",
      "Ricardo Vilalta",
      "Albert M. K. Cheng",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2017",
    "url": "https://ieeexplore.ieee.org/document/8046776",
    "bibTeX": {
      "@inproceedings": "rincon-ipccc17",
      "author": "Carlos A. Rincon and Jehan-Fran\\c{c}ois P\\^aris and Ricardo Vilalta and Albert M. K. Cheng and Darrell D. E. Long",
      "title": "Disk Failure Prediction in Heterogeneous Environments",
      "booktitle": "Proceedings of the International Symposium on Performance Evaluation of Computer and Telecommunication Systems",
      "year": 2017,
      "month": "jul",
      "publisher": "IEEE"
    }
  },
  {
    "id": 181,
    "title": "Pirogue: A lighter dynamic version of the Raft distributed consensus algorithm",
    "short_description": "",
    "full_content": "",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Dec 1, 2015",
    "url": "https://ieeexplore.ieee.org/document/7410281",
    "bibTeX": {
      "@inproceedings": "pris-ipccc15",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Pirogue: A lighter dynamic version of the Raft distributed consensus algorithm",
      "booktitle": "Proceedings of the Thirty-fourth International Performance of Computers and Communication Conference",
      "year": 2015,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 182,
    "title": "Triple failure tolerant storage systems using only exclusive-or parity calculations",
    "short_description": "",
    "full_content": "We present a disk array organization that can survive three simultaneous disk failures while only using exclusive-or operations to calculate the parities that generate this failure tolerance. The reliability of storage systems using magnetic disks depends on how prone individual disks are to failure. Unfortunately, disk failure rates are impossible to predict and it is well known that individual batches might be subject to much higher failure rates at some point during their lifetime. It is also known that many disk drive families, but not all, suffer a substantially higher failure rate at the beginning and some at the end of their economic lifespan. Our proposed organization can be built on top of a dense two-failure tolerant layout using only exclusive-or operations and with a ratio of parity to data disks of 2/k. If the disk failure rates are higher than expected, the new organization can be super-imposed on the existing two-failure tolerant organization by introducing (k+1)/2 new parity disks and (k+1)/2 new reliability stripes to yield a three-failure tolerant layout without moving any data or calculating any other parity but the new one. We derive the organization using a graph visualization and a construction by Lawless of factoring a complete graph into paths.",
    "author": [
      "Thomas Schwarz",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Nov 1, 2015",
    "url": "https://ieeexplore.ieee.org/document/7371868",
    "bibTeX": {
      "@inproceedings": "schwarz-prdc15",
      "author": "Thomas Schwarz and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Triple failure tolerant storage systems using only exclusive-or parity calculations",
      "booktitle": "Proceedings of the Twenty-first IEEE Pacific Rim International Symposium on Dependable Computing",
      "year": 2015,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 183,
    "title": "Reducing the Energy Footprint of a Distributed Consensus Algorithm",
    "short_description": "",
    "full_content": "The Raft consensus algorithm is a new distributed consensus algorithm that is both easier to understand and more straightforward to implement than the older Paxos algorithm. Its major limitation is its high energy footprint. As it relies on majority consensus voting for deciding when to commit an update, Raft requires five participants to protect against two simultaneous failures. We propose two methods for reducing this huge energy footprint. Our first proposal consists of adjusting Raft quorums in a way that would allow updates to proceed with as few as two servers while requiring a larger quorum for electing a new leader. Our second proposal consists of replacing one or two of the five Raft servers with witnesses, that is, lightweight servers that maintain the same metadata as other servers but hold no data and can therefore run on very low-power hosts. We show that these substitutions have little impact on the cluster availability but very different impacts on the risks of incurring a data loss.",
    "author": [
      "Jehan-François Pâris",
      "Darrell Long"
    ],
    "date": "Sep 1, 2015",
    "url": "https://ieeexplore.ieee.org/document/7371967",
    "bibTeX": {
      "@inproceedings": "pris-edcc15",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell Long",
      "title": "Reducing the Energy Footprint of a Distributed Consensus Algorithm",
      "booktitle": "Proceedings of the European Dependable Computing Conference",
      "year": 2015,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 184,
    "title": "Cooperative mode: Comparative storage metadata verification applied to the Xbox 360",
    "short_description": "",
    "full_content": "This work addresses the question of determining the correctness of forensic file system analysis software. Current storage systems are built on theory that is robust but not invincible to faults, from software, hardware, or adversaries. Given a parsing of a storage system of unknown provenance, the lack of a sound and complete analytic theory means the parsing's correctness cannot be proven. However, with recent advances in digital forensic theory, a measure of its incorrectness can be taken. We present FSNView, an N-Version programming utility. FSNView reports exhaustively the metadata of a single disk image, using multiple storage system parsers. Each parser reports its perspective of the metadata in Digital Forensics XML, a storage language used recently in a study on differential analysis. We repurpose the tools used for studying the changes in file systems from time to the changes in file systems from perspective. The differences in metadata summaries immediately note bugs in at least one of the tools employed. Diversity in tools and their analysis algorithms strengthens the analysis of a storage subject. We apply file system differencing to study the external storage of the Microsoft Xbox 360 game console. The console's storage serves as an exemplar analysis subject; the described strategy is general to storage system analysis. The custom volume management and new-though-familiar file system are features typical to an embedded system analysis. Two open-source utilities developed solely for analyzing this game console, and a third developed for general file system forensics, are extended to compare storage system metadata perspectives. We present a new file system and revisions to the DFXML language, library, and differencing process, which were necessary to enable a principled, automatic evaluation of storage analysis tools.",
    "author": [
      "Alex Nelson",
      "Erik Steggall",
      "Darrell Long"
    ],
    "date": "Aug 1, 2014",
    "url": "https://www.sciencedirect.com/science/article/pii/S1742287614000474",
    "bibTeX": {
      "@inproceedings": "nelson-dfrws14",
      "author": "Alex Nelson and Erik Steggall and Darrell Long",
      "title": "Cooperative mode: Comparative storage metadata verification applied to the Xbox 360",
      "booktitle": "Proceedings of the Digital Forensics Research Conference",
      "year": 2014,
      "month": "aug",
      "publisher": "ACM"
    }
  },
  {
    "id": 185,
    "title": "DBaaS-Expert: A Recommender for the Selection of the Right Cloud Database",
    "short_description": "",
    "full_content": "",
    "author": [
      "Soror Sahri",
      "Rim Moussa",
      "Salima Benbernou",
      "Darrell Long"
    ],
    "date": "Jun 1, 2014",
    "url": "https://scholar.google.com/scholar?q=DBaaS-Expert%3A%20A%20Recommender%20for%20the%20Selection%20of%20the%20Right%20Cloud%20Database",
    "bibTeX": {
      "@inproceedings": "sahri-procee14",
      "author": "Soror Sahri and Rim Moussa and Salima Benbernou and Darrell Long",
      "title": "DBaaS-Expert: A Recommender for the Selection of the Right Cloud Database",
      "booktitle": "Proceedings of the Twenty-first International Symposium on Methodologies for Intelligent Systems",
      "year": 2014,
      "month": "jun",
      "publisher": "Springer"
    }
  },
  {
    "id": 186,
    "title": "Reliability Challenges for Storing Exabytes",
    "short_description": "",
    "full_content": "As we move towards data centers at the exascale, the reliability challenges of such enormous storage systems are daunting. We demonstrate how such systems will suffer substantial annual data loss if only traditional reliability mechanisms are employed. We argue that the architecture for exascale storage systems should incorporate novel mechanisms at or below the object level to address this problem. Our argument for such a research focus is that focusing solely on the device level will not scale, and in this study we analytically evaluate how rapidly this problem manifests.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long",
      "Thomas Schwarz"
    ],
    "date": "Feb 1, 2014",
    "url": "https://ieeexplore.ieee.org/document/6785458",
    "bibTeX": {
      "@inproceedings": "amer-procee14",
      "author": "Ahmed Amer and Darrell D. E. Long and Thomas Schwarz",
      "title": "Reliability Challenges for Storing Exabytes",
      "booktitle": "Proceedings of the International Conference on Computing, Networking and Communications",
      "year": 2014,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 187,
    "title": "Zero-Maintenance Disk Arrays",
    "short_description": "",
    "full_content": "We present a disk array architecture that does not require users to perform any maintenance tasks over the expected lifetime of the array. Preliminary results indicate that the key factor in the feasibility of our design is the failure rate of unused spare disks. As long as these rates remain negligible, zero maintenance disk arrays with at least 77 disks can provide a five-year reliability of five nines (99.999 percent) with a space overhead comparable to that of mirroring. If this is not the case, we would need between 64 and 70 percent extra spare disks to achieve the same five-year reliability, which would result in a higher space overhead.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Thomas Schwarz, S.J."
    ],
    "date": "Dec 1, 2013",
    "url": "https://ieeexplore.ieee.org/document/6820858",
    "bibTeX": {
      "@inproceedings": "pris-prdc13",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Thomas Schwarz, S.J.",
      "title": "Zero-Maintenance Disk Arrays",
      "booktitle": "Proceedings of the IEEE Nineteenth Pacific Rim International Symposium on Dependable Computing",
      "year": 2013,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 188,
    "title": "Reliability of Disk Arrays with Double Parity",
    "short_description": "",
    "full_content": "Abstract—We present a general method for estimating the risk of data loss in arbitrary two-dimensional RAID arrays where each data disk belongs to exactly two single-parity stripes. We start by representing each array organization by a graph where each parity stripe, and its associated parity disk, is represented by a node and each data disk by an edge. We then use this representation to identify and enumerate minimal sets of disk failures, say, triple failures, quadruple failures and so forth, that will cause a data loss. The overall probabilities that a given number n of disk failures will cause a data loss is then given by the ratio of the total number of fatal disk failures involving n disks over the total number of possible failures of n disks. To illustrate the power of our method, we apply it to two distinct, archival two-dimensional array organizations. The first, ”square ” organization is a traditional square layout where data disks are formed into a square and the parity stripes are formed by the rows and columns in the square. Hence a square layout organization with n2 data disks will have 2n parity disks. The second, ”complete ” organization corresponds to a closer weave, where all parity stripes intersect and each intersection contains a parity disk. This organization with n parity disks will have n(n− 1)/2 data disks. Our results show that previous ad hoc estimates of the reliability of these arrays significantly underestimated their reliability by assuming that either all triple or all quadruple disk failures were fatal. We show that the two two-dimensional array organizations exhibit mean times to data loss and five-year survival rates that are very similar to those of a RAID Level 6 organization of much smaller capacity. Our complete organization is about 4.5 times and the square organization is about 8 times more reliable than a disk array with same storage capacity built from RAID level 6 stripes. Index Terms—Disk array organization, archival storage sys-tem, Markov model, mean time to data loss, five year survival rate I.",
    "author": [
      "Thomas Schwarz",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Dec 1, 2013",
    "url": "https://ieeexplore.ieee.org/document/6820846",
    "bibTeX": {
      "@inproceedings": "schwarz-prdc13",
      "author": "Thomas Schwarz and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Reliability of Disk Arrays with Double Parity",
      "booktitle": "Proceedings of the IEEE Nineteenth Pacific Rim International Symposium on Dependable Computing",
      "year": 2013,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 189,
    "title": "Three-Dimensional Redundancy Codes for Archival Storage",
    "short_description": "",
    "full_content": "Fault-tolerant disk arrays rely on replication or erasure-coding to reconstruct lost data after a disk failure. As disk capacity increases, so does the risk of encountering irrecoverable read errors that would prevent the full recovery of the lost data. We propose a three-dimensional erasure-coding technique that reduces that risk by guaranteeing full recovery in the presence of all triple and nearly all quadruple disk failures. Our solution performs better than existing solutions, such as sets of disk arrays using Reed-Solomon codes against triple failures in each individual array. Given its very high reliability, it is especially suited to the needs of very large data sets that must be preserved over long periods of time.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Witold Litwin"
    ],
    "date": "Aug 1, 2013",
    "url": "https://ieeexplore.ieee.org/document/6730780",
    "bibTeX": {
      "@inproceedings": "pris-sim13",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Witold Litwin",
      "title": "Three-Dimensional Redundancy Codes for Archival Storage",
      "booktitle": "Proceedings of the Twenty-first International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems",
      "year": 2013,
      "month": "aug",
      "publisher": "IEEE"
    }
  },
  {
    "id": 190,
    "title": "Accelerated Chaining: A Better Way to Harness Peer Power in Video-on-Demand Applications",
    "short_description": "",
    "full_content": "We present a more efficient chaining protocol for video-on-demand applications. Chaining protocols require each client to forward the video data it receives to the next client watching the same video. Unlike all extant chaining protocols, our protocol requires these clients to forward these data at a rate slightly higher than the video consumption rate. Our simulations indicate that increasing the client video forwarding rate by 5 percent is sufficient to virtually eliminate the server workload for a two-hour video when request arrival rates remain above 30 requests per hour. Categories and Subject Descriptions: H.5.1 [Information Systems]: INFORMATION INTERFACES AND PRESENTATION -- Multimedia Information Systems. General Terms: Algorithms; performance; design.",
    "author": [
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Darrell Long"
    ],
    "date": "Mar 1, 2011",
    "url": "https://dl.acm.org/doi/10.1145/1982185.1982237",
    "bibTeX": {
      "@inproceedings": "pris-procee11",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Darrell Long",
      "title": "Accelerated Chaining: A Better Way to Harness Peer Power in Video-on-Demand Applications",
      "booktitle": "Proceedings of the Twenty-sixth ACM Symposium on Applied Computing",
      "year": 2011,
      "month": "mar",
      "publisher": "ACM"
    }
  },
  {
    "id": 191,
    "title": "Duplicate Data Elimination in a SAN File System",
    "short_description": "",
    "full_content": "Duplicate Data Elimination (DDE) is our method for identifying and coalescing identical data blocks in Storage Tank, a SAN file system. On-line file systems pose a unique set of performance and implementation challenges for this feature. Existing techniques, which are used to improve both storage and network utilization, do not satisfy these constraints. Our design employs a combination of content hashing, copy-on-write, and lazy updates to achieve its functional and performance goals. DDE executes primarily as a background process. The design also builds on Storage Tank's FlashCopy function to ease implementation. Storage Tank technology is available today in the IBM Total Storage SAN File System (SANFS). However, this paper and research is based on underlying Storage Tank technology and may not become part of the IBM TotalStorage SAN File System product. We include an analysis of selected real-world data sets that is aimed at demonstrating the space-saving potential of coalescing duplicate data. Our results show that DDE can reduce storage consumption by up to 80",
    "author": [
      "Bo Hong",
      "Demyn Plantenberg",
      "Darrell D. E. Long",
      "Miriam Sivan-Zimet"
    ],
    "date": "Apr 1, 2004",
    "url": "https://ssrc.us/media/pubs/eb0bb6eeade3ea84a23bf30d1b8a1862c458fe8b.pdf",
    "bibTeX": {
      "@inproceedings": "hong-msst04",
      "author": "Bo Hong and Demyn Plantenberg and Darrell D. E. Long and Miriam Sivan-Zimet",
      "title": "Duplicate Data Elimination in a SAN File System",
      "booktitle": "Proceedings of the Twenty-first Symposium on Mass Storage Systems",
      "year": 2004,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 193,
    "title": "Tabbycat: An Inexpensive Scalable Server for Video-on-Demand",
    "short_description": "",
    "full_content": "Tabbycat is a video server prototype demonstrating the benefits of a proactive approach for distributing popular videos on demand to a large customer base. Rather than reacting to individual customer requests, Tabbycat broadcasts the contents of the most popular videos according to a fixed schedule. As a result, the number of customers watching a given video does not affect the cost of distributing it. We found that one workstation with a single ATA disk drive and a Fast Ethernet interface could distribute three two-hour videos while achieving a maximum customer waiting time of less than four minutes.",
    "author": [
      "Karthik Thirumalai",
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "May 1, 2003",
    "url": "https://ieeexplore.ieee.org/document/1204467",
    "bibTeX": {
      "@inproceedings": "thirumalai-procee03",
      "author": "Karthik Thirumalai and Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Tabbycat: An Inexpensive Scalable Server for Video-on-Demand",
      "booktitle": "Proceedings of the IEEE 2003 International Conference on Communications",
      "year": 2003,
      "month": "may",
      "publisher": "IEEE"
    }
  },
  {
    "id": 194,
    "title": "A Proactive Implementation of Interactive Video-on-Demand",
    "short_description": "",
    "full_content": "Most broadcasting protocols for video-on-demand do not allow the customer to pause, move fast-forward or backward while watching a video. We propose a broadcasting protocol implementing these features in a purely proactive fashion. Our protocol implements rewind and pause interactions at the set-top box level by requiring the set-top box to keep in its buffer all video data it has received from the server until the customer has finished watching the video. It implements fast-forward by letting the video server transmit video data more frequently than needed by customers watching the video in sequence. As a result, any customer having watched the first x minutes of a video will be able to fast-forward to any scene within the first 2x or 3x minutes of the video. We show that this expanding horizon feature can be provided at a reasonable cost. We also show how our protocol can accommodate customers connected to the service through a device lacking either the ability to receive data at more than two times the video consumption rate or the storage space required to store more than 20 to 25 percent of the video they are watching. While these customers will not have access to any of the interactive features provided by our protocol, they will be able to watch videos after the same wait time as all other customers.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2003",
    "url": "https://scholar.google.com/scholar?q=A%20Proactive%20Implementation%20of%20Interactive%20Video-on-Demand",
    "bibTeX": {
      "@inproceedings": "pris-ipccc03",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "A Proactive Implementation of Interactive Video-on-Demand",
      "booktitle": "Proceedings of the International Performance, Computing, and Communications Conference",
      "year": 2003,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 195,
    "title": "Increasing Predictive Accuracy by Prefetching Multiple Program and User Specific Files",
    "short_description": "",
    "full_content": "Prefetching multiple files per prediction can improve the predictive accuracy. However, it comes with the cost of using extra cache space and disk bandwidth. This paper discusses the most Recent distinct n Successor (RnS) model and uses it to demonstrate the effectiveness of our earlier work, Program-based Last n Successor (PLnS) model, a program-based prediction algorithm . We analyze the simulation results from different trace data and show that PLnS can perform better than RnS while it only predicts at most 59 RnS when the n in PLnS equals to two. PLnS is a good candidate when considering prefetching multiple files per prediction to improve predictive accuracy.",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "Jun 1, 2002",
    "url": "https://scholar.google.com/scholar?q=Increasing%20Predictive%20Accuracy%20by%20Prefetching%20Multiple%20Program%20and%20User%20Specific%20Files",
    "bibTeX": {
      "@inproceedings": "yeh-procee02",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Increasing Predictive Accuracy by Prefetching Multiple Program and User Specific Files",
      "booktitle": "Proceedings of the Sixteenth Annual International Symposium on High Performance Computing Systems and Applications",
      "year": 2002,
      "month": "jun",
      "publisher": "IEEE"
    }
  },
  {
    "id": 196,
    "title": "Visualizing I/O Predictability",
    "short_description": "",
    "full_content": "We propose a novel method to study storage system predictability based on the visualization of file successor entropy, a form of conditional entropy drawn from a file access trace. First-order conditional entropy can be used as a measure of predictability. It is superior to the more common measures such as independent likelihood of data access. For file access data, we developed a visualization tool that produces 3D graphical views of the variation in predictability of successive access events on a per-file basis. Our visualization tool provides interactive observation of the variations in predictability according to some arbitrary criterion, e.g. time of day, program identifier, user groups, or any other classification of files. Four entropy data sets were extracted from various file system traces. These four data sets are representative of the variability in file access patterns for different machine use: server personal workstation, large number of interactive users, and heavy write activity. Visualization results show that there is strong predictability among files and optimizations would be profitable.",
    "author": [
      "Alison Luo",
      "Ahmed Amer",
      "Scott Speidel",
      "Darrell D. E. Long",
      "Alex Pang"
    ],
    "date": "Jun 1, 2002",
    "url": "https://scholar.google.com/scholar?q=Visualizing%20I/O%20Predictability",
    "bibTeX": {
      "@inproceedings": "luo-procee02",
      "author": "Alison Luo and Ahmed Amer and Scott Speidel and Darrell D. E. Long and Alex Pang",
      "title": "Visualizing I/O Predictability",
      "booktitle": "Proceedings of the First International Symposium on 3D Data Processing, Visualization, and Transmission",
      "year": 2002,
      "month": "jun",
      "publisher": "IEEE"
    }
  },
  {
    "id": 197,
    "title": "A Dynamic Heuristic Broadcasting Protocol for Video-on-Demand",
    "short_description": "",
    "full_content": "Most existing distribution protocols for video-on-demand are tailored for a specific range of video access rates and perform poorly beyond that range. We present a dynamic heuristic broadcasting protocol that performs as well as stream tapping with unlimited extra tapping at low video access rates and has the same average bandwidth requirements as the best existing broadcasting protocols at high video access rates. We also show how our protocol can handle compressed video and adapt itself to the individual bandwidth requirements of each video.",
    "author": [
      "Scott R. Carter",
      "Jehan-François Pâris",
      "Saurabh Mohan",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2001",
    "url": "https://scholar.google.com/scholar?q=A%20Dynamic%20Heuristic%20Broadcasting%20Protocol%20for%20Video-on-Demand",
    "bibTeX": {
      "@inproceedings": "carter-procee01",
      "author": "Scott R. Carter and Jehan-Fran\\c{c}ois P\\^aris and Saurabh Mohan and Darrell D. E. Long",
      "title": "A Dynamic Heuristic Broadcasting Protocol for Video-on-Demand",
      "booktitle": "Proceedings of the Twenty-first International Conference on Distributed Computing Systems",
      "year": 2001,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 198,
    "title": "An Analytical Study of Opportunistic Lease Renewal",
    "short_description": "",
    "full_content": "We present opportunistic renewal, a lease management protocol designed to keep distributed file systems or distributed shared memories consistent in the presence of a network partition or other computer failures. Our treatment includes an analytical model of the protocol that compares performance with existing lease protocols and quantifies improvements. In addition, this analytical model provides the structure to understand message overhead and availability trade-offs when selecting lease parameters. We include results demonstrating that opportunistic renewal substantially reduces the network overhead associated with lease renewal. As a corollary, opportunistic renewal can reduce the lease length at any given network overhead; , by a factor of 50 at 1 makes leasing less intrusive and shorter lease periods allow a system to recover from failure more quickly.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2001",
    "url": "https://scholar.google.com/scholar?q=An%20Analytical%20Study%20of%20Opportunistic%20Lease%20Renewal",
    "bibTeX": {
      "@inproceedings": "burns-procee01",
      "author": "Randal C. Burns and Robert M. Rees and Darrell D. E. Long",
      "title": "An Analytical Study of Opportunistic Lease Renewal",
      "booktitle": "Proceedings of the Twenty-first International Conference on Distributed Computing Systems",
      "year": 2001,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 199,
    "title": "Efficient Implementation of Interactive Video-on-Demand",
    "short_description": "",
    "full_content": "The key performance bottleneck for a video-on-demand (VOD) server is bandwidth, which controls the number of clients the server can simultaneously support. Previous work has shown that a strategy called stream tapping can make efficient use of bandwidth when clients are not allowed to interact (through VCR-like controls) with the video they are viewing. Here we present an interactive version of stream tapping and analyze its performance through the use of discrete event simulation. In particular, we show that stream tapping can use as little as 10 stream of data to each client request.",
    "author": [
      "Steven W. Carter",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Aug 1, 2000",
    "url": "https://scholar.google.com/scholar?q=Efficient%20Implementation%20of%20Interactive%20Video-on-Demand",
    "bibTeX": {
      "@inproceedings": "carter-sim00",
      "author": "Steven W. Carter and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Efficient Implementation of Interactive Video-on-Demand",
      "booktitle": "Proceedings of the Eighth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems",
      "year": 2000,
      "month": "aug",
      "publisher": "IEEE"
    }
  },
  {
    "id": 200,
    "title": "Consistency and Locking for Distributing Updates to Web Servers Using a File System",
    "short_description": "",
    "full_content": "Distributed file systems are often used to replicate a website's content among its many servers. However, for content that needs to be dynamically updated and distributed to many servers, file system locking protocols exhibit high latency and heavy network usage. Poor performance arises because the Web-serving workload differs from the assumed workload. To address the shortcomings of file systems, we introduce the publish consistency model well suited to the Web-serving workload and implement it in the producer-consumer locking protocol. A comparison of this protocol against other file system protocols by simulation shows that producer-consumer locking removes almost all latency due to protocol overhead and significantly reduces network load.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Darrell D. E. Long"
    ],
    "date": "Jun 1, 2000",
    "url": "https://scholar.google.com/scholar?q=Consistency%20and%20Locking%20for%20Distributing%20Updates%20to%20Web%20Servers%20Using%20a%20File%20System",
    "bibTeX": {
      "@inproceedings": "burns-procee00",
      "author": "Randal C. Burns and Robert M. Rees and Darrell D. E. Long",
      "title": "Consistency and Locking for Distributing Updates to Web Servers Using a File System",
      "booktitle": "Proceedings of Performance and Architecture of Web Servers",
      "year": 2000,
      "month": "jun",
      "publisher": "ACM"
    }
  },
  {
    "id": 201,
    "title": "Safe Caching in a Distributed File System for Network Attached Storage",
    "short_description": "",
    "full_content": "In a distributed file system built on network attached storage, client computers access data directly from shared storage, rather than submitting I/O requests through a server. Without a server marshaling access to data, if a computer fails or becomes isolated in a network partition while holding locks on cached data objects, those objects become inaccessible to other computers until a locking authority can guarantee that the lock holder will not again directly access these data. We describe a server that acts as the locking authority and implements a lease-based protocol for revoking access to data objects locked by an isolated or failed computer. When a lease expires, the server can be assured that the client no longer acts on locked data, and can safely redistribute locks to other clients. During normal operation, this protocol invokes no message overhead, and uses no memory and performs no computation at the locking authority.",
    "author": [
      "Randal C. Burns",
      "Robert M. Rees",
      "Darrell D. E. Long"
    ],
    "date": "May 1, 2000",
    "url": "https://scholar.google.com/scholar?q=Safe%20Caching%20in%20a%20Distributed%20File%20System%20for%20Network%20Attached%20Storage",
    "bibTeX": {
      "@inproceedings": "burns-procee00",
      "author": "Randal C. Burns and Robert M. Rees and Darrell D. E. Long",
      "title": "Safe Caching in a Distributed File System for Network Attached Storage",
      "booktitle": "Proceedings of the International Parallel and Distributed Processing Symposium",
      "year": 2000,
      "month": "may",
      "publisher": "IEEE"
    }
  },
  {
    "id": 202,
    "title": "A Stack Model Based Replacement Policy for a Non-Volatile Write Cache",
    "short_description": "",
    "full_content": "The use of non-volatile write caches is an effective technique to bridge the performance gap between I/O systems and processor speed. Using such caches provides two benefits: some writes will be avoided because dirty blocks will be overwritten in the cache, and physically contiguous dirty blocks can be grouped into a single I/O operation. We present a new block replacement policy that efficiently expels only blocks which are not likely to be accessed again and coalesces writes to disk. In a series of trace-based simulation experiments, we show that a modestly sized cache managed with our replacement policy can reduce the number of writes to disk by 75 percent and often did better. We also show that our new policy is more effective than block replacement policies that take advantage of either spatial locality or temporal locality, but not both.",
    "author": [
      "Jehan-François Pâris",
      "Theodore R. Haining",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 2000",
    "url": "https://scholar.google.com/scholar?q=A%20Stack%20Model%20Based%20Replacement%20Policy%20for%20a%20Non-Volatile%20Write%20Cache",
    "bibTeX": {
      "@inproceedings": "paris-msst00",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Theodore R. Haining and Darrell D. E. Long",
      "title": "A Stack Model Based Replacement Policy for a Non-Volatile Write Cache",
      "booktitle": "Proceedings of the Eighth NASA Goddard Conference on Mass Storage Systems and Technologies",
      "year": 2000,
      "month": "mar",
      "publisher": "IEEE"
    }
  },
  {
    "id": 203,
    "title": "A Reactive Broadcasting Protocol for Video on Demand",
    "short_description": "",
    "full_content": "Broadcasting can reduce the cost of video on demand services by distributing in a more efficient fashion videos that are simultaneously watched by many viewers. Rather than waiting for individual requests, broadcasting schedules ahead of time repeated broadcasts of the various segments of a video. Hence, it performs best when it is used to distribute videos that are concurrently watched by hundreds or thousands of viewers. We propose in this paper a reactive broadcasting protocol that addresses the problem of distributing moderately popular videos in an efficient fashion. Like all efficient broadcasting protocols, reactive broadcasting assumes that the customer set-top box has enough local storage to store at least one half of each video being watched. Unlike other broadcasting protocols, reactive broadcasting only broadcasts the later portions of each video being distributed. The initial segment of each video is distributed on demand using a stream tapping protocol. Our simulations show that reactive broadcasting outperforms both purely reactive and purely proactive approaches for a large range of video request rates.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 2000",
    "url": "https://scholar.google.com/scholar?q=A%20Reactive%20Broadcasting%20Protocol%20for%20Video%20on%20Demand",
    "bibTeX": {
      "@inproceedings": "pris-icme00",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "A Reactive Broadcasting Protocol for Video on Demand",
      "booktitle": "Proceedings of the Multimedia Computing and Networking Conference",
      "year": 2000,
      "month": "jan",
      "publisher": "ACM"
    }
  },
  {
    "id": 204,
    "title": "Combining Pay-per-View and Video-on-Demand Services",
    "short_description": "",
    "full_content": "Most efforts aimed at reducing the costs of video-on-demand services have focussed on reducing the cost of distributing the top ten to twenty videos by broadcasting them in a periodic fashion rather than waiting for individual requests. Unfortunately nearly all existing VOD broadcasting protocols require client set-top boxes (STB) to include enough local storage to store up to 55 percent of each video being viewed. Here we present a novel VOD broadcasting protocol that does not make that demand. Our Dual Broadcasting protocol can accommodate clients who do not have any storage device in their STB while providing a much lower maximum waiting time to customers whose STB includes a disk drive. We also discuss two possible extensions to this new protocol. One of them is aimed at reducing the bandwidth requirements of the protocol while the other extends the functionality of the VOD service by providing reverse and fast forward controls. : video-on-demand, pay per view, broadcasting protocols, harmonic broadcasting, pagoda broadcasting.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 1999",
    "url": "https://scholar.google.com/scholar?q=Combining%20Pay-per-View%20and%20Video-on-Demand%20Services",
    "bibTeX": {
      "@inproceedings": "pris-sim99",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "Combining Pay-per-View and Video-on-Demand Services",
      "booktitle": "Proceedings of the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems",
      "year": 1999,
      "month": "oct",
      "publisher": "IEEE"
    }
  },
  {
    "id": 205,
    "title": "Management Policies for Non-volatile Write Caches",
    "short_description": "",
    "full_content": "Many computer hardware and software architectures buffer data in memory to improve system performance. Volatile disk or file caches are sometimes used to delay the propagation of writes to disk (called delayed writes). While delayed writes improve system performance, volatile caches can cause the loss of vital data during sudden failure. In this study, we investigate managing non-volatile RAM (NVRAM) caches with different simple strategies to delay writes to disk. We evaluate the performance of NVRAM caches using three measures of merit: the number of stalled writes which wait while the cache is cleaned before being serviced, the mean service time for I/O requests, and the number of writes generated by cleaning the cache. Our results show that even small non-volatile write caches using simple management policies can reduce the number of writes to disk by at least 70 Our results also show that the number of stalled writes is high: 30 best and nearly 100 Adding pro-active purging effectively decreases both stalled writes and disk write activity.",
    "author": [
      "Theodore R. Haining",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 1999",
    "url": "https://scholar.google.com/scholar?q=Management%20Policies%20for%20Non-volatile%20Write%20Caches",
    "bibTeX": {
      "@inproceedings": "haining-ipccc99",
      "author": "Theodore R. Haining and Darrell D. E. Long",
      "title": "Management Policies for Non-volatile Write Caches",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1999,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 206,
    "title": "A Hybrid Broadcasting Protocol for Video on Demand",
    "short_description": "",
    "full_content": "Broadcasting protocols can improve the efficiency of video on demand services by reducing the bandwidth required to transmit videos that are simultaneously watched by many viewers. It has been recently shown that broadcasting protocols using a very large number of very low bandwidth streams for each video required less total bandwidth than protocols using a few high-bandwidth streams shared by all videos. We present a hybrid broadcasting protocol that combines the advantages of these two classes of protocols. Our pagoda broadcasting protocol uses only a small number of high-bandwidth streams and requires only slightly more bandwidth than the best extant protocols to achieve a given maximum waiting time.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Jan 1, 1999",
    "url": "https://scholar.google.com/scholar?q=A%20Hybrid%20Broadcasting%20Protocol%20for%20Video%20on%20Demand",
    "bibTeX": {
      "@inproceedings": "pris-icme99",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "A Hybrid Broadcasting Protocol for Video on Demand",
      "booktitle": "Proceedings of the Multimedia Computing and Networking Conference",
      "year": 1999,
      "month": "jan",
      "publisher": "ACM"
    }
  },
  {
    "id": 207,
    "title": "Long-term File Activity and Inter-reference Patterns",
    "short_description": "",
    "full_content": "We compare and contrast long-term file system activity for different Unix environments for periods of 120 to 280 days. Our focus is on finding common long-term activity trends and reference patterns. Our analysis shows that 90",
    "author": [
      "Timothy Gibson",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Dec 1, 1998",
    "url": "https://scholar.google.com/scholar?q=Long-term%20File%20Activity%20and%20Inter-reference%20Patterns",
    "bibTeX": {
      "@inproceedings": "gibson-procee98",
      "author": "Timothy Gibson and Ethan L. Miller and Darrell D. E. Long",
      "title": "Long-term File Activity and Inter-reference Patterns",
      "booktitle": "Proceedings of the Computer Measurement Group Conference",
      "year": 1998,
      "month": "dec",
      "publisher": "CMG"
    }
  },
  {
    "id": 208,
    "title": "A Low Bandwidth Broadcasting Protocol for Video on Demand",
    "short_description": "",
    "full_content": "Broadcasting protocols can improve the efficiency of video on demand services by reducing the bandwidth required to transmit videos that are simultaneously watched by many viewers. We present a polyharmonic broadcasting protocol that requires less bandwidth than the best extant protocols to achieve the same low maximum waiting time. We also show how to modify the protocol to accommodate very long videos without increasing the buffering capacity of the set-top box.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Oct 1, 1998",
    "url": "https://scholar.google.com/scholar?q=A%20Low%20Bandwidth%20Broadcasting%20Protocol%20for%20Video%20on%20Demand",
    "bibTeX": {
      "@inproceedings": "pris-procee98",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "A Low Bandwidth Broadcasting Protocol for Video on Demand",
      "booktitle": "Proceedings of the International Conference on Computer Communication and Networks",
      "year": 1998,
      "month": "oct",
      "publisher": "IEEE"
    }
  },
  {
    "id": 209,
    "title": "Efficient Broadcasting Protocols for Video on Demand",
    "short_description": "",
    "full_content": "Broadcasting protocols can improve the efficiency of video on demand services by reducing the bandwidth required to transmit videos that are simultaneously watched by many viewers. We present here two broadcasting protocols that achieve nearly the same low bandwidth as the best extant broadcasting protocol while guaranteeing a lower maximum access time. Our first protocol, cautious harmonic broadcasting, requires somewhat more bandwidth than our second protocol, quasi-harmonic broadcasting, but is also much simpler to implement. : video on demand, video broadcasting.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 1998",
    "url": "https://scholar.google.com/scholar?q=Efficient%20Broadcasting%20Protocols%20for%20Video%20on%20Demand",
    "bibTeX": {
      "@inproceedings": "pris-sim98",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "Efficient Broadcasting Protocols for Video on Demand",
      "booktitle": "Proceedings of the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems",
      "year": 1998,
      "month": "jul",
      "publisher": "IEEE"
    }
  },
  {
    "id": 210,
    "title": "Efficient Distributed Back-up with Delta Compression",
    "short_description": "",
    "full_content": "Inexpensive storage and more powerful processors have resulted in a proliferation of data that needs to be reliably backed up. Network resource limitations make it increasingly difficult to backup a distributed file system on a nightly or even weekly basis. By using delta compression algorithms, which minimally encode a version of a file using only the bytes that have changed, a backup system can compress the data sent to a server. With the delta backup technique, we can achieve significant savings in network transmission time over previous techniques. Our measurements indicate that file system data may, on average, be compressed to within 10 approximately 45 have also been backed up in the previous week. Based on our measurements, we conclude that a small file store on the client that contains copies of previously backed up files can be used to retain versions in order to generate delta files. To reduce the load on the backup server, we implement a modified version storage architecture, version jumping, that allows us to restore delta encoded file versions with at most two accesses to tertiary storage. This minimizes server workload and network transmission time on file restore.",
    "author": [
      "Randal C. Burns",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 1997",
    "url": "https://dl.acm.org/doi/10.1145/266220.266223",
    "bibTeX": {
      "@inproceedings": "burns-procee97",
      "author": "Randal C. Burns and Darrell D. E. Long",
      "title": "Efficient Distributed Back-up with Delta Compression",
      "booktitle": "Proceedings of I/O in Parallel and Distributed Systems",
      "year": 1997,
      "month": "nov",
      "publisher": "ACM"
    }
  },
  {
    "id": 211,
    "title": "Improving Video-on-Demand Server Efficiency Through Stream Tapping",
    "short_description": "",
    "full_content": "Efficiency is essential for Video-on-Demand (VOD) to be successful. Conventional VOD servers are inefficient; they dedicate a disk stream for each client, quickly using up all available streams. However, several systems have been proposed that allow clients to share streams. We present a new system called stream tapping that allows a client to greedily 'tap' data from any stream on the VOD server containing video data the client can use. This is accomplished through the use of a small buffer on the client set-top box and requires less than 20",
    "author": [
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 1997",
    "url": "https://ieeexplore.ieee.org/document/623313",
    "bibTeX": {
      "@inproceedings": "carter-procee97",
      "author": "Steven W. Carter and Darrell D. E. Long",
      "title": "Improving Video-on-Demand Server Efficiency Through Stream Tapping",
      "booktitle": "Proceedings of the Sixth International Conference on Computer Communications and Networks",
      "year": 1997,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 212,
    "title": "Voting Without Version Numbers",
    "short_description": "",
    "full_content": "Voting protocols are widely used to provide mutual exclusion in distributed systems and to guarantee the consistency of replicated data in the presence of network partitions. Unfortunately, the most efficient voting protocols require fairly complex metadata to assert which replicas are up to date and to denote the replicas that belong to that set. We present a much simpler technique that does not require version numbers and maintains only n + (n) bits of state per replica. We show, under standard Markovian assumptions, that a static voting protocol using our method provides nearly the same data availability as a static voting protocol using version numbers. Moreover a dynamic voting protocol using our method provides the same data availability as a dynamic voting protocol using much more complex metadata.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Feb 1, 1997",
    "url": "https://scholar.google.com/scholar?q=Voting%20Without%20Version%20Numbers",
    "bibTeX": {
      "@inproceedings": "long-ipccc97",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Voting Without Version Numbers",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1997,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 213,
    "title": "A Linear Time, Constant Space Differencing Algorithm",
    "short_description": "",
    "full_content": "An efficient differencing algorithm can be used to compress version of files for both transmission over low bandwidth channels and compact storage. This can greatly reduce network traffic and execution time for distributed applications which include software distribution, source code control, file system replication, and data backup and restore. An algorithm for such applications needs to be both general and efficient; able to compress binary inputs in linear time. We present such an algorithm for differencing files at the granularity of a byte. The algorithm uses constant memory and handles arbitrarily large input files. While the algorithm makes minor sacrifices in compression to attain linear runtime performance, it outperforms the byte-wise differencing algorithms that we have encountered in the literature on all inputs.",
    "author": [
      "Randal C. Burns",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 1997",
    "url": "https://ieeexplore.ieee.org/document/582199",
    "bibTeX": {
      "@inproceedings": "burns-ipccc97",
      "author": "Randal C. Burns and Darrell D. E. Long",
      "title": "A Linear Time, Constant Space Differencing Algorithm",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1997,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 214,
    "title": "A Dynamic Disk Spin-down Technique for Mobile Computing",
    "short_description": "",
    "full_content": "We address the problem of deciding when to spin down the disk of a mobile computer in order to extend battery life. Since one of the most critical resources in mobile computing environments is battery life, good energy conservation methods can dramatically increase the utility of mobile systems. We use a simple and efficient algorithm based on machine learning techniques that has excellent performance in practice. Our experimental results are based on traces collected from HP C2474s disks. Using this data, the algorithm outperforms several algorithms that are theoretically optimal in under various worst-case assumptions, as well as the best fixed time-out strategy. In particular, the algorithm reduces the power consumption of the disk to about half (depending on the disk's properties) of the energy consumed by a one minute fixed time-out. Since the algorithm adapts to usage patterns, it uses as little as 88 computed in retrospect.",
    "author": [
      "David P. Helmbold",
      "Darrell D. E. Long",
      "Bruce Sherrod"
    ],
    "date": "Nov 1, 1996",
    "url": "https://dl.acm.org/doi/10.1145/236387.236423",
    "bibTeX": {
      "@inproceedings": "helmbold-procee96",
      "author": "David P. Helmbold and Darrell D. E. Long and Bruce Sherrod",
      "title": "A Dynamic Disk Spin-down Technique for Mobile Computing",
      "booktitle": "Proceedings of the Second Annual International Conference on Mobile Computing and Networking",
      "year": 1996,
      "month": "nov",
      "publisher": "ACM"
    }
  },
  {
    "id": 215,
    "title": "A Leaner, More Efficient, Available Copy Protocol",
    "short_description": "",
    "full_content": "Available copy protocols provide the highest data availability and data reliability of all replication protocols that do not regenerate failed replicas. Unfortunately, all existing implementations of available copy protocols either rely on complex procedures for ascertaining which replicas are up to date after a total failure or have to wait for the recovery of all failed sites. We present a simple technique for efficiently implementing the available copy protocol. Our protocol does not require version numbers and maintains only n + (n) bits of state per replica. We also show under standard Markovian assumptions that our new protocol provides the same data availability as the best feasible implementations of the available copy protocol.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Oct 1, 1996",
    "url": "https://scholar.google.com/scholar?q=A%20Leaner%2C%20More%20Efficient%2C%20Available%20Copy%20Protocol",
    "bibTeX": {
      "@inproceedings": "long-procee96",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "A Leaner, More Efficient, Available Copy Protocol",
      "booktitle": "Proceedings of the Symposium on Parallel and Distributed Processing",
      "year": 1996,
      "month": "oct",
      "publisher": "IEEE"
    }
  },
  {
    "id": 216,
    "title": "A Longitudinal Study of Internet Host Reliability",
    "short_description": "",
    "full_content": "",
    "author": [
      "Darrell Long",
      "Andrew Muir",
      "Richard Golding"
    ],
    "date": "Sep 1, 1995",
    "url": "https://ieeexplore.ieee.org/document/403267",
    "bibTeX": {
      "@inproceedings": "long-procee95",
      "author": "Darrell Long and Andrew Muir and Richard Golding",
      "title": "A Longitudinal Study of Internet Host Reliability",
      "booktitle": "Proceedings of the Fourteenth Symposium on Reliable Distributed Systems",
      "year": 1995,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 217,
    "title": "Adding Adaptive Flow Control to Swift/RAID",
    "short_description": "",
    "full_content": "We discuss an adaptive flow control mechanism for the Swift/RAID distributed file system. Our goal is to achieve near-optimal performance on heterogeneous networks where available load capacity varies due to other network traffic. The original Swift/RAID prototype used synchronous communication, achieving throughput considerably less than available network capacity. We designed and implemented an adaptive flow control mechanism that provides greatly improved performance. Our design uses a simple automatic repeat request (ARQ) go back N protocol coupled with the congestion avoidance and control mechanism developed for the Transmission Control Protocol (TCP). The Swift/RAID implementation contains a transfer plan executor to isolate all of the communications code from the rest of Swift. The adaptive flow control design was implemented entirely in this module. Results from experimental data show the adaptive design achieving an increase in throughput for reads from 671 KB/s for the original synchronous implementation to 927 KB/s (a 38 and an increase from 375 KB/s to 559 KB/s (a 49",
    "author": [
      "Chane L. Fullmer",
      "Darrell D. E. Long",
      "Luis-Felipe Cabrera"
    ],
    "date": "Mar 1, 1995",
    "url": "https://scholar.google.com/scholar?q=Adding%20Adaptive%20Flow%20Control%20to%20Swift/RAID",
    "bibTeX": {
      "@inproceedings": "fullmer-ipccc95",
      "author": "Chane L. Fullmer and Darrell D. E. Long and Luis-Felipe Cabrera",
      "title": "Adding Adaptive Flow Control to Swift/RAID",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1995,
      "month": "mar",
      "publisher": "IEEE"
    }
  },
  {
    "id": 218,
    "title": "An Architecture Supporting Real-Time and Retrospective Environmental Data Management",
    "short_description": "",
    "full_content": "The Real-Time Environmental Information Network and Analysis System (REINAS) is a distributed database environment supporting both real-time and retrospective regional scale environmental science. Continuous real-time data is acquired from dispersed sensors and input to a logically integrated but physically distributed database. An integrated problem-solving environment supports visualization and modeling by users requiring insight into historical, current, and predicted oceanographic and meteorological conditions. REINAS supports both collaborative and single-user scientific work in a distributed environment. The goals and design of key data management aspects of REINAS are described.",
    "author": [
      "Darrell D. E. Long",
      "Patrick E. Mantey",
      "Theodore R. Haining",
      "Bruce R. Montague",
      "Eric C. Rosen"
    ],
    "date": "Jun 1, 1994",
    "url": "https://scholar.google.com/scholar?q=An%20Architecture%20Supporting%20Real-Time%20and%20Retrospective%20Environmental%20Data%20Management",
    "bibTeX": {
      "@inproceedings": "long-adb94",
      "author": "Darrell D. E. Long and Patrick E. Mantey and Theodore R. Haining and Bruce R. Montague and Eric C. Rosen",
      "title": "An Architecture Supporting Real-Time and Retrospective Environmental Data Management",
      "booktitle": "Proceedings of the First International Conference for Applications of Databases (ADB)",
      "year": 1994,
      "month": "jun",
      "publisher": "Springer"
    }
  },
  {
    "id": 219,
    "title": "Performance Guarantees on ATM Networks",
    "short_description": "",
    "full_content": "Recent developments in ATM technology has made multiplexing of a wide range of traffic with diverse performance requirements important. The statistical multiplexing of several traffic types such as voice, video and data can lead to network congestion thus violating the quality of service (QOS) guarantees. In order to meet the performance requirements of the applications an efficient transport protocol and congestion control mechanism becomes necessary. We propose a transport mechanism for guaranteeing application required QOS requirements over an ATM network. Leaky-bucket congestion control scheme is used as the traffic policing mechanism. Source characterization of voice and video are modeled as Markov Modulated Poisson Processes (MMPP) and the proposed transport mechanism is evaluated by simulation on a single and multihop network with multimedia traffic. The performance of the scheme is examined as a function of source characteristics and the effect of statistical multiplexing is also investigated. The results indicate that the leaky-bucket scheme and statistical multiplexing are more effective for bursty traffic. This calls for a dynamic control mechanism to achieve optimal system utilization with guaranteed QOS.",
    "author": [
      "Cheng Tang",
      "Shree Murthy",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 1994",
    "url": "https://scholar.google.com/scholar?q=Performance%20Guarantees%20on%20ATM%20Networks",
    "bibTeX": {
      "@inproceedings": "tang-ipccc94",
      "author": "Cheng Tang and Shree Murthy and Darrell D. E. Long",
      "title": "Performance Guarantees on ATM Networks",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1994,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 220,
    "title": "The refdbms Distributed Bibliographic Database System",
    "short_description": "",
    "full_content": "Refdbms is a database system for sharing bibliographic references among many users at sites on a wide-area network such as the Internet. This paper describes our experiences in building and using refdbms for the last two years. It summarizes the collection of facilities that refdbms provides, and gives detailed information on how well refdbms functions as a collaborative, wide-area, distributed information system.",
    "author": [
      "Richard A. Golding",
      "Darrell D. E. Long",
      "John Wilkes"
    ],
    "date": "Jan 1, 1994",
    "url": "https://scholar.google.com/scholar?q=The%20refdbms%20Distributed%20Bibliographic%20Database%20System",
    "bibTeX": {
      "@inproceedings": "golding-usenix94",
      "author": "Richard A. Golding and Darrell D. E. Long and John Wilkes",
      "title": "The refdbms Distributed Bibliographic Database System",
      "booktitle": "Proceedings of the 1994 Usenix Winter Technical Conference",
      "year": 1994,
      "month": "jan",
      "publisher": "Usenix Association"
    }
  },
  {
    "id": 221,
    "title": "Providing Performance Guarantees in an FDDI Network",
    "short_description": "",
    "full_content": "A network subsystem supporting a continuous media file system must guarantee a minimum throughput, a maximum delay, and a maximum jitter. We present a transport protocol that provides these guarantees. To support different types of service, our protocol is built from modules selected to meet the requirements of each communication session. A buffering technique is used to provide jitter guarantees. To provide throughput and delay guarantees, network performance is optimized based on the required transfer rate. The effects of controlling transmission rate and packet size are presented. The resulting transport protocol is modeled on a simulated FDDI network and the results are analyzed. We show that the protocol provides the required guarantees for the anticipated types of traffic.",
    "author": [
      "Darrell D. E. Long",
      "Carol Osterbrock",
      "Luis-Felipe Cabrera"
    ],
    "date": "May 1, 1993",
    "url": "https://scholar.google.com/scholar?q=Providing%20Performance%20Guarantees%20in%20an%20FDDI%20Network",
    "bibTeX": {
      "@inproceedings": "long-procee93",
      "author": "Darrell D. E. Long and Carol Osterbrock and Luis-Felipe Cabrera",
      "title": "Providing Performance Guarantees in an FDDI Network",
      "booktitle": "Proceedings of the Thirteenth International Conference on Distributed Computing Systems",
      "year": 1993,
      "month": "may",
      "publisher": "IEEE"
    }
  },
  {
    "id": 222,
    "title": "Scheduling Real-Time Disk Transfers for Continuous Media Applications",
    "short_description": "",
    "full_content": "We study how continuous media data can be stored and accessed in the Swift distributed I/O architecture. We provide a scheme for scheduling real-time data transfers that satisfies the strict requirements of continuous media applications. Our scheme allows large data objects to be stored and retrieved concurrently from multiple disks so as to satisfy the high data rate requirements which are typical of real-time video and audio data. To do this, data transfer requests are split into smaller requests which are then handled by the various components of Swift. We study on-line algorithms that respond to a data request by promising to either satisfy or reject it. Each response must be made before the next request is seen by the algorithm. We discuss two different performance measures to evaluate such algorithms and show that no on-line algorithm can optimize these criteria to less than a constant fraction of the optimal. Finally, we propose an algorithm for handling such requests on-line and the related data structures.",
    "author": [
      "Darrell D. E. Long",
      "Madhukar N. Thakur"
    ],
    "date": "Apr 1, 1993",
    "url": "https://ieeexplore.ieee.org/document/289755",
    "bibTeX": {
      "@inproceedings": "long-msst93",
      "author": "Darrell D. E. Long and Madhukar N. Thakur",
      "title": "Scheduling Real-Time Disk Transfers for Continuous Media Applications",
      "booktitle": "Proceedings of the Twelfth Symposium on Mass Storage Systems",
      "year": 1993,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 223,
    "title": "A Simulation Study of Replication Control Protocols Using Volatile Witnesses",
    "short_description": "",
    "full_content": "Voting protocols guarantee the consistency of replicated data objects by disallowing all access requests that cannot gather a sufficient quorum of replicas. The performance of voting protocols can be greatly enhanced by adding to the replicas small independent entities that hold no data but can attest to the state of the replicated data object. It has been recently proposed to store these witnesses in volatile storage. Volatile witnesses repond faster to write requests than those stored in stable storage. They can also be more easily regenerated as many local area networks contain a majority of diskless sites. We present a simulation study of the availability afforded by two voting protocols using volatile witnesses and investigate the impact of access rates, network topology and witness placement on the availability of the replicated data.",
    "author": [
      "Perry K. Sloope",
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 1992",
    "url": "https://scholar.google.com/scholar?q=A%20Simulation%20Study%20of%20Replication%20Control%20Protocols%20Using%20Volatile%20Witnesses",
    "bibTeX": {
      "@inproceedings": "sloope-sim92",
      "author": "Perry K. Sloope and Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "A Simulation Study of Replication Control Protocols Using Volatile Witnesses",
      "booktitle": "Proceedings of the Twenty-fifth Annual Simulation Symposium",
      "year": 1992,
      "month": "apr",
      "publisher": "Society for Computer Simulation"
    }
  },
  {
    "id": 224,
    "title": "Protecting Replicated Objects Against Media Failures",
    "short_description": "",
    "full_content": "We present a replication control protocol that provides excellent data availabilities while guaranteeing that all writes to the object are recorded in at least two replicas. The protocol, robust dynamic voting (RDV) accepts reads and writes as long as at least two replicas remain available. The replicated object remains inaccessible until either the two last available replicas recover or one of the two last available replicas can collect the votes of a majority of replicas. We evaluate the read and write availabilities of replicated data objects managed by the RDV protocol and compare them with those of replicated objects managed by majority consensus voting, dynamic voting and hybrid dynamic voting protocols. We show that RDV can provide extra protection against media failures with no significant loss of availability.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 1992",
    "url": "https://scholar.google.com/scholar?q=Protecting%20Replicated%20Objects%20Against%20Media%20Failures",
    "bibTeX": {
      "@inproceedings": "pris-procee92",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Protecting Replicated Objects Against Media Failures",
      "booktitle": "Proceedings of the Workshop on Research Issues on Transaction and Query Processing",
      "year": 1992,
      "month": "feb",
      "publisher": "ACM"
    }
  },
  {
    "id": 225,
    "title": "Quorum-oriented Multicast Protocols for Data Replication",
    "short_description": "",
    "full_content": "Many wide-area distributed applications use replicated data to improve the availability of the data, and to improve access latency by locating copies of the data near to their use. This paper presents a new family of communication protocols, called quorum multicasts, that provide efficient communication services for widely replicated data. Quorum multicasts are similar to ordinary multicasts, which deliver a message to a set of destinations. The new protocols extend this model by allowing delivery to a subset of the destinations, selected according to distance or expected data currency. These protocols provide well-defined failure semantics, and can distinguish between communication failure and replica failure with high probability. We have evaluated their performance, taking measurements communication latency and failure in the Internet. A simulation study of quorum multicasts shows that they provide low latency and require few messages. A second study that measured a test application running at several sites confirmed these results.",
    "author": [
      "Richard A. Golding",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 1992",
    "url": "https://scholar.google.com/scholar?q=Quorum-oriented%20Multicast%20Protocols%20for%20Data%20Replication",
    "bibTeX": {
      "@inproceedings": "golding-procee92",
      "author": "Richard A. Golding and Darrell D. E. Long",
      "title": "Quorum-oriented Multicast Protocols for Data Replication",
      "booktitle": "Proceedings of the Eighth International Conference on Data Engineering",
      "year": 1992,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 226,
    "title": "A Study of the Reliability of Internet Sites",
    "short_description": "",
    "full_content": "Modeling the reliability of distributed systems requires a good understanding of the reliability of the components. Careful modeling allows highly fault-tolerant distributed data applications to be constructed at the least cost. Failure and repair rates of components are often assumed to be exponentially distributed. This hypothesis is testable for failure rates, though the process of gathering and reducing the data to a usable form can be difficult. By applying an appropriate test statistic, some samples were found to have a realistic chance of being drawn from an exponential distribution, while others can be confidently classed as non-exponential. Data were collected from a large number of hosts via the Internet. Almost all of the visible Internet (over 350,000 hosts) were considered, and more than 68,000 of these that were judged likely to respond were queried. These hosts were sampled several times to obtain up-times, and finally to determine average host availability. Estimates of availability, mean-time-to-failure (MTTF) and mean-time-to-repair (MTTR) were derived. The results reported here correspond with those commonly seen in practice.",
    "author": [
      "Darrell D. E. Long",
      "John L. Carroll",
      "C. J. Park"
    ],
    "date": "Sep 1, 1991",
    "url": "https://scholar.google.com/scholar?q=A%20Study%20of%20the%20Reliability%20of%20Internet%20Sites",
    "bibTeX": {
      "@inproceedings": "long-procee91",
      "author": "Darrell D. E. Long and John L. Carroll and C. J. Park",
      "title": "A Study of the Reliability of Internet Sites",
      "booktitle": "Proceedings of the Tenth Symposium on Reliable Distributed Systems",
      "year": 1991,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 227,
    "title": "Exploiting Multiple I/O Streams to Provide High Data-Rates",
    "short_description": "",
    "full_content": "We present an I/O architecture, called Swift, that addresses the problem of data-rate mismatches between the requirements of an application, the maximum data-rate of the storage devices, and the data-rate of the interconnection medium. The goal of Swift is to support integrated continuous multimedia in general purpose distributed systems. In installations with a high-speed interconnection medium, Swift will provide high data-rate transfers by using multiple slower storage devices in parallel. The data-rates obtained with this approach scale well when using multiple storage devices and multiple interconnections. Swift has the flexibility to use any appropriate storage technology, including disk arrays. The ability to adapt to technological advances will allow Swift to provide for ever increasing I/O demands. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. Using a single Ethernet-based local-area network and three servers, the prototype provides data-rates that are almost three times as fast as access to the local SCSI disk in the case of writes. When compared with NFS, the Swift prototype provides double the data-rate for reads and eight times the data-rate for writes. The data-rate of our prototype scales almost linearly in the number of servers and the number of network segments. Its performance is shown to be limited by the speed of the Ethernet-based local-area network. We also constructed a simulation model to show how the Swift architecture can exploit storage, communication, and processor advances, and to locate the components that will limit I/O performance. In a simulated gigabit/second token ring local-area network the data-rates are seen to scale proportionally to the size of the transfer unit and to the number of storage agents.",
    "author": [
      "Darrell D. E. Long",
      "Luis-Felipe Cabrera"
    ],
    "date": "Jun 1, 1991",
    "url": "https://scholar.google.com/scholar?q=Exploiting%20Multiple%20I/O%20Streams%20to%20Provide%20High%20Data-Rates",
    "bibTeX": {
      "@inproceedings": "long-usenix91",
      "author": "Darrell D. E. Long and Luis-Felipe Cabrera",
      "title": "Exploiting Multiple I/O Streams to Provide High Data-Rates",
      "booktitle": "1991 Summer Usenix Technical Conference",
      "year": 1991,
      "month": "jun",
      "publisher": "Usenix Association"
    }
  },
  {
    "id": 228,
    "title": "The Reliability of Regeneration-Based Replica Control Protocols",
    "short_description": "",
    "full_content": "The accessibility of vital information can be enhanced by replicating the data on several sites and employing a consistency control protocol to manage the copies. For many applications, the reliability of a system is a more important measure of its performance than its availability. These applications are characterized by the property that interruptions of service are intolerable and often involve interaction with real-time processes. The reliability of a replicated data object depends on maintaining a viable set of current replicas. When storage is limited, it may not be feasible to simply replicate a data object at enough sites to achieve the desired level of reliability. If new replicas of a data object can be created faster than a system failure can be repaired, better reliability can be achieved by creating new replicas on additional sites in response to changes in the system configuration. Several strategies for replica maintenance are considered, and the benefits of each are analyzed. Formulas describing the reliability of the replicated data object are presented, and closed-form solutions are given for the tractible cases. Numerical solutions, validated by simulation results, are used to analyze the trade-offs between reliability and storage cost. With estimates of the mean times to site failure and repair in a given system, the numerical techniques presented here can be applied to predict the fewest number of replicas required to provide the desired level of reliability.",
    "author": [
      "Darrell D. E. Long",
      "John L. Carroll",
      "Kris Stewart"
    ],
    "date": "Jun 1, 1989",
    "url": "https://scholar.google.com/scholar?q=The%20Reliability%20of%20Regeneration-Based%20Replica%20Control%20Protocols",
    "bibTeX": {
      "@inproceedings": "long-procee89",
      "author": "Darrell D. E. Long and John L. Carroll and Kris Stewart",
      "title": "The Reliability of Regeneration-Based Replica Control Protocols",
      "booktitle": "Proceedings of the International Conference on Distributed Computing Systems",
      "year": 1989,
      "month": "jun",
      "publisher": "IEEE"
    }
  },
  {
    "id": 229,
    "title": "The Effect of Failure and Repair Distributions on Consistency Protocols for Replicated Data Objects",
    "short_description": "",
    "full_content": "The accessibility of vital information can be enhanced by replicating the data on several sites, and employing a consistency control protocol to manage the copies. Various protocols have been proposed to ensure that only current copies of the data can be accessed. The effect these protocols have on the accessibility of the replicated data is investigated by simulating the operation of the network and measuring the performance. Several strategies for replica maintenance are considered, and the benefits of each are analyzed. The details of the simulations are discussed. Measurements of the reliability and the availability of the replicated data are compared and contrasted. The sensitivity of the Available Copy and Dynamic-linear Voting protocols to common patterns of site failures and repairs is studied in detail. Exponential, Erlang, uniform, and hyperexponential distributions are considered, and the effect the second moments have on the results is analyzed. The relative performance of competing protocols is shown to be only marginally affected by non-exponential distributions, validating the robustness of the exponential approximations.",
    "author": [
      "John L. Carroll",
      "Darrell D. E. Long"
    ],
    "date": "Mar 1, 1989",
    "url": "https://scholar.google.com/scholar?q=The%20Effect%20of%20Failure%20and%20Repair%20Distributions%20on%20Consistency%20Protocols%20for%20Replicated%20Data%20Objects",
    "bibTeX": {
      "@inproceedings": "carroll-sim89",
      "author": "John L. Carroll and Darrell D. E. Long",
      "title": "The Effect of Failure and Repair Distributions on Consistency Protocols for Replicated Data Objects",
      "booktitle": "Proceedings of the Twenty-second Annual Simulation Symposium",
      "year": 1989,
      "month": "mar",
      "publisher": "Society for Computer Simulation"
    }
  },
  {
    "id": 230,
    "title": "Reliability of Replicated Data Objects",
    "short_description": "",
    "full_content": "Improved fault tolerance of many applications can be achieved by replicating data at several sites. This data redundancy requires a protocol to maintain the consistency of the data object in the prescence of site failures. The most commonly used scheme is voting. Voting and its variants are unaffected by network partitions. When network partitions cannot occur, better performance can be achieved with availability copy protocols. Common measures of dependability include reliability, which is the probability that a replicated object will remian constantly available over a fixed time period. We investigate the reliability of replicated data objects managed by voting, available copy and their variants. Where possible, closed-form expressions for the reliability of the various consistency protocols are derived using standard Markovian assumptions. In other cases, numerical solutions are found and validated with simulation results.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris",
      "John L. Carroll"
    ],
    "date": "Mar 1, 1989",
    "url": "https://scholar.google.com/scholar?q=Reliability%20of%20Replicated%20Data%20Objects",
    "bibTeX": {
      "@inproceedings": "long-ipccc89",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris and John L. Carroll",
      "title": "Reliability of Replicated Data Objects",
      "booktitle": "Proceedings of the International Performance Conference on Computers and Communication",
      "year": 1989,
      "month": "mar",
      "publisher": "IEEE"
    }
  },
  {
    "id": 231,
    "title": "Regeneration Protocols for Replicated Objects",
    "short_description": "",
    "full_content": "The reliability and availability of replicated data can often be increased by generating new replicas when some become inaccessible due to system malfunctions. This technique has been used in the Regeneration Algorithm, a replica control protocol based on file regeneration. The read and write availabilities of replicated data managed by the Regeneration Algorithm are evaluated and two new regeneration protocols are presented that overcome some of its limitations. The first protocol combines regeneration and the Available Copy approach to improve availability of replicated data. The second combines regeneration and the Dynamic Voting approach to guarantee data consistency in the presence of network partitions while maintaining a high availability. Expressions for the availabilities of replicated data managed by both protocols are derived and found to improve significantly on the availability achieved using extant consistency protocols.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Feb 1, 1989",
    "url": "https://scholar.google.com/scholar?q=Regeneration%20Protocols%20for%20Replicated%20Objects",
    "bibTeX": {
      "@inproceedings": "long-procee89",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Regeneration Protocols for Replicated Objects",
      "booktitle": "Proceedings of the International Conference on Data Engineering",
      "year": 1989,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 232,
    "title": "A Realistic Evaluation of Optimistic Dynamic Voting",
    "short_description": "",
    "full_content": "When data is replicated, an access protocol must be chosen to insure the presentation of a consistent view of the data. Protocols based on quorum consensus provide good availability with the added benefit of mutual exclusion. Of the protocols based on quorum consensus, the dynamic voting protocols provide the highest known availability. We describe a dynamic voting protocol that does not need the instantaneous state information required by the same performance as the original dynamic voting in the asymptotic case, and quickly converges to it for realistic access rates. Our protocol does this at a cost in network similar to that of statistic majority consensus voting. The first realistic analysis of the availability afforded by dynamic voting protocols is presented, taking the access frequency into account. The analysis confirms our hypothesis that delaying state information does not appreciably affect availability. Discrete event simulation is used to confirm and to extend the results we obtain using analytic models.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Oct 1, 1988",
    "url": "https://scholar.google.com/scholar?q=A%20Realistic%20Evaluation%20of%20Optimistic%20Dynamic%20Voting",
    "bibTeX": {
      "@inproceedings": "long-procee88",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "A Realistic Evaluation of Optimistic Dynamic Voting",
      "booktitle": "Proceedings of the Seventh Symposium on Reliable Distributed Systems",
      "year": 1988,
      "month": "oct",
      "publisher": "IEEE"
    }
  },
  {
    "id": 233,
    "title": "A Realistic Evaluation of Consistency Algorithms for Replicated Files",
    "short_description": "",
    "full_content": "Data are often replicated in distributed systems to protect them against site failures and network malfunctions. When this is the case, an access policy must be chosen to insure that a consistent view of the data is always presented. Voting protocols guarantee consistency of replicated data in the presence of any scenario involving non-Byzantine site failures and network partitions. While Static Majority Consensus Voting protocols use static quorums, Dynamic Voting protocols, like Dynamic Voting and Lexicographic Dynamic Voting, dynamically adjust quorums to changes in the status of the network of sites holding the copies. The availabilities of replicated data managed by these three protocols are compared using a simulation model with realistic parameters. Dynamic Voting is found to perform better than Majority Consensus Voting for all files having more than three copies while Lexicographic Dynamic Voting performs much better than the two other protocols for all eleven configurations under study.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Alexander Glockner"
    ],
    "date": "Mar 1, 1988",
    "url": "https://scholar.google.com/scholar?q=A%20Realistic%20Evaluation%20of%20Consistency%20Algorithms%20for%20Replicated%20Files",
    "bibTeX": {
      "@inproceedings": "pris-sim88",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Alexander Glockner",
      "title": "A Realistic Evaluation of Consistency Algorithms for Replicated Files",
      "booktitle": "Proceedings of the Twenty-first Annual Simulation Symposium",
      "year": 1988,
      "month": "mar",
      "publisher": "Society for Computer Simulation"
    }
  },
  {
    "id": 234,
    "title": "Efficient Dynamic Voting Algorithms",
    "short_description": "",
    "full_content": "Voting protocols guarantee consistency of replicated data in the presence of any scenario involving non- Byzantine site failures and network positions. While Static Majority Consensus Voting algorithms use static quorums, Dynamic Voting algorithms dynamically adjust quorums to changes in the status of the network of site holding the copies. We propose in this paper two novel dynamic voting algorithms. One, called Optimistic Dynamic Voting, operates on possibly out-of-date information, which greatly increases the efficiency of the algorithm and simplifies its implementation. The other, called Topological Dynamic Voting, explicitly takes into account the topology of the network on which the copies reside to increase the availability of the replicated data. We also compare availabilities of replicated data managed by both algorithms with those of data managed by existing voting protocols using a simulation model with realistic parameters. Optimistic Dynamic Voting is found to perform as well as the best existing voting algorithms while Topological Dynamic Voting performs much better than all other voting algorithms when two or more copies reside in the same non-partitionable group.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Feb 1, 1988",
    "url": "https://scholar.google.com/scholar?q=Efficient%20Dynamic%20Voting%20Algorithms",
    "bibTeX": {
      "@inproceedings": "pris-procee88",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Efficient Dynamic Voting Algorithms",
      "booktitle": "Proceedings of the International Conference on Data Engineering",
      "year": 1988,
      "month": "feb",
      "publisher": "IEEE"
    }
  },
  {
    "id": 235,
    "title": "Block-Level Consistency of Replicated Files",
    "short_description": "",
    "full_content": "We investigate the construction of a reliable device. Such a device appears to the file system as an ordinary block-structured device, but is implemented as a set of server processes on several sites. This allows for replication while leaving the operating system kernel and the file system unchanged. The regular structure of the block-level replication environment allows the use of consistency control algorithms that are simpler and less network intensive. We present three algorithms for maintaining file consistency in a block-level replication environment. The first is a majority consensus voting algorithm that recovers blocks only when required for data access; the second is a variant of the available copy scheme modified for replication at the block level; the third is a na\"ive version of the available copy scheme that does not maintain any failure information. Each scheme is evaluated in terms of availability and network traffic. While block-level replication is shown to allow improvements in the network traffic burden incurred by voting, available copy schemes are shown to have better availability and require significantly less traffic than voting schemes. The na\"ive available copy variant proposed here is shown to be the algorithm of choice.",
    "author": [
      "John L. Carroll",
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Sep 1, 1987",
    "url": "https://scholar.google.com/scholar?q=Block-Level%20Consistency%20of%20Replicated%20Files",
    "bibTeX": {
      "@inproceedings": "carroll-procee87",
      "author": "John L. Carroll and Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "Block-Level Consistency of Replicated Files",
      "booktitle": "Proceedings of the International Conference on Distributed Computing Systems",
      "year": 1987,
      "month": "sep",
      "publisher": "IEEE"
    }
  },
  {
    "id": 236,
    "title": "On Improving the Availability of Replicated Files",
    "short_description": "",
    "full_content": "To improve the availability and reliability of files the data are often replicated at several sites. A scheme must then be chosen to maintain the consistency of the file contents in the presence of site failures. The most commonly used scheme is voting. Voting is popular because it is simple and robust: voting schemes do not depend on any sophisticated message passing scheme and are unaffected by network partitions. When network partitions can not occur, better availabilities and reliabilities can be achieved with the available copy scheme. This scheme is somewhat more complex than voting as the recovery algorithm invoked after a failure of all sites has to know which site holding a copy failed last. We present in this paper a new method aimed at finding this site. It consists of recording those sites which received the most recent update; this information can then be used to determine which site holds the most recent version of the file upon site recovery. Unlike other methods, our approach does not require any monitoring of site failures and has a much lower overhead. We also derive, under standard Markovian assumptions, closed-form expressions for the availability of replicated files managed by voting, available copy and a simplified scheme that does not keep track of the last copy to fail.",
    "author": [
      "Darrell D. E. Long",
      "Jehan-François Pâris"
    ],
    "date": "Mar 1, 1987",
    "url": "https://scholar.google.com/scholar?q=On%20Improving%20the%20Availability%20of%20Replicated%20Files",
    "bibTeX": {
      "@inproceedings": "long-procee87",
      "author": "Darrell D. E. Long and Jehan-Fran\\c{c}ois P\\^aris",
      "title": "On Improving the Availability of Replicated Files",
      "booktitle": "Proceedings of the Symposium on Reliability in Distributed Software and Database Systems",
      "year": 1987,
      "month": "mar",
      "publisher": "IEEE"
    }
  },
  {
    "id": 237,
    "title": "Improving Disk Array Reliability Through Faster Repairs",
    "short_description": "",
    "full_content": "Disk scrubbing periodically scans the contents of a disk array to detect the presence of irrecoverable read errors and reconstitute the contents of the lost blocks using the built-in redundancy of the disk array. We address the issue of scheduling scrubbing runs in disk arrays that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole array whenever a disk failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of disk arrays over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twenty-four hour interval.",
    "author": [
      "Jehan-François Pâris",
      "Thomas Schwarz, S.J.",
      "Darrell D. E. Long"
    ],
    "date": "Dec 1, 2016",
    "url": "https://ieeexplore.ieee.org/document/7820607",
    "bibTeX": {
      "@inproceedings": "paris-ipccc16",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Thomas Schwarz, S.J. and Darrell D. E. Long",
      "title": "Improving Disk Array Reliability Through Faster Repairs",
      "booktitle": "Proceedings of the Thirty-fifth International Performance of Computers and Communication Conference",
      "year": 2016,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 238,
    "title": "Horus: Fine-Grained Encryption-Based Security for High Performance Petascale Storage",
    "short_description": "",
    "full_content": "With the growing use of large-scale distributed systems, the likelihood that at least one node is compromised is increasing. Large-scale systems that process sensitive data such as geographic data with defense implications, drug modeling, nuclear explosion modeling, and private genomic data would benefit greatly from strong security for their storage. Our approach, Horus, encrypts large datasets using keyed hash trees (KHTs) to generate different keys for each region of the dataset, providing fine-grained security: the key for one region cannot be used to access another region. Horus also reduces key management and distribution overhead while providing end-to-end data encryption and reducing the need to trust system operators or cloud service providers.",
    "author": [
      "Ranjana Rajendran",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2011",
    "url": "https://dl.acm.org/doi/10.1145/2159352.2159354",
    "bibTeX": {
      "@inproceedings": "rajendran-pdsw11",
      "author": "Ranjana Rajendran and Ethan L. Miller and Darrell D. E. Long",
      "title": "Horus: Fine-Grained Encryption-Based Security for High Performance Petascale Storage",
      "booktitle": "Proceedings of the Sixth Petascale Data Storage Workshop",
      "year": 2011,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 239,
    "title": "Using a Shared Storage Class Memory Device to Improve the Reliability of RAID Arrays",
    "short_description": "",
    "full_content": "Storage class memories (SCMs) constitute an emerging class of non-volatile storage devices that promise to be significantly faster and more reliable than magnetic disks. We propose to add one of these devices to each group of two or three RAID level arrays and store on it additional parity data. We show that the new organization can tolerate all double disk failures, between 75 and 90 percent of all triple disk failures and between 50 and 70 percent of all failures involving two disks and the SCM device without incurring any data loss. As a result, the additional parity device increases the mean time to data loss of the arrays in the group it protects by at least 200-fold.",
    "author": [
      "Sara Chaarawi",
      "Jehan-François Pâris",
      "Ahmed Amer",
      "Thomas Schwarz",
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 2010",
    "url": "https://scholar.google.com/scholar?q=Using%20a%20Shared%20Storage%20Class%20Memory%20Device%20to%20Improve%20the%20Reliability%20of%20RAID%20Arrays",
    "bibTeX": {
      "@inproceedings": "chaarawi-pdsw10",
      "author": "Sara Chaarawi and Jehan-Fran\\c{c}ois P\\^aris and Ahmed Amer and Thomas Schwarz and Darrell D. E. Long",
      "title": "Using a Shared Storage Class Memory Device to Improve the Reliability of RAID Arrays",
      "booktitle": "Proceedings of the Fifth Petascale Data Storage Workshop",
      "year": 2010,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 240,
    "title": "Fived: A Service-Based Architecture Implementation to Innovate at the Endpoints",
    "short_description": "",
    "full_content": "Security functions such as access control, encryption and authentication are typically left up to applications on the modern Internet. There is no unified system to implement these critical features. The access control that does exist on the network doesn't integrate well with user authentication systems, so access control decisions are based on the network location of a computer rather than the privilege level of its user. Just about every layer of the Internet provides optional encryption, yet most data on the Internet continues to be sent in the clear. Application developers routinely make mistakes in security critical code leading to bugs that manifest in worms, malware or provide a doorway for actively malicious attackers. We propose a unified session layer that integrates trustworthiness features into the core of the network. This would reverse the fortunes of security on the Internet and lead us toward a safer, more secure global network.",
    "author": [
      "D. J. Capelis",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 2010",
    "url": "https://scholar.google.com/scholar?q=Fived%3A%20A%20Service-Based%20Architecture%20Implementation%20to%20Innovate%20at%20the%20Endpoints",
    "bibTeX": {
      "@inproceedings": "capelis-sigcomm10",
      "author": "D. J. Capelis and Darrell D. E. Long",
      "title": "Fived: A Service-Based Architecture Implementation to Innovate at the Endpoints",
      "booktitle": "Proceedings of SIGCOMM 2010",
      "year": 2010,
      "month": "sep",
      "publisher": "ACM"
    }
  },
  {
    "id": 241,
    "title": "Energy-Reliability Tradeoffs in Sensor Network Storage",
    "short_description": "",
    "full_content": "Sensor nodes that store their data locally are increasingly being deployed in hostile and remote environments such as active volcanoes and battlefields. Observations gathered in these environments are often irreplaceable, and must be protected from loss due to node failures. Nodes may fail individually due to power depletion or hardware/software problems, or they may suffer correlated failures from localized destructive events such as fire or rockfall. While many file systems can guard against these events, they do not consider energy usage in their approach to redundancy. We examine tradeoffs between energy and reliability in three contexts: choice of redundancy technique, choice of redundancy nodes, and frequency of verifying correctness of remotely-stored data. By matching the choice of reliability techniques to the failure characteristics of sensor networks in hostile and inaccessible environments, we can build systems that use less energy while providing higher system reliability.",
    "author": [
      "Neerja Bhatnagar",
      "Kevin M. Greenan",
      "Rosie Wacha",
      "Ethan L. Miller",
      "Darrell D. E. Long"
    ],
    "date": "Jun 1, 2008",
    "url": "https://scholar.google.com/scholar?q=Energy-Reliability%20Tradeoffs%20in%20Sensor%20Network%20Storage",
    "bibTeX": {
      "@inproceedings": "bhatnagar-procee08",
      "author": "Neerja Bhatnagar and Kevin M. Greenan and Rosie Wacha and Ethan L. Miller and Darrell D. E. Long",
      "title": "Energy-Reliability Tradeoffs in Sensor Network Storage",
      "booktitle": "Proceedings of Fifth Workshop on Embedded Networked Sensors",
      "year": 2008,
      "month": "jun",
      "publisher": "ACM"
    }
  },
  {
    "id": 242,
    "title": "Ensuring Data Survival in Solid-State Storage Devices",
    "short_description": "",
    "full_content": "",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Thomas J. E. Schwarz"
    ],
    "date": "Feb 1, 2008",
    "url": "https://scholar.google.com/scholar?q=Ensuring%20Data%20Survival%20in%20Solid-State%20Storage%20Devices",
    "bibTeX": {
      "@inproceedings": "pris-procee08",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Thomas J. E. Schwarz",
      "title": "Ensuring Data Survival in Solid-State Storage Devices",
      "booktitle": "Proceedings of the First International Workshop on Storage and I/O Virtualization, Performance, Energy, Evaluation and Dependability",
      "year": 2008,
      "month": "feb",
      "publisher": "ACM"
    }
  },
  {
    "id": 243,
    "title": "An Analytic Study of Stream Tapping Protocols",
    "short_description": "",
    "full_content": "We present the first analytic study of stream tapping protocols, a family of protocols that provide the most efficient way to distribute videos on demand at low to medium request arrival rates, say, less than ten requests per hour for a two-hour video. The main results of this study are analytical solutions for the optimal operational points of stream tapping, stream tapping with small client buffers, stream tapping with partial preloading and stream tapping with proactive streams. In addition we introduce a new stream tapping protocol with batching that caps the bandwidth requirements of stream tapping at high to very high arrival rates.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2006",
    "url": "https://scholar.google.com/scholar?q=An%20Analytic%20Study%20of%20Stream%20Tapping%20Protocols",
    "bibTeX": {
      "@inproceedings": "pris-icme06",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "An Analytic Study of Stream Tapping Protocols",
      "booktitle": "Proceedings of the 2006 IEEE International Conference on Multimedia",
      "year": 2006,
      "month": "jul",
      "publisher": "ACM"
    }
  },
  {
    "id": 244,
    "title": "Expecting the Unexpected: Adaptation for Predictive Energy Conservation",
    "short_description": "",
    "full_content": "The use of access predictors to improve storage device performance has been investigated for both improving access times, as well as a means of reducing energy consumed by the disk. Such predictors also offer us an opportunity to demonstrate the benefits of an adaptive approach to handling unexpected workloads, whether they are the result of natural variation or deliberate attempts to generate a problematic workload. Such workloads can pose a threat to system availability if they result in the excessive consumption of potentially limited resources such as energy. We propose that actively reshaping a disk access workload, using a dynamically self-adjusting access predictor, allows for consistently good performance in the face of varying workloads. Specifically, we describe how our Best Shifting prefetching policy, by adapting to the needs of the currently observed workload, can use 15",
    "author": [
      "Jeffrey P. Rybczynski",
      "Darrell D. E. Long",
      "Ahmed Amer"
    ],
    "date": "Nov 1, 2005",
    "url": "https://scholar.google.com/scholar?q=Expecting%20the%20Unexpected%3A%20Adaptation%20for%20Predictive%20Energy%20Conservation",
    "bibTeX": {
      "@inproceedings": "rybczynski-procee05",
      "author": "Jeffrey P. Rybczynski and Darrell D. E. Long and Ahmed Amer",
      "title": "Expecting the Unexpected: Adaptation for Predictive Energy Conservation",
      "booktitle": "Proceedings of the International Workshop on Storage Security and Survivability",
      "year": 2005,
      "month": "nov",
      "publisher": "ACM"
    }
  },
  {
    "id": 245,
    "title": "The Case for Aggressive Partial Preloading in Video-on-Demand Broadcasting Protocols",
    "short_description": "",
    "full_content": "Broadcasting protocols for video-on-demand usually consume over fifty percent of their bandwidth to distribute the first ten to fifteen minutes of the videos they distribute. Since all these protocols require the user set-top box to include a disk drive, we propose to use this drive to store the first five to twenty minutes of the ten to twenty most popular videos. This will provide low-cost instant access to these videos.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "Aug 1, 2001",
    "url": "https://scholar.google.com/scholar?q=The%20Case%20for%20Aggressive%20Partial%20Preloading%20in%20Video-on-Demand%20Broadcasting%20Protocols",
    "bibTeX": {
      "@inproceedings": "pris-icme01",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "The Case for Aggressive Partial Preloading in Video-on-Demand Broadcasting Protocols",
      "booktitle": "Proceedings of the 2001 IEEE International Conference on Multimedia",
      "year": 2001,
      "month": "aug",
      "publisher": "ACM"
    }
  },
  {
    "id": 246,
    "title": "Caching Files with a Program-based Last $n$ Successors Model",
    "short_description": "",
    "full_content": "Recent increases in CPU performance have outpaced increases in hard drive performance. As a result, disk operations have become more expensive in terms of CPU cycles spent waiting for disk operations to complete. File prediction can mitigate this problem by prefetching files into cache before they are accessed. However, incorrect prediction is to a certain degree both unavoidable and costly. We present the Program-based Last N Successors (PLNS) file prediction model that identifies relationships between files through the names of the programs accessing them. Our simulation results show that PLNS makes at least 21.11",
    "author": [
      "Tsozen Yeh",
      "Darrell D. E. Long",
      "Scott A. Brandt"
    ],
    "date": "Jun 1, 2001",
    "url": "https://scholar.google.com/scholar?q=Caching%20Files%20with%20a%20Program-based%20Last%20%24n%24%20Successors%20Model",
    "bibTeX": {
      "@inproceedings": "yeh-procee01",
      "author": "Tsozen Yeh and Darrell D. E. Long and Scott A. Brandt",
      "title": "Caching Files with a Program-based Last $n$ Successors Model",
      "booktitle": "Proceedings of the Workshop on Caching, Coherence and Consistency",
      "year": 2001,
      "month": "jun",
      "publisher": "ACM"
    }
  },
  {
    "id": 247,
    "title": "Adverse Filtering Effects and the Resilience of Aggregating Caches",
    "short_description": "",
    "full_content": "We introduce the aggregating cache, and demonstrate how it can be used to reduce the number of file retrieval requests made by a caching client, improving storage system performance by reducing the impact of latency. The aggregating cache utilizes predetermined groupings of files to perform group retrievals. These groups are maintained by the server, and built dynamically using observed inter-file relationships. Through a simple analytical model we demonstrate how this mechanism has the potential to reduce average latencies by 75 Through trace-based simulation we demonstrate that a simple aggregating cache can reduce the number of demand fetches by almost 50 improving cache hit ratios by up to 5",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Jun 1, 2001",
    "url": "https://scholar.google.com/scholar?q=Adverse%20Filtering%20Effects%20and%20the%20Resilience%20of%20Aggregating%20Caches",
    "bibTeX": {
      "@inproceedings": "amer-procee01",
      "author": "Ahmed Amer and Darrell D. E. Long",
      "title": "Adverse Filtering Effects and the Resilience of Aggregating Caches",
      "booktitle": "Proceedings of the Workshop on Caching, Coherence and Consistency",
      "year": 2001,
      "month": "jun",
      "publisher": "ACM"
    }
  },
  {
    "id": 248,
    "title": "Dynamic Relationships and the Persistence of Pairings",
    "short_description": "",
    "full_content": "The ability to automatically hoard data on a computer's local store would go a long way towards freeing the mobile user from dependence on the network and potentially unbounded latencies. An important step in developing a fully automated file hoarding algorithm is the ability to automatically identify strong relationships between files. We present a mechanism for visualizing the degree of long-term relationships inherent in a file access stream. We do this by comparing the performance of static and dynamic relationship predictors. We demonstrate that even the simplest associations (from a static/first-successor predictor) maintain relatively high accuracy over extended periods of time, closely tracking the performance of an equivalent dynamic (last-successor) predictor. We then introduce rank-difference plots, a visualization technique which allows us to demonstrate how this behavior is caused by stable static pairings of files that are lost by the adaptation of the dynamic predictor for a substantial subset of frequently accessed files. We conclude by demonstrating how a third pairing mechanism can make use of these observations to outperform both the dynamic and static predictors.",
    "author": [
      "Ahmed Amer",
      "Darrell D. E. Long"
    ],
    "date": "Apr 1, 2001",
    "url": "https://scholar.google.com/scholar?q=Dynamic%20Relationships%20and%20the%20Persistence%20of%20Pairings",
    "bibTeX": {
      "@inproceedings": "amer-procee01",
      "author": "Ahmed Amer and Darrell D. E. Long",
      "title": "Dynamic Relationships and the Persistence of Pairings",
      "booktitle": "Proceedings of the International Workshop on Wireless Networks and Mobile Computing",
      "year": 2001,
      "month": "apr",
      "publisher": "IEEE"
    }
  },
  {
    "id": 249,
    "title": "A Universal Distribution Protocol for Video-on-Demand",
    "short_description": "",
    "full_content": "Most existing distribution protocols for video-on-demand are tailored for a specific range of request arrival rates and do not perform well beyond that range. We present a universal distribution protocol based on Juhn and Tseng's fast broadcasting protocol. Our protocol performs as well as the best reactive protocol at low to moderate request arrival rates and reverts to the fast broadcasting protocol at high arrival rates. : video-on-demand, broadcasting protocols, stream tapping.",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Jul 1, 2000",
    "url": "https://scholar.google.com/scholar?q=A%20Universal%20Distribution%20Protocol%20for%20Video-on-Demand",
    "bibTeX": {
      "@inproceedings": "pris-icme00",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "A Universal Distribution Protocol for Video-on-Demand",
      "booktitle": "Proceedings of the 2000 IEEE International Conference on Multimedia",
      "year": 2000,
      "month": "jul",
      "publisher": "ACM"
    }
  },
  {
    "id": 250,
    "title": "Limiting the Client Bandwidth of Broadcasting Protocols for Videos on Demand",
    "short_description": "",
    "full_content": "Broadcasting protocols can lower the cost of video-on-demand services by more efficiently distributing all videos that are simultaneously watched by many viewers. The most efficient broadcasting protocols require a customer set-top box capable of capturing data from five to seven video channels at the same time. We show how to modify existing broadcasting protocols so that their client bandwidth would never exceed three to four channels and apply our method to the fast broadcasting and the new pagoda broadcasting protocols. Our data show that this modification has only a moderate effect on the overall performance of the two protocols because their server bandwidth never increases by more than 15 percent. : video-on-demand, broadcasting protocols, fast broadcasting, new pagoda broadcasting.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long"
    ],
    "date": "May 1, 2000",
    "url": "https://scholar.google.com/scholar?q=Limiting%20the%20Client%20Bandwidth%20of%20Broadcasting%20Protocols%20for%20Videos%20on%20Demand",
    "bibTeX": {
      "@inproceedings": "pris-euromedia00",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long",
      "title": "Limiting the Client Bandwidth of Broadcasting Protocols for Videos on Demand",
      "booktitle": "Proceedings of Euromedia 2000",
      "year": 2000,
      "month": "may",
      "publisher": "Society for Computer Simulation"
    }
  },
  {
    "id": 251,
    "title": "Bandwidth Allocation Issues in Near Video on Demand Services",
    "short_description": "",
    "full_content": "",
    "author": [
      "Jehan-François Pâris",
      "Steven W. Carter",
      "Darrell D. E. Long"
    ],
    "date": "Sep 1, 1998",
    "url": "https://scholar.google.com/scholar?q=Bandwidth%20Allocation%20Issues%20in%20Near%20Video%20on%20Demand%20Services",
    "bibTeX": {
      "@inproceedings": "pris-icme98",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Steven W. Carter and Darrell D. E. Long",
      "title": "Bandwidth Allocation Issues in Near Video on Demand Services",
      "booktitle": "Proceedings of the Third Annual Multimedia Technology and Applications Conference",
      "year": 1998,
      "month": "sep",
      "publisher": "ACM"
    }
  },
  {
    "id": 252,
    "title": "A Replicated Monitoring Tool",
    "short_description": "",
    "full_content": "Modeling the reliability of distributed systems requires a good understanding of the reliability of the components. Careful modeling allows highly fault-tolerant distributed applications to be constructed at the least cost. Realistic estimates can be found by measuring the performance of actual systems. An enormous amount of information about system performance can be acquired with no special privileges via the Internet. A distributed monitoring tool called a tattler is described. The system is composed of a group of tattler processes that monitor a set of selected hosts. The tattlers cooperate to provide a fault-tolerant distributed data base of information about the hosts they monitor. They use weak-consistency replication techniques to ensure their own fault-tolerance and the eventual consistency of the data base that they maintain.",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 1992",
    "url": "https://scholar.google.com/scholar?q=A%20Replicated%20Monitoring%20Tool",
    "bibTeX": {
      "@inproceedings": "long-wmrd92",
      "author": "Darrell D. E. Long",
      "title": "A Replicated Monitoring Tool",
      "booktitle": "Proceedings of the Second Workshop on the Management of Replicated Data",
      "year": 1992,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 253,
    "title": "Analysis of Replication Control Protocols",
    "short_description": "",
    "full_content": "In recent years many replication control protocols have been proposed, but too often these protocols are presented with insufficient evidence to demonstrate superiority over existing protocols. In this article, some simple analytical tools are presented that allow replication control protocols to be compared. A dynamic voting/ protocol is used as an example.",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 1990",
    "url": "https://scholar.google.com/scholar?q=Analysis%20of%20Replication%20Control%20Protocols",
    "bibTeX": {
      "@inproceedings": "long-wmrd90",
      "author": "Darrell D. E. Long",
      "title": "Analysis of Replication Control Protocols",
      "booktitle": "Proceedings of the First Workshop on the Management of Replicated Data",
      "year": 1990,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 254,
    "title": "The Management of Consistency in Fault-Tolerant File Systems",
    "short_description": "",
    "full_content": "Available copy protocols guarantee the consistency of replicated data objects against any combination of non-Byzantine failures that do not result in partial communication failures. While the original available copy protocol assumed instantaneous detection of failures and instantaneous propagation of this information, more realistic protocols that do not rely on these assumptions have been devised. Two such protocols are investigated in this paper: a naive available copy (NAC) protocol that does not maintain any state information, and an optimistic available copy (OAC) protocol that only maintains state information at write and recovery times. Markov models are used to compare the performance of these two protocols with that of the original available copy protocol. These protocols are shown to perform nearly as well as the original available copy protocol, which is shown to perform much better than quorum consensus protocols.",
    "author": [
      "Jehan-François Pâris",
      "Darrell D. E. Long",
      "Walter A. Burkhard"
    ],
    "date": "Dec 1, 1987",
    "url": "https://scholar.google.com/scholar?q=The%20Management%20of%20Consistency%20in%20Fault-Tolerant%20File%20Systems",
    "bibTeX": {
      "@inproceedings": "pris-procee87",
      "author": "Jehan-Fran\\c{c}ois P\\^aris and Darrell D. E. Long and Walter A. Burkhard",
      "title": "The Management of Consistency in Fault-Tolerant File Systems",
      "booktitle": "Proceedings of the Workshop on Fault Tolerance in Parallel and Distributed Computing",
      "year": 1987,
      "month": "dec",
      "publisher": "IEEE"
    }
  },
  {
    "id": 255,
    "title": "Optimistic Algorithms for Replicated Data Management",
    "short_description": "",
    "full_content": "",
    "author": [
      "Darrell D. E. Long"
    ],
    "date": "Nov 1, 1987",
    "url": "https://scholar.google.com/scholar?q=Optimistic%20Algorithms%20for%20Replicated%20Data%20Management",
    "bibTeX": {
      "@inproceedings": "long-procee87",
      "author": "Darrell D. E. Long",
      "title": "Optimistic Algorithms for Replicated Data Management",
      "booktitle": "Proceedings of the Second Workshop on Large-Grained Parallelism",
      "year": 1987,
      "month": "nov",
      "publisher": "IEEE"
    }
  },
  {
    "id": 256,
    "title": "Towards a Multipurpose Forecast Systems for the Columbia River Estuary",
    "short_description": "",
    "full_content": "",
    "author": [
      "António M. Baptista",
      "Michael Wilkin",
      "Phillip Pearson",
      "Paul Turner",
      "Cole McCandlish",
      "Phillip Barrett",
      "Salil Das",
      "Wendy Sommerfield",
      "Ming Qi",
      "Neetu Nangia",
      "David Jay",
      "Darrell Long",
      "Calton Pu",
      "John Hunt",
      "Zhaoqing Yang",
      "Edward Myers",
      "Jeff Darland",
      "Anna Farrenkopf"
    ],
    "date": "Nov 1, 1998",
    "url": "https://scholar.google.com/scholar?q=Towards%20a%20Multipurpose%20Forecast%20Systems%20for%20the%20Columbia%20River%20Estuary",
    "bibTeX": {
      "@inproceedings": "baptista-procee98",
      "author": "António M. Baptista and Michael Wilkin and Phillip Pearson and Paul Turner and Cole McCandlish and Phillip Barrett and Salil Das and Wendy Sommerfield and Ming Qi and Neetu Nangia and David Jay and Darrell Long and Calton Pu and John Hunt and Zhaoqing Yang and Edward Myers and Jeff Darland and Anna Farrenkopf",
      "title": "Towards a Multipurpose Forecast Systems for the Columbia River Estuary",
      "booktitle": "Proceedings of the Ocean Community Conference",
      "year": 1998,
      "month": "nov",
      "publisher": "MTS"
    }
  },
  {
    "id": 257,
    "title": "Combining Chunk Boundary and Chunk Signature Calculations for Deduplication (Combinar la calculacion de limites de trozos y de firmas de trozos para deduplicacion)",
    "short_description": "",
    "full_content": "Many modern, large-scale storage solutions offer deduplication, which can achieve impressive compression rates for many loads, especially for backups. When accepting new data for storage, deduplication checks whether parts of the data is already stored. If this is the case, then the system does not store that part of the new data but replaces it with a reference to the location where the data already resides. A typical deduplication system breaks data into chunks, hashes each chunk, and uses an index to see whether the chunk has already been stored. Variable chunk systems offer better compression, but process data byte-for-byte twice, first to calculate the chunk boundaries and then to calculate the hash. This limits the ingress bandwidth of a system. We propose a method to reuse the chunk boundary calculations in order to strengthen the collision resistance of the hash, allowing us to use a faster hashing method with fewer bytes or a much larger (256 times by adding two bytes) storage system with the same high assurance against chunk collision and resulting data loss. --Deduplication, Algebraic Signatures.",
    "author": [
      "Witold Litwin",
      "Darrell Long",
      "Thomas Schwarz"
    ],
    "date": "Jan 1, 2012",
    "url": "https://scholar.google.com/scholar?q=Combining%20Chunk%20Boundary%20and%20Chunk%20Signature%20Calculations%20for%20Deduplication%20%28Combinar%20la%20calculacion%20de%20limites%20de%20trozos%20y%20de%20firmas%20de%20trozos%20para%20deduplicacion%29",
    "bibTeX": {
      "@article": "litwin-latam12",
      "author": "Witold Litwin and Darrell Long and Thomas Schwarz",
      "title": "Combining Chunk Boundary and Chunk Signature Calculations for Deduplication (Combinar la calculacion de limites de trozos y de firmas de trozos para deduplicacion)",
      "journal": "IEEE Latin America Transactions (Revista IEEE América Latina)",
      "volume": 10,
      "number": 1,
      "year": 2012,
      "month": "jan",
      "pages": "1305--1311"
    }
  }
]
